{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaeo3m6civnc"
      },
      "source": [
        "# **Trabajo Pr√°ctico Final - TUIA NLP 2024**\n",
        "\n",
        "## **Chatbot experto en Eurogames con RAG y Agentes ReAct**\n",
        "\n",
        "### **Autor**: Tom√°s Valentino Avecilla\n",
        "### **legajo**: --------\n",
        "### **Fecha**: 16 de diciembre de 2024\n",
        "### **Materia**: Procesamiento del Lenguaje Natural (NLP)  \n",
        "### **Instituci√≥n**: Facultad de Ciencias Exactas, ingenieria y Agrimensura -- UNR\n",
        "\n",
        "---\n",
        "\n",
        "## **Descripci√≥n del Trabajo**\n",
        "Este cuaderno contiene el desarrollo del Trabajo Pr√°ctico Final para la materia TUIA NLP 2024. El objetivo es implementar un **chatbot experto** sobre un juego de mesa tipo Eurogame, utilizando las t√©cnicas de **Retrieval-Augmented Generation (RAG)** y **Agentes ReAct**.\n",
        "\n",
        "El proyecto incluye:\n",
        "- Recolecci√≥n de informaci√≥n desde m√∫ltiples fuentes: texto, datos tabulares y bases de grafos.\n",
        "- Construcci√≥n de bases de datos vectoriales, tabulares y de grafos.\n",
        "- Implementaci√≥n de clasificadores y sistemas de recuperaci√≥n din√°mica de informaci√≥n.\n",
        "- Desarrollo de un agente que combina herramientas para responder consultas complejas.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laUFu54--eTf"
      },
      "source": [
        "## Preparaci√≥n del Entorno de Trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "EF0P5Oln-rDu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# --- 1. Actualizaci√≥n del Sistema ---\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-spa tesseract-ocr-eng  # Herramientas de OCR y dependencias\n",
        "!pip install python-decouple\n",
        "\n",
        "# --- 2. Instalaci√≥n de Bibliotecas Generales ---\n",
        "!pip install gdown requests python-docx  # Descarga de archivos, solicitudes web, manejo de archivos docx\n",
        "\n",
        "# --- 3. Procesamiento de Im√°genes y OCR ---\n",
        "!pip install pdf2image pytesseract  # Extracci√≥n de im√°genes y OCR desde PDFs\n",
        "\n",
        "# --- 4. Web Scraping y Automatizaci√≥n ---\n",
        "!pip install selenium webdriver-manager  # Automatizaci√≥n de navegaci√≥n web\n",
        "\n",
        "# --- 5. Procesamiento del Lenguaje Natural ---\n",
        "!pip install transformers  # Modelos de Hugging Face y Sentence Transformers\n",
        "!pip install --upgrade sentence_transformers\n",
        "!python -m spacy download es_core_news_md en_core_web_sm  # Modelo en espa√±ol para spaCy\n",
        "!pip install translators  # Traducci√≥n autom√°tica de texto\n",
        "!pip install langdetect\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "!pip install --upgrade chromadb neo4j pydgraph  # Bases de datos vectoriales, de grafos y almacenamiento\n",
        "\n",
        "# --- 7. Modelos de Machine Learning y Deep Learning ---\n",
        "!pip install torch\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install rank_bm25\n",
        "\n",
        "# --- 8. Herramientas para Agentes ReAct ---\n",
        "!pip install llama-index-llms-ollama llama-index pygoogleweather wikipedia  # Agentes y conectores para datos externos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Ym2dmNZf-jDo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# --- 1. Importaciones B√°sicas y Manejo de Archivos ---\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import uuid\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import logging\n",
        "from time import time\n",
        "import json\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import docx\n",
        "from docx import Document\n",
        "\n",
        "# --- 2. Procesamiento de Im√°genes y OCR ---\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "# --- 3. Web Scraping ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# --- 4. Procesamiento del Lenguaje Natural ---\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import translators as ts\n",
        "from langdetect import detect\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- 5. Modelos & Embeddings ---\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from huggingface_hub import InferenceClient\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "import chromadb  # Base de datos vectorial\n",
        "from neo4j import GraphDatabase  # Base de datos de grafos\n",
        "\n",
        "# --- 7. Agentes y Herramientas ReAct ---\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent.react.formatter import ReActChatFormatter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZAbT5Vc2kB"
      },
      "source": [
        "## Recolecci√≥n de Informaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BluNQKmNhYi4"
      },
      "source": [
        "### **1:** üéÆ Reglas y Jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2YVnGhEzNQ0"
      },
      "source": [
        "#### Archivos Descargados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzfzPLRm9Gh5"
      },
      "source": [
        "Primero vamos a usar tres documentos descargados de la seccion ***files*** del sitio BGG _[link](https://boardgamegeek.com/boardgame/371942/the-white-castle/files)_\n",
        "Contamos con 3 documentos PDF\n",
        "\n",
        "\n",
        "1.   Reglamento en Espa√±ol\n",
        "2.   Reglamento en Ingles\n",
        "3.   Guia Rapida en ingles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGBUyJJX9jHv",
        "outputId": "f5c7c305-ec00-4a6f-a137-183a34f2a567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\n",
            "To: /content/reglamento_en.pdf\n",
            "100% 13.2M/13.2M [00:00<00:00, 23.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\n",
            "To: /content/qs_en.pdf\n",
            "100% 446k/446k [00:00<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\" --output \"reglamento_en.pdf\"\n",
        "!gdown \"1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\" --output \"qs_en.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XvjR5eP_NiT"
      },
      "source": [
        "Los 3 PDFs son imagenes por lo cual vamos a tener que extraer el texto y para eso usaremos un ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th1OJLibBFIB"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n para crear carpetas si no existen\n",
        "def crear_carpeta(nombre_carpeta):\n",
        "    if not os.path.exists(nombre_carpeta):\n",
        "        os.makedirs(nombre_carpeta)\n",
        "\n",
        "# Carpetas para guardar las im√°genes\n",
        "carpeta_en = \"imgs_reglamento_en\"\n",
        "carpeta_qs = \"imgs_qs_en\"\n",
        "\n",
        "# Crear carpetas\n",
        "crear_carpeta(carpeta_en)\n",
        "crear_carpeta(carpeta_qs)\n",
        "\n",
        "# Convertir PDFs en listas de im√°genes\n",
        "imgs_reglamento_en = convert_from_path(\"reglamento_en.pdf\")\n",
        "imgs_qs_en = convert_from_path(\"qs_en.pdf\")\n",
        "\n",
        "# Guardar las im√°genes en sus carpetas correspondientes\n",
        "for i, imagen in enumerate(imgs_reglamento_en):\n",
        "    imagen.save(os.path.join(carpeta_en, f'pagina_en_{i + 1}.png'), 'PNG')\n",
        "\n",
        "for i, imagen in enumerate(imgs_qs_en):\n",
        "    imagen.save(os.path.join(carpeta_qs, f'qs_en_{i + 1}.png'), 'PNG')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUn_5YrLCKpO",
        "outputId": "36cf0b6c-2b6b-46e7-eb8b-cc2e060dfe4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto procesado y guardado en: texto_reglamento_en\n",
            "Texto procesado y guardado en: texto_qs_en\n"
          ]
        }
      ],
      "source": [
        "# Configurar pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "# Funci√≥n para procesar im√°genes y extraer texto\n",
        "def procesar_imagenes(carpeta_imagenes, carpeta_salida, idioma=\"eng\"):\n",
        "    # Crear carpeta de salida si no existe\n",
        "    if not os.path.exists(carpeta_salida):\n",
        "        os.makedirs(carpeta_salida)\n",
        "\n",
        "    # Iterar sobre las im√°genes en la carpeta\n",
        "    for imagen_nombre in sorted(os.listdir(carpeta_imagenes)):  # Ordenar para mantener secuencia\n",
        "        if imagen_nombre.endswith(\".png\"):\n",
        "            ruta_imagen = os.path.join(carpeta_imagenes, imagen_nombre)\n",
        "\n",
        "            # Extraer texto de la imagen\n",
        "            texto = pytesseract.image_to_string(Image.open(ruta_imagen), lang=idioma)\n",
        "\n",
        "            # Guardar texto en un archivo\n",
        "            nombre_txt = os.path.splitext(imagen_nombre)[0] + \".txt\"\n",
        "            ruta_salida = os.path.join(carpeta_salida, nombre_txt)\n",
        "\n",
        "            with open(ruta_salida, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(texto)\n",
        "\n",
        "    print(f\"Texto procesado y guardado en: {carpeta_salida}\")\n",
        "\n",
        "# Procesar las im√°genes de cada carpeta\n",
        "procesar_imagenes(\"imgs_reglamento_en\", \"texto_reglamento_en\", idioma=\"eng\")\n",
        "procesar_imagenes(\"imgs_qs_en\", \"texto_qs_en\", idioma=\"eng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeLxKhjxvhwD",
        "outputId": "fdb10be4-ff58-46c6-c88f-f5f483713968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos creados en: documentos\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Funci√≥n avanzada para limpiar texto y normalizarlo\n",
        "def limpiar_texto(texto):\n",
        "\n",
        "    # Paso 1: Normalizar texto a Unicode est√°ndar (NFKC)\n",
        "    texto = unicodedata.normalize(\"NFKC\", texto)\n",
        "\n",
        "    # Paso 2: Eliminar m√∫ltiples espacios, tabulaciones y l√≠neas redundantes\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    texto = re.sub(r'[^a-zA-Z0-9\\s.,!?¬ø¬°]', '', texto)\n",
        "\n",
        "    # Paso 3: Convertir a min√∫sculas para uniformidad sem√°ntica\n",
        "    texto = texto.lower()\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "# Funci√≥n para crear documentos a partir de los archivos .txt en cada carpeta\n",
        "def crear_documentos(carpeta_lista, carpeta_destino_docs):\n",
        "    # Crear carpeta de destino para los documentos si no existe\n",
        "    if not os.path.exists(carpeta_destino_docs):\n",
        "        os.makedirs(carpeta_destino_docs)\n",
        "\n",
        "    # Iterar sobre las carpetas en la lista proporcionada\n",
        "    for carpeta in sorted(carpeta_lista):  # Ordenar para mantener consistencia\n",
        "        if os.path.isdir(carpeta):  # Procesar solo carpetas\n",
        "            # Crear un documento Word\n",
        "            doc = Document()\n",
        "\n",
        "            # Iterar sobre los archivos .txt en la carpeta\n",
        "            for archivo_txt in sorted(os.listdir(carpeta)):  # Ordenar para mantener secuencia\n",
        "                if archivo_txt.endswith(\".txt\"):\n",
        "                    ruta_txt = os.path.join(carpeta, archivo_txt)\n",
        "\n",
        "                    with open(ruta_txt, \"r\", encoding=\"utf-8\") as file:\n",
        "                        texto = file.read()\n",
        "\n",
        "                    # Limpiar el texto antes de agregarlo al documento\n",
        "                    texto = limpiar_texto(texto)\n",
        "\n",
        "                    doc.add_paragraph(texto)\n",
        "\n",
        "            # Guardar el documento en la carpeta de destino\n",
        "            nombre_doc = f\"{os.path.basename(carpeta)}.docx\"\n",
        "            ruta_doc = os.path.join(carpeta_destino_docs, nombre_doc)\n",
        "            doc.save(ruta_doc)\n",
        "\n",
        "    print(f\"Documentos creados en: {carpeta_destino_docs}\")\n",
        "\n",
        "# Lista de carpetas y carpeta de destino\n",
        "carpetas = [\"texto_reglamento_en\", \"texto_qs_en\"]\n",
        "carpeta_destino_docs = \"documentos\"\n",
        "\n",
        "# Crear documentos\n",
        "crear_documentos(carpetas, carpeta_destino_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdWgfoZPzP56"
      },
      "source": [
        "#### Scrapping de jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ7yO2an5lZX"
      },
      "source": [
        "Haremos un scrapping de la siguiente [rese√±a](https://misutmeeple.com/2023/11/resena-the-white-castle/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKOKbAVQ1waB",
        "outputId": "39d73380-d5bc-4118-bc37-1b6bd6f0863d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como: rese√±a.docx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# URL de la p√°gina que deseas scrapear\n",
        "url = 'https://misutmeeple.com/2023/11/resena-the-white-castle/'\n",
        "\n",
        "# Hacer la solicitud HTTP para obtener el HTML de la p√°gina\n",
        "response = requests.get(url)\n",
        "html = response.text\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Seleccionar el div con la clase espec√≠fica\n",
        "div = soup.find('div', class_='entry-content single-content')\n",
        "\n",
        "# Asegurarse de que el div existe antes de continuar\n",
        "if div:\n",
        "    # Crear un nuevo documento Word\n",
        "    doc = Document()\n",
        "\n",
        "    # Iterar sobre los elementos h2, h3 y p dentro del div\n",
        "    for elemento in div.find_all(['h2', 'h3', 'p'], recursive=True):\n",
        "        etiqueta = elemento.name\n",
        "        texto = elemento.get_text(strip=True)\n",
        "\n",
        "        # Agregar texto al documento seg√∫n el tipo de etiqueta\n",
        "        if etiqueta == 'h2':\n",
        "            doc.add_heading(texto, level=1)\n",
        "        elif etiqueta == 'h3':\n",
        "            doc.add_heading(texto, level=2)\n",
        "        elif etiqueta == 'p':\n",
        "            doc.add_paragraph(texto)\n",
        "\n",
        "    # Guardar el documento Word\n",
        "    doc_name = \"rese√±a.docx\"\n",
        "    doc.save(doc_name)\n",
        "\n",
        "    print(f\"Documento guardado como: {doc_name}\")\n",
        "else:\n",
        "    print(\"Error.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7EsuI7g9hG"
      },
      "source": [
        "### **2:** üèØ The White Castle Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68I7kA99MF9g"
      },
      "source": [
        "##### Configuracion de drivers para no generar conflictos en el uso de selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCbsuXOtjIJ"
      },
      "source": [
        "Nos aseguramos de que el sistema tiene las bibliotecas necesarias para ejecutar aplicaciones gr√°ficas (como Chrome) en un entorno Linux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckhcQr9tmly",
        "outputId": "c6742294-dcf8-42b2-9435-72005308b261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.19).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxi6 set to manually installed.\n",
            "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-dev set to manually installed.\n",
            "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-xcb1 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  gconf-service gconf-service-backend gconf2-common libdbus-glib-1-2\n",
            "The following NEW packages will be installed:\n",
            "  gconf-service gconf-service-backend gconf2-common libdbus-glib-1-2 libgconf-2-4 libglu1-mesa\n",
            "0 upgraded, 6 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 1,071 kB of archives.\n",
            "After this operation, 8,675 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdbus-glib-1-2 amd64 0.112-2build1 [65.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf2-common all 3.2.6-7ubuntu2 [698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgconf-2-4 amd64 3.2.6-7ubuntu2 [86.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service-backend amd64 3.2.6-7ubuntu2 [59.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service amd64 3.2.6-7ubuntu2 [17.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Fetched 1,071 kB in 2s (583 kB/s)\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 123714 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdbus-glib-1-2_0.112-2build1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Selecting previously unselected package gconf2-common.\n",
            "Preparing to unpack .../1-gconf2-common_3.2.6-7ubuntu2_all.deb ...\n",
            "Unpacking gconf2-common (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package libgconf-2-4:amd64.\n",
            "Preparing to unpack .../2-libgconf-2-4_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service-backend.\n",
            "Preparing to unpack .../3-gconf-service-backend_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service.\n",
            "Preparing to unpack .../4-gconf-service_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../5-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up gconf2-common (3.2.6-7ubuntu2) ...\n",
            "\n",
            "Creating config file /etc/gconf/2/path with new version\n",
            "Setting up libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Actualizar los repositorios\n",
        "!apt-get update\n",
        "!apt-get install -y wget curl unzip\n",
        "!apt-get install -y libx11-dev libx11-xcb1 libglu1-mesa libxi6 libgconf-2-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BD-ThPEtqiU"
      },
      "source": [
        "Descargamos e instalamos Google Chrome para poder usarlo con Selenium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ZiLbvwtsUD",
        "outputId": "22ab6a18-3210-490d-b5a0-28e598bc8a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-15 21:30:57--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.175.136, 142.251.175.190, 142.251.175.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.175.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112421156 (107M) [application/x-debian-package]\n",
            "Saving to: ‚Äògoogle-chrome-stable_current_amd64.deb‚Äô\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.21M   235MB/s    in 0.5s    \n",
            "\n",
            "2024-12-15 21:30:58 (235 MB/s) - ‚Äògoogle-chrome-stable_current_amd64.deb‚Äô saved [112421156/112421156]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123883 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (131.0.6778.139-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 51 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 2s (4,764 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 124000 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (131.0.6778.139-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalar Google Chrome (√∫ltima versi√≥n estable)\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt --fix-broken install -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctEwpcXt3gt"
      },
      "source": [
        "Descargamos ChromeDriver (necesario para Selenium) y lo mueve a una ubicaci√≥n accesible globalmente en el sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uIIpMdJ8j0rb",
        "outputId": "d500e203-876d-4234-9ba0-d0c4932682a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-15 21:31:17--  https://chromedriver.storage.googleapis.com/131.0.6778.87/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 172.217.194.207, 172.253.118.207, 74.125.200.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|172.217.194.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-12-15 21:31:18 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open chromedriver_linux64.zip, chromedriver_linux64.zip.zip or chromedriver_linux64.zip.ZIP.\n",
            "mv: cannot stat 'chromedriver': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wget https://chromedriver.storage.googleapis.com/131.0.6778.87/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip\n",
        "!mv chromedriver /usr/local/bin/chromedriver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qIhAs2t8mx"
      },
      "source": [
        "Configura el navegador para ejecutarse en segundo plano sin mostrar interfaz gr√°fica, ideal para entornos como servidores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8-Cg4Kmt9i_"
      },
      "outputs": [],
      "source": [
        "# Configurar las opciones de Chrome para usarlo en modo headless\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')  # Modo sin cabeza\n",
        "chrome_options.add_argument('--no-sandbox')  # Evitar problemas de sandboxing\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Usar en contenedores o entornos con poca memoria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H60n54f-MN3J"
      },
      "source": [
        "#### Datos Tabulares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpcGmlUy6pmu"
      },
      "source": [
        "A continuacion haremos el scrapping de la pagina [board game geek](https://boardgamegeek.com/boardgame/371942/the-white-castle/) para extraer datos numericos e insertarlos en un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V6O609PpgN6"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# URL del juego\n",
        "url = 'https://boardgamegeek.com/boardgame/371942/the-white-castle'\n",
        "\n",
        "# Abrir la p√°gina\n",
        "driver.get(url)\n",
        "\n",
        "# Esperar unos segundos para que cargue el contenido din√°mico\n",
        "time.sleep(8)\n",
        "\n",
        "# Obtener el HTML completo de la p√°gina cargada\n",
        "html = driver.page_source\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Extraer los datos y guardarlos en variables\n",
        "rating_value = soup.find('span', itemprop='ratingValue')\n",
        "rating_value = rating_value.text.strip() if rating_value else 'N/A'\n",
        "\n",
        "year_section = soup.find('span', class_='game-year')\n",
        "game_year = year_section.text.strip() if year_section else 'N/A'\n",
        "game_year = game_year.strip(\"()\").strip()  # Eliminar par√©ntesis y espacios adicionales\n",
        "game_year = int(game_year) if game_year.isdigit() else 'N/A'\n",
        "\n",
        "review_count_section = soup.find('meta', itemprop='reviewCount')\n",
        "review_count_value = int(review_count_section['content']) if review_count_section else 'N/A'\n",
        "\n",
        "min_players = max_players = play_time = suggested_age = complexity = 'N/A'\n",
        "\n",
        "# Extraer los elementos de gameplay\n",
        "gameplay_section = soup.find('ul', class_='gameplay')\n",
        "if gameplay_section:\n",
        "    gameplay_items = gameplay_section.find_all('li', class_='gameplay-item')\n",
        "    for item in gameplay_items:\n",
        "        title = item.find('h3').text.strip() if item.find('h3') else 'N/A'\n",
        "\n",
        "        if title == \"Number of Players\":\n",
        "            min_players = int(item.find('meta', itemprop='minValue')['content']) if item.find('meta', itemprop='minValue') else 'N/A'\n",
        "            max_players = int(item.find('meta', itemprop='maxValue')['content']) if item.find('meta', itemprop='maxValue') else 'N/A'\n",
        "\n",
        "        elif title == \"Play Time\":\n",
        "            play_time = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Suggested Age\":\n",
        "            suggested_age = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Complexity\":\n",
        "            complexity = float(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "like_count_section = soup.find('a', class_='game-action-play-count')\n",
        "likes = like_count_section.text.strip() if like_count_section else 'N/A'\n",
        "\n",
        "# Extraer precios sugeridos\n",
        "prices = soup.find_all('div', class_='summary-sale-item-price')\n",
        "precio_sugerido = prices[1].text.strip() if len(prices) > 0 else 'N/A'\n",
        "\n",
        "# Cerrar el WebDriver\n",
        "driver.quit()\n",
        "\n",
        "# Crear un diccionario para los datos\n",
        "data = {\n",
        "    \"Rating\": [float(rating_value) if rating_value != 'N/A' else None],\n",
        "    \"Year\": [game_year if game_year != 'N/A' else None],\n",
        "    \"Review Count\": [review_count_value if review_count_value != 'N/A' else None],\n",
        "    \"Min Players\": [min_players if min_players != 'N/A' else None],\n",
        "    \"Max Players\": [max_players if max_players != 'N/A' else None],\n",
        "    \"Play Time (min)\": [play_time if play_time != 'N/A' else None],\n",
        "    \"Suggested Age\": [suggested_age if suggested_age != 'N/A' else None],\n",
        "    \"Complexity\": [complexity if complexity != 'N/A' else None],\n",
        "    \"Likes\": [int(likes.replace('K', '000').replace('.', '')) if 'K' in likes else int(likes) if likes != 'N/A' else None],\n",
        "    \"Price (USD)\": [float(precio_sugerido.replace(\"from $\", \"\").replace(\"$\", \"\")) if precio_sugerido != 'N/A' else None]\n",
        "}\n",
        "# Crear el DataFrame\n",
        "df_castle = pd.DataFrame(data)\n",
        "df_castle.head()\n",
        "\n",
        "# Descargar df en csv\n",
        "df_castle.to_csv('castle.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qG42vAhfh1"
      },
      "source": [
        "### **3:** üí¨ Comentarios y opiniones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPtkW3iLnkT"
      },
      "source": [
        "Con el siguiente scrapping traemos a un documento todos lo comentarios en el [foro](https://boardgamegeek.com/boardgame/371942/the-white-castle/forums)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldXKjRVTiTOq",
        "outputId": "ba66b106-ad49-498b-f9b7-f7f033a1c426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como 'comentarios.docx'\n"
          ]
        }
      ],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "# Crear un nuevo documento Word\n",
        "doc = Document()\n",
        "\n",
        "# Funci√≥n para scrapear los hilos individuales\n",
        "def get_thread_details(thread_url):\n",
        "    driver.get(thread_url)\n",
        "    time.sleep(3)  # Esperar a que se cargue el contenido din√°mico\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Buscar los comentarios dentro de las etiquetas <gg-markup-safe-html>\n",
        "    comments = soup.find_all('gg-markup-safe-html')\n",
        "    thread_content = \"\"\n",
        "\n",
        "    for comment in comments:\n",
        "        thread_content += comment.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "    return thread_content\n",
        "\n",
        "# Loop para iterar sobre varias p√°ginas\n",
        "for id in [1, 2, 3, 4]:\n",
        "    url = f'https://boardgamegeek.com/boardgame/371942/the-white-castle/forums/0?pageid={id}'\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Obtener el HTML completo de la p√°gina cargada\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Seleccionar todos los <li> con la clase 'summary-item ng-scope'\n",
        "    li_items = soup.find_all('li', class_='summary-item ng-scope')\n",
        "\n",
        "    # Iterar sobre cada elemento <li>\n",
        "    for li in li_items:\n",
        "        # Extraer el t√≠tulo\n",
        "        title = li.find('h3', class_='m-0 fs-sm text-inherit leading-inherit text-inline')\n",
        "        if title:\n",
        "            title_text = title.get_text(strip=True)\n",
        "            doc.add_paragraph(f\"T√≠tulo: {title_text}\")\n",
        "\n",
        "        # Extraer el enlace del hilo\n",
        "        link = li.find('a', {'ng-href': True})\n",
        "        if link:\n",
        "            thread_url = \"https://boardgamegeek.com\" + link['ng-href']\n",
        "\n",
        "            # Obtener los detalles del hilo (comentarios)\n",
        "            thread_details = get_thread_details(thread_url)\n",
        "            doc.add_paragraph(thread_details)\n",
        "\n",
        "        # Agregar un salto de p√°gina despu√©s de cada hilo\n",
        "        doc.add_paragraph('')\n",
        "\n",
        "# Guardar el documento Word con el contenido scrapeado\n",
        "doc.save('comentarios.docx')\n",
        "print(\"Documento guardado como 'comentarios.docx'\")\n",
        "\n",
        "# Cerrar el navegador al final\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYUJ1v6OlMi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jeKsDhlNZ_-"
      },
      "source": [
        "Organizados los documentos que usaremos en la carpeta documentos, el archivo rese√±a.docx y el archivo comentarios.docx vamos a proceder a crear las bdds.\n",
        "Cabe destacar que se accedera a los documentos a partir de una carpeta drive para no tener que repetir el proceso de recoleccion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjhPfbqfBX8L"
      },
      "source": [
        "## Terminada la Recolecci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6YqZS3aO8LQ",
        "outputId": "dd534ac2-86c5-4f9f-fa7c-545b0edd08a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\n",
            "To: /content/rulebook_english.docx\n",
            "100% 30.6k/30.6k [00:00<00:00, 41.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\n",
            "To: /content/quick_start_english.docx\n",
            "100% 21.0k/21.0k [00:00<00:00, 31.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\n",
            "To: /content/rese√±a_espa√±ol.docx\n",
            "100% 31.1k/31.1k [00:00<00:00, 37.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\n",
            "To: /content/comentarios.docx\n",
            "100% 257k/257k [00:00<00:00, 5.45MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\n",
            "To: /content/castle.csv\n",
            "100% 150/150 [00:00<00:00, 497kB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title Docs\n",
        "!gdown \"1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\" --output \"rulebook_english.docx\"\n",
        "!gdown \"1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\" --output \"quick_start_english.docx\"\n",
        "!gdown \"1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\" --output \"rese√±a_espa√±ol.docx\"\n",
        "!gdown \"1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\" --output \"comentarios.docx\"\n",
        "!gdown \"1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\" --output \"castle.csv\"\n",
        "df_castle = pd.read_csv('castle.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-X8cb0NBbPv",
        "outputId": "c4e52a6c-7311-4896-8d73-29517f9913b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo rese√±a_espa√±ol.docx procesado y eliminado correctamente. Traducci√≥n guardada en translated_rese√±a_espa√±ol.docx.\n",
            "Archivo comentarios.docx procesado y eliminado correctamente. Traducci√≥n guardada en translated_comentarios.docx.\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n para dividir texto en fragmentos m√°s peque√±os\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para traducir un texto a ingl√©s\n",
        "def traducir_a_ingles(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='auto', to_language='en') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "# Funci√≥n para extraer texto de un archivo .docx\n",
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Funci√≥n para guardar texto en un archivo .docx\n",
        "def save_text_to_docx(text, file_path):\n",
        "    try:\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(text)\n",
        "        doc.save(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo {file_path}: {e}\")\n",
        "\n",
        "# Procesar m√∫ltiples archivos\n",
        "def procesar_archivos(file_names):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extraer texto del archivo\n",
        "            text = extract_text_from_docx(filename)\n",
        "            if not text.strip():\n",
        "                print(f\"El archivo {filename} est√° vac√≠o o no se pudo leer.\")\n",
        "                continue\n",
        "\n",
        "            # Traducir texto a ingl√©s\n",
        "            translated_text = traducir_a_ingles(text)\n",
        "\n",
        "            # Guardar el texto traducido en un nuevo archivo\n",
        "            new_filename = f\"translated_{os.path.basename(filename)}\"\n",
        "            save_text_to_docx(translated_text, new_filename)\n",
        "\n",
        "            # Eliminar archivo original\n",
        "            os.remove(filename)\n",
        "            print(f\"Archivo {filename} procesado y eliminado correctamente. Traducci√≥n guardada en {new_filename}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando el archivo {filename}: {e}\")\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"rese√±a_espa√±ol.docx\", \"comentarios.docx\"]\n",
        "procesar_archivos(archivos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNynjmUMVr3"
      },
      "source": [
        "## Construccion de Bases de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IomKasVMiEJ"
      },
      "source": [
        "### Base de Datos Vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "collapsed": true,
        "id": "pYfLbjDbQI6y"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Funci√≥n para dividir texto en chunks por oraciones usando SpaCy\n",
        "def dividir_texto_por_oraciones(texto, max_length=1000):\n",
        "    doc = nlp(texto)\n",
        "    oraciones = [sent.text for sent in doc.sents]\n",
        "    fragmentos, fragmento_actual = [], \"\"\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if len(fragmento_actual) + len(oracion) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual.strip())\n",
        "            fragmento_actual = oracion\n",
        "        else:\n",
        "            fragmento_actual += \" \" + oracion if fragmento_actual else oracion\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual.strip())\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para extraer metadatos con NER y POS, refinados\n",
        "from collections import Counter\n",
        "\n",
        "def extraer_metadatos(texto, n_keywords=3):\n",
        "    \"\"\"\n",
        "    Extrae metadatos de un texto incluyendo entidades nombradas y las palabras clave m√°s relevantes.\n",
        "\n",
        "    :param texto: Texto del cual extraer los metadatos.\n",
        "    :param n_keywords: N√∫mero de palabras clave m√°s relevantes a extraer.\n",
        "    :return: Un diccionario con entidades y palabras clave.\n",
        "    \"\"\"\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Extraer entidades relevantes\n",
        "    entidades = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"TIME\", \"MONEY\"}]\n",
        "\n",
        "    # Extraer palabras clave relevantes: solo sustantivos (NOUN) y verbos (VERB)\n",
        "    palabras_clave = [token.text.lower() for token in doc if token.pos_ in {\"NOUN\", \"VERB\"} and len(token.text) > 2]\n",
        "\n",
        "    # Contar la frecuencia de las palabras clave\n",
        "    palabras_frecuentes = Counter(palabras_clave).most_common(n_keywords)\n",
        "\n",
        "    # Seleccionar las palabras clave m√°s frecuentes\n",
        "    palabras_clave_relevantes = [palabra for palabra, _ in palabras_frecuentes]\n",
        "\n",
        "    # Convertir las listas a strings\n",
        "    return {\n",
        "        \"entities\": \", \".join(entidades),  # Unir las entidades en un string separado por comas\n",
        "        \"keywords\": \", \".join(palabras_clave_relevantes)  # Unir las palabras clave en un string separado por comas\n",
        "    }\n",
        "\n",
        "\n",
        "# Funci√≥n para calcular embeddings promediados\n",
        "def average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "# Funci√≥n para generar embeddings normalizados\n",
        "def generar_embeddings(texto, tokenizer, model):\n",
        "    inputs = tokenizer(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "    return F.normalize(embeddings, p=2, dim=1).squeeze().numpy()\n",
        "\n",
        "# Crear la conexi√≥n a ChromaDB\n",
        "client_castle = chromadb.Client()\n",
        "\n",
        "#client_castle.delete_collection(name=\"white_castle_embeddings\")\n",
        "collection = client_castle.create_collection(name=\"white_castle_embeddings\")\n",
        "\n",
        "# Procesar archivos y llenar la base de datos\n",
        "def procesar_archivos_y_llenar_bd(file_names, tokenizer, model, collection):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        text = extract_text_from_docx(filename)\n",
        "        if not text.strip():\n",
        "            print(f\"El archivo {filename} est√° vac√≠o o no se pudo leer.\")\n",
        "            continue\n",
        "\n",
        "        # Dividir el texto en chunks por oraciones\n",
        "        chunks = dividir_texto_por_oraciones(text, max_length=1000)\n",
        "\n",
        "        # Determinar el tipo de archivo basado en el nombre (por ejemplo, por el t√≠tulo del archivo)\n",
        "        if \"rulebook\" in filename:\n",
        "            tipo = \"rules\"\n",
        "        elif \"comentarios\" in filename:\n",
        "            tipo = \"comments\"\n",
        "        elif \"quick_start\" in filename:\n",
        "            tipo = \"quick_start\"\n",
        "        elif \"rese√±a\" in filename:\n",
        "            tipo = \"review\"\n",
        "        else:\n",
        "            tipo = \"unknown\"  # Si no se reconoce, se marca como \"unknown\"\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            try:\n",
        "                # Generar embeddings\n",
        "                embedding = generar_embeddings(chunk, tokenizer, model)\n",
        "\n",
        "                # Extraer metadatos din√°micos con NER y POS\n",
        "                dynamic_metadata = extraer_metadatos(chunk)\n",
        "\n",
        "                # Combinar metadatos\n",
        "                metadatos = {**dynamic_metadata, \"type\": tipo, \"filename\": filename}\n",
        "\n",
        "                # Agregar a la base de datos\n",
        "                collection.add(\n",
        "                    documents=[chunk],\n",
        "                    metadatas=[metadatos],\n",
        "                    ids=[str(uuid.uuid4())],\n",
        "                    embeddings=[embedding]\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar el chunk {i} del archivo {filename}: {e}\")\n",
        "\n",
        "# Configuraci√≥n del modelo y tokenizador\n",
        "tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_comentarios.docx\", \"translated_rese√±a_espa√±ol.docx\"]\n",
        "\n",
        "procesar_archivos_y_llenar_bd(archivos, tokenizer_e5, model_e5, collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SILkkBm8kD0y"
      },
      "outputs": [],
      "source": [
        "def realizar_consulta(texto, tokenizer, model, collection, k=3):\n",
        "    # Generar el embedding del texto de consulta\n",
        "    embedding = generar_embeddings(texto, tokenizer, model)\n",
        "\n",
        "    # Realizar la b√∫squeda en la colecci√≥n de ChromaDB\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=[embedding],\n",
        "        n_results=k  # N√∫mero de resultados que deseas obtener\n",
        "    )\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Consulta de ejemplo\n",
        "texto_consulta = \"Explain the rules of the game.\"\n",
        "\n",
        "# Realizar la consulta en la colecci√≥n\n",
        "r = realizar_consulta(texto_consulta, tokenizer_e5, model_e5, collection)\n",
        "print(r['documents'])\n",
        "# Mostrar los resultados de la consulta, incluyendo el score de similitud\n",
        "for i, doc in enumerate(r['documents'][0]):\n",
        "    similarity_score = r['distances'][0][i]\n",
        "    metadata = r['metadatas'][0][i]  # Metadatos asociados con el documento\n",
        "    print(f\"Resultado {i+1}:\")\n",
        "    print(f\"Texto: {doc}\")  # Texto del documento\n",
        "    print(f\"Metadatos: {metadata}\")  # Metadatos del documento\n",
        "    print(f\"Similitud: {similarity_score:.4f}\")  # Formatear el primer score de similitud\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPW3Uj9MkTG"
      },
      "source": [
        "### Base de Datos de Grafos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vvSIQXrhtq9D"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Crear el cliente de Hugging Face con tu API Key\n",
        "qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "# Funci√≥n para crear el nodo central \"The White Castle\"\n",
        "def crear_nodo_central(driver):\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            session.run(\"\"\"\n",
        "                MERGE (central:Game {name: 'The White Castle'})\n",
        "            \"\"\")\n",
        "            print(\"Nodo central 'The White Castle' creado o ya existente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al crear el nodo central: {e}\")\n",
        "\n",
        "# Funci√≥n para leer archivos .docx y dividir el texto en fragmentos\n",
        "def leer_documento(file_path, max_length=1000):\n",
        "    try:\n",
        "        # Leer el documento .docx\n",
        "        doc = docx.Document(file_path)\n",
        "        texto = \"\\n\".join([parrafo.text.strip() for parrafo in doc.paragraphs])\n",
        "\n",
        "        # Dividir el texto en fragmentos si excede el max_length\n",
        "        palabras = texto.split()\n",
        "        fragmentos = []\n",
        "        fragmento_actual = \"\"\n",
        "\n",
        "        for palabra in palabras:\n",
        "            if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "                fragmentos.append(fragmento_actual)\n",
        "                fragmento_actual = palabra\n",
        "            else:\n",
        "                fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "        if fragmento_actual:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "\n",
        "        return fragmentos\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Funci√≥n para extraer las tr√≠adas RDF de un p√°rrafo\n",
        "def extraer_triadass_rdf(texto_parrafo):\n",
        "    try:\n",
        "        # Usar el cliente de Hugging Face para obtener la respuesta del modelo\n",
        "        response = qwen_client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "            messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system used to generate RDF triplets for a graph in a Neo4j Aura database. \"\n",
        "                        \"The central node 'The White Castle' has already been created in the database. \"\n",
        "                        \"Your priority must be finding relevant information related to the game 'The White Castle', \"\n",
        "                        \"such as the designer, creators, or important relationships. \"\n",
        "                        \"Focus only on triplets where either the subject or object is 'The White Castle'. \"\n",
        "                        \"Do not attempt to recreate 'The White Castle'. \"\n",
        "                        \"Your output must be like this: (subject, predicate, object)\"\n",
        "                    ),\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": f\"extract this in RDF triples: {texto_parrafo}\"}\n",
        "            ],\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        triadass_rdf_texto = response.choices[0].message['content'].strip()\n",
        "        if not triadass_rdf_texto:\n",
        "            print(\"No triples found.\")\n",
        "            return []\n",
        "\n",
        "        print(triadass_rdf_texto)\n",
        "        return triadass_rdf_texto\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener la respuesta del modelo: {e}\")\n",
        "        return []\n",
        "\n",
        "# Funci√≥n para parsear las tr√≠adas RDF extra√≠das\n",
        "def parsear_triadass_rdf(texto):\n",
        "    triadass = []\n",
        "    lineas = texto.split(\"\\n\")\n",
        "    for linea in lineas:\n",
        "        if linea.strip():\n",
        "            # Asumimos que las tripletas se presentan entre par√©ntesis\n",
        "            partes = linea.replace('(', '').replace(')', '').split(\",\")  # Separar por comas y eliminar par√©ntesis\n",
        "            if len(partes) == 3:\n",
        "                sujeto = partes[0].strip()\n",
        "                predicado = partes[1].strip()\n",
        "                objeto = partes[2].strip()\n",
        "\n",
        "                # Verificar si 'The White Castle' est√° presente\n",
        "                if \"The White Castle\" in (sujeto, objeto):\n",
        "                    triadass.append((sujeto, predicado, objeto))\n",
        "                else:\n",
        "                    print(f\"Tripleta descartada (no incluye 'The White Castle'): {linea}\")\n",
        "    return triadass\n",
        "\n",
        "# Funci√≥n para almacenar las tr√≠adas RDF en Neo4j\n",
        "def almacenar_triadass_rdf(triadass, driver):\n",
        "    with driver.session() as session:\n",
        "        for sujeto, predicado, objeto in triadass:\n",
        "            try:\n",
        "                # Asegurarse de que las cadenas no tengan caracteres problem√°ticos\n",
        "                sujeto = sujeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "                objeto = objeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "                # Crear nodos y relaciones\n",
        "                cypher_query = f\"\"\"\n",
        "                MERGE (sujeto:Entity {{name: '{sujeto}'}})\n",
        "                MERGE (objeto:Entity {{name: '{objeto}'}})\n",
        "                MERGE (sujeto)-[:{predicado.replace(' ', '_').upper()}]->(objeto)\n",
        "                \"\"\"\n",
        "                session.run(cypher_query)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar la tr√≠ada ({sujeto}, {predicado}, {objeto}): {e}\")\n",
        "\n",
        "# Funci√≥n para procesar m√∫ltiples archivos y almacenarlos en Neo4j\n",
        "def procesar_documentos(archivos, driver):\n",
        "    for file_path in archivos:\n",
        "        print(f\"Procesando el archivo: {file_path}\")\n",
        "        parrafos = leer_documento(file_path)\n",
        "        for parrafo in parrafos:\n",
        "            print(f\"Extrayendo tr√≠adas para el p√°rrafo: {parrafo}\")\n",
        "            triadass_rdf_texto = extraer_triadass_rdf(parrafo)\n",
        "            if triadass_rdf_texto:  # Solo procesar si se extrajeron tr√≠adas\n",
        "                triadass = parsear_triadass_rdf(triadass_rdf_texto)\n",
        "                if triadass:\n",
        "                    almacenar_triadass_rdf(triadass, driver)\n",
        "                else:\n",
        "                    print(f\"No se encontraron tr√≠adas v√°lidas en el p√°rrafo: {parrafo}\")\n",
        "\n",
        "# Funci√≥n principal para inicializar la conexi√≥n con Neo4j y procesar los documentos\n",
        "def main():\n",
        "    from neo4j import GraphDatabase\n",
        "\n",
        "    # Conectar a Neo4j\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "    # Crear el nodo central\n",
        "    crear_nodo_central(driver)\n",
        "\n",
        "    # Archivos a procesar\n",
        "    archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_rese√±a_espa√±ol.docx\"]\n",
        "\n",
        "    # Procesar los documentos y almacenar las tr√≠adas RDF en Neo4j\n",
        "    procesar_documentos(archivos, driver)\n",
        "\n",
        "    # Cerrar la conexi√≥n con Neo4j\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kf-lcXlhi8n"
      },
      "source": [
        "## Clasificador Basado en modelo entrenado con ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzMp-Hkhoi8",
        "outputId": "e69ed8d3-30a2-4539-eb21-54519fb4c9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n: 0.9090909090909091\n",
            "Reporte de clasificaci√≥n:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89         5\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       0.80      1.00      0.89         4\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.93      0.93        11\n",
            "weighted avg       0.93      0.91      0.91        11\n",
            "\n",
            "Prompt: '¬øQu√© dice el manual sobre el uso de cartas especiales?' - Clasificaci√≥n: rules\n",
            "Prompt: '¬øEs este juego adecuado para jugadores avanzados?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øC√≥mo recomiendan jugar en partidas de 2 jugadores?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øQu√© pasa si se agotan los recursos en el tablero?' - Clasificaci√≥n: rules\n",
            "Prompt: '¬øEs m√°s divertido jugar en parejas o individualmente?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øQu√© estrategias funcionan mejor con 4 jugadores?' - Clasificaci√≥n: reviews\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Dataset ampliado de prompts\n",
        "dataset = [\n",
        "    # Reglas (rules)\n",
        "    (0, \"¬øCu√°les son las reglas para construir un edificio?\"),\n",
        "    (0, \"¬øQu√© pasa si hay un empate en los puntos?\"),\n",
        "    (0, \"¬øCu√°ntas acciones puedo realizar en un turno?\"),\n",
        "    (0, \"¬øQu√© ocurre si no puedo pagar un recurso necesario?\"),\n",
        "    (0, \"¬øC√≥mo se resuelve un desempate en el final del juego?\"),\n",
        "    (0, \"¬øPuedo construir m√°s de un edificio en un solo turno?\"),\n",
        "    (0, \"¬øQu√© limitaciones existen para colocar fichas?\"),\n",
        "    (0, \"¬øEs obligatorio usar todos los recursos en un turno?\"),\n",
        "    (0, \"¬øQu√© pasa si termino un turno con m√°s de tres cartas?\"),\n",
        "    (0, \"¬øC√≥mo se distribuyen los puntos al final del juego?\"),\n",
        "    (0, \"¬øPuedo intercambiar recursos con otros jugadores?\"),\n",
        "    (0, \"¬øQu√© acciones est√°n permitidas en la fase de preparaci√≥n?\"),\n",
        "    (0, \"¬øCu√°ntos turnos tiene cada ronda?\"),\n",
        "    (0, \"¬øCu√°l es el orden para activar habilidades especiales?\"),\n",
        "    (0, \"¬øQu√© reglas aplican para las cartas especiales?\"),\n",
        "    (0, \"¬øPuedo usar habilidades en el turno de otro jugador?\"),\n",
        "    (0, \"¬øQu√© ocurre si el mazo de cartas se agota?\"),\n",
        "    (0, \"¬øHay un l√≠mite de fichas que puedo usar en un turno?\"),\n",
        "\n",
        "    # Rese√±as (reviews)\n",
        "    (1, \"¬øQu√© opinan los jugadores sobre la tem√°tica del juego?\"),\n",
        "    (1, \"¬øEste juego es recomendado para principiantes?\"),\n",
        "    (1, \"¬øC√≥mo describen los jugadores la complejidad del juego?\"),\n",
        "    (1, \"¬øQu√© tan rejugable es este juego seg√∫n las rese√±as?\"),\n",
        "    (1, \"¬øEs un buen juego para jugar en familia?\"),\n",
        "    (1, \"¬øCu√°l es la duraci√≥n t√≠pica de una partida?\"),\n",
        "    (1, \"¬øQu√© tan equilibradas est√°n las estrategias disponibles?\"),\n",
        "    (1, \"¬øC√≥mo se compara este juego con otros del mismo g√©nero?\"),\n",
        "    (1, \"¬øLos componentes del juego tienen buena calidad?\"),\n",
        "    (1, \"¬øQu√© aspectos destacan m√°s los jugadores en sus rese√±as?\"),\n",
        "    (1, \"¬øHay alguna rese√±a negativa sobre el juego?\"),\n",
        "    (1, \"¬øEste juego es m√°s adecuado para expertos o principiantes?\"),\n",
        "    (1, \"¬øQu√© tan divertido es jugar con grupos grandes?\"),\n",
        "    (1, \"¬øLas reglas son f√°ciles de aprender seg√∫n las rese√±as?\"),\n",
        "    (1, \"¬øLos gr√°ficos del juego ayudan a la inmersi√≥n?\"),\n",
        "    (1, \"¬øEs un juego m√°s social o estrat√©gico?\"),\n",
        "    (1, \"¬øLos jugadores mencionan alg√∫n problema recurrente en el dise√±o?\"),\n",
        "    (1, \"¬øSe necesitan expansiones para disfrutar el juego al m√°ximo?\"),\n",
        "\n",
        "    # Comentarios (comments)\n",
        "    (2, \"¬øCu√°les son las mejores estrategias iniciales?\"),\n",
        "    (2, \"¬øHay estrategias avanzadas para ganar m√°s puntos?\"),\n",
        "    (2, \"¬øQu√© tipo de combinaciones de cartas son m√°s efectivas?\"),\n",
        "    (2, \"¬øC√≥mo maximizar los recursos en las primeras rondas?\"),\n",
        "    (2, \"¬øEs mejor centrarse en la defensa o en la ofensiva?\"),\n",
        "    (2, \"¬øQu√© habilidades son m√°s √∫tiles para principiantes?\"),\n",
        "    (2, \"¬øHay estrategias espec√≠ficas para jugar con 2 jugadores?\"),\n",
        "    (2, \"¬øCu√°l es la mejor manera de gestionar los recursos limitados?\"),\n",
        "    (2, \"¬øC√≥mo sacar ventaja de los bonos de las cartas especiales?\"),\n",
        "    (2, \"¬øQu√© t√°cticas recomiendan los jugadores experimentados?\"),\n",
        "    (2, \"¬øC√≥mo adaptar la estrategia dependiendo de los oponentes?\"),\n",
        "    (2, \"¬øEs m√°s beneficioso priorizar los edificios grandes?\"),\n",
        "    (2, \"¬øQu√© estrategias funcionan mejor en partidas r√°pidas?\"),\n",
        "    (2, \"¬øC√≥mo influye el orden de turno en la estrategia?\"),\n",
        "    (2, \"¬øCu√°l es el mejor momento para usar cartas especiales?\"),\n",
        "    (2, \"¬øQu√© cartas son clave para asegurar la victoria?\"),\n",
        "    (2, \"¬øC√≥mo combinar habilidades de edificios para optimizar el puntaje?\"),\n",
        "    (2, \"¬øQu√© errores comunes se deben evitar en estrategias avanzadas?\"),\n",
        "]\n",
        "\n",
        "\n",
        "# Mapeo de categor√≠as\n",
        "categories = {0: \"rules\", 1: \"reviews\", 2: \"comments\"}\n",
        "\n",
        "# Preparar datos\n",
        "X = [text for _, text in dataset]\n",
        "y = [label for label, _ in dataset]\n",
        "\n",
        "# Generar embeddings para el dataset\n",
        "X_vectorized = model.encode(X)\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar el modelo\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Precisi√≥n:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Reporte de clasificaci√≥n:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Nuevos prompts de prueba\n",
        "new_prompts = [\n",
        "    \"¬øQu√© dice el manual sobre el uso de cartas especiales?\",\n",
        "    \"¬øEs este juego adecuado para jugadores avanzados?\",\n",
        "    \"¬øC√≥mo recomiendan jugar en partidas de 2 jugadores?\",\n",
        "    \"¬øQu√© pasa si se agotan los recursos en el tablero?\",\n",
        "    \"¬øEs m√°s divertido jugar en parejas o individualmente?\",\n",
        "    \"¬øQu√© estrategias funcionan mejor con 4 jugadores?\",\n",
        "]\n",
        "new_embeddings = model.encode(new_prompts)\n",
        "new_predictions = classifier.predict(new_embeddings)\n",
        "\n",
        "# Mostrar resultados\n",
        "for prompt, pred in zip(new_prompts, new_predictions):\n",
        "    print(f\"Prompt: '{prompt}' - Clasificaci√≥n: {categories[pred]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPqLVthBGho"
      },
      "source": [
        "## Queries Dinamicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqtXwrAY-ZO"
      },
      "source": [
        "#### Doc Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "L9TBVV-BZFnX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DocSearch:\n",
        "    def __init__(self, db_client, collection):\n",
        "        self.db_client = db_client\n",
        "        self.collection = collection\n",
        "        self.tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_e5.to(self.device)\n",
        "\n",
        "        # Usar el pipeline de Hugging Face para el modelo BGE ReRanker\n",
        "        self.reranker = pipeline(\"text-classification\", model=\"BAAI/bge-reranker-v2-m3\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    def average_pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "    def generar_embeddings(self, texto):\n",
        "        inputs = self.tokenizer_e5(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_e5(**inputs)\n",
        "            embeddings = self.average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "        return F.normalize(embeddings, p=2, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    def rerank(self, query, top_documents):\n",
        "        query_prediction = self.reranker(query)\n",
        "        query_score = query_prediction[0]['score']\n",
        "\n",
        "        document_scores = [\n",
        "            self.reranker(doc[\"document\"])[0]['score'] for doc in top_documents\n",
        "        ]\n",
        "\n",
        "        min_score, max_score = min(document_scores), max(document_scores)\n",
        "        document_scores = [(score - min_score) / (max_score - min_score) for score in document_scores]\n",
        "        scores = [0.5 * query_score + 0.5 * doc_score for doc_score in document_scores]\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            doc[\"rerank_score\"] = scores[i]\n",
        "\n",
        "        return sorted(top_documents, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    def realizar_consulta(self, texto, k=3):\n",
        "        embedding = self.generar_embeddings(texto)\n",
        "        resultados = self.collection.query(\n",
        "            query_embeddings=[embedding], n_results=k\n",
        "        )\n",
        "        return resultados\n",
        "\n",
        "    def penalizar_redundancia(self, top_documents, threshold=0.95, penalty_score=0.5):\n",
        "        \"\"\"\n",
        "        Penaliza documentos similares usando similitud coseno entre embeddings.\n",
        "        Garantiza que al menos un documento ser√° devuelto.\n",
        "        Tambi√©n penaliza documentos con el metadato 'filename: translated_comentarios.docx' reduciendo su puntuaci√≥n.\n",
        "        \"\"\"\n",
        "        if not top_documents:\n",
        "            return top_documents  # Retornar directamente si la lista est√° vac√≠a\n",
        "\n",
        "        embeddings = [self.generar_embeddings(doc[\"document\"]) for doc in top_documents]\n",
        "        sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "        penalized_docs = []\n",
        "        added_indices = set()\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            score = 1.0  # Puntuaci√≥n predeterminada\n",
        "\n",
        "            # Penalizar documentos con filename: translated_comentarios.docx\n",
        "            if doc.get(\"metadata\", {}).get(\"filename\") == \"translated_comentarios.docx\":\n",
        "                score -= penalty_score  # Reducir la puntuaci√≥n de estos documentos\n",
        "\n",
        "            # Incluir documentos solo si no son demasiado similares a los ya seleccionados\n",
        "            if all(sim_matrix[i][j] < threshold for j in range(len(top_documents)) if i != j and j in added_indices):\n",
        "                penalized_docs.append({**doc, \"penalized_score\": score})\n",
        "                added_indices.add(i)\n",
        "\n",
        "        # Garantizar que al menos un documento est√© presente\n",
        "        if not penalized_docs:\n",
        "            penalized_docs.append(top_documents[0])  # Incluir el primer documento por defecto\n",
        "\n",
        "        # Ordenar los documentos penalizados por su puntuaci√≥n\n",
        "        penalized_docs.sort(key=lambda x: x.get(\"penalized_score\", 1.0), reverse=True)\n",
        "\n",
        "        return penalized_docs\n",
        "\n",
        "\n",
        "    def hybrid_search(self, prompt, n_results=3, n_rerank=8, redundancy_threshold=0.95) -> str:\n",
        "        try:\n",
        "            resultados = self.realizar_consulta(prompt, k=n_rerank)\n",
        "            documents = resultados['documents'][0]\n",
        "            metadatas = resultados['metadatas'][0]\n",
        "            distances = resultados['distances'][0]\n",
        "\n",
        "            tokenized_documents = [doc.split() for doc in documents]\n",
        "            tokenized_query = prompt.split()\n",
        "            bm25 = BM25Okapi(tokenized_documents)\n",
        "            keyword_scores = bm25.get_scores(tokenized_query)\n",
        "            keyword_scores = (np.array(keyword_scores) - np.min(keyword_scores)) / (np.max(keyword_scores) - np.min(keyword_scores))\n",
        "\n",
        "            semantic_scores = 1 - (np.array(distances) - np.min(distances)) / (np.max(distances) - np.min(distances))\n",
        "            combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
        "\n",
        "            top_documents = [\n",
        "                {\"document\": doc, \"metadata\": meta, \"score\": score}\n",
        "                for doc, meta, score in zip(documents, metadatas, combined_scores)\n",
        "            ]\n",
        "            top_documents = sorted(top_documents, key=lambda x: x[\"score\"], reverse=True)[:n_rerank]\n",
        "\n",
        "            # Aplicar penalizaci√≥n de redundancia\n",
        "            top_documents = self.penalizar_redundancia(top_documents, threshold=redundancy_threshold)\n",
        "\n",
        "            # Reordenar con el modelo ReRanker\n",
        "            reranked_results = self.rerank(prompt, top_documents)[:n_results]\n",
        "\n",
        "            result_string = \"\\n\\n\".join(\n",
        "                [\n",
        "                    f\"{i+1}. Document:\\n\\\"{res['document'][:1000]}...\\\"\\n\"\n",
        "                ]\n",
        "            )\n",
        "            return f\"Results:\\n\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error en hybrid_search: {e}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmeZ9GChkBHq",
        "outputId": "d35be216-0437-4e6f-9759-ad38d10dacb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Results:\n",
            "\n",
            "1. Document:\n",
            "\"I kind of want to tell someone \"we're playing White Castle tonight\" and then bring that thing out. That would actually be funny! Enlace al hilo: https://boardgamegeek.com/thread/3220134/number-of-courtiers-in-the-rooms Hi all, at the gate field of the castle there is no limit of courtiers. This rule is mentioned in the rulebook. But how is the situation in the rooms of the castle? thanks, rolwin https://boardgamegeek.com/thread/3168240/can-there-be-two-co... No limit There is no limit of courtiers in the rooms. btw: We thought about this and tried a house rule of 1 meeple per color per room to push players to move upwards in the castle. Didn't change anything in the game experience Enlace al hilo:\n",
            "https://boardgamegeek.com/thread/3220104/daimyo-seals-for-passing-checkpoint...\"\n",
            "Metadata: {'entities': 'tonight, Enlace al, Enlace al', 'filename': 'translated_comentarios.docx', 'keywords': 'castle, limit, courtiers', 'type': 'comments'}\n",
            "Score: 0.5001\n",
            "\n",
            "2. Document:\n",
            "\"Here flies the flag of the Sakai clan, commanded by Daimio Sakai Tadakiyo. The\n",
            "White Castle is a Euro-like game with resource management mechanisms, worker placement, and dice placement to perform actions. During the game, over the course of three rounds (in which you only have 9 turns, 3 times per round), players send members of their clan to tend the gardens, defend the castle or move up the social ladder of the nobility. At the end of the game, players are awarded victory points in a variety of ways. The first thing you notice is that the box in which the game comes is much smaller than similar games of this category. It's the size of a Carcassonne box, which would almost give you the impression that The White Castle is a game of the same quality/difficulty. And that is not the case. You get a lot more for your thirty euros (which is a real bargain). Once the box is open, you notice that there is no insert and that all the space is filled with ... the game. And that's darn clever!...\"\n",
            "Metadata: {'entities': 'Sakai, Daimio Sakai Tadakiyo, Carcassonne', 'filename': 'translated_comentarios.docx', 'keywords': 'game, box, clan', 'type': 'comments'}\n",
            "Score: 0.0976\n",
            "\n",
            "3. Document:\n",
            "\"The White Castle box 77 cards 82 tiles 5 boards 85 wooden pieces 15 dice 1 rulebook The\n",
            "components are high quality with wooden meeples used, dice of 3 different colors, and cardboard tokens. The game comes with 3 cardboard bridges that you will construct when first opening the game. Blocks are used to keep track of resources on player boards, and cards are used to change actions throughout the game. How‚Äôs It Play? Players will be taking turns choosing one of the dice on one of the three bridges to take actions with. The game consists of 3 rounds where players will take 3 turns in each round, so 9 turns total. When choosing a die to perform your action with, you can choose the higher value die of a certain color, or the lower valued die of a certain color. The color will determine what actions you can perform on the board, and how much money you will gain or possibly pay....\"\n",
            "Metadata: {'entities': '', 'filename': 'translated_comentarios.docx', 'keywords': 'game, dice, used', 'type': 'comments'}\n",
            "Score: 0.0343\n"
          ]
        }
      ],
      "source": [
        "doc_search = DocSearch(client_castle, collection)\n",
        "query = \"What are the rules of the white castel\"\n",
        "results = doc_search.hybrid_search(query, n_results=3, n_rerank=8, redundancy_threshold=0.9)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYHHExIeZDx2"
      },
      "source": [
        "#### Tabular Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "OCwISpD-ZFQ-"
      },
      "outputs": [],
      "source": [
        "class TabularSearch:\n",
        "    def __init__(self, data_frame: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Inicializa la clase de b√∫squeda tabular con par√°metros fijos.\n",
        "\n",
        "        :param data_frame: DataFrame de Pandas donde se realizar√° la b√∫squeda.\n",
        "        \"\"\"\n",
        "        self.data_frame = data_frame\n",
        "        self.temperature = 0.4  # Configuraci√≥n fija\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]  # Configuraci√≥n fija\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_tabular(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Env√≠a un prompt al modelo Qwen para generar la consulta tabular.\n",
        "\n",
        "        :param prompt: Prompt que incluye instrucci√≥n, contexto, entrada, y salida deseada.\n",
        "        :return: Consulta generada por el modelo.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are a system used to generate a query to a tabular search into a dataframe. \"\n",
        "                            \"The dataframe contains the following columns: \"\n",
        "                            \"'Rating', 'Year', 'Review Count', 'Min Players', 'Max Players', 'Play Time (min)', \"\n",
        "                            \"'Suggested Age', 'Complexity', 'Likes', 'Price (USD)'. \"\n",
        "                            \"Your queries should be simple and direct. You just need to give the column content\"\n",
        "                            \"You have only one observation so dont extend the query\"\n",
        "                            \"Your Output must be just the code to access the column\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            return response.choices[0].message[\"content\"].strip()\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error al llamar al modelo Qwen: {e}\")\n",
        "\n",
        "    def tabular_search(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Realiza una b√∫squeda tabular basada en el prompt generado y devuelve los resultados como una cadena.\n",
        "\n",
        "        :param prompt: Prompt que el modelo usar√° para generar una consulta.\n",
        "        :return: Resultados filtrados del DataFrame formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Llamar al modelo para generar la consulta tabular\n",
        "            tabular_query = self.query_model_for_tabular(prompt)\n",
        "            print(f\"Consulta generada: {tabular_query}\")\n",
        "\n",
        "            # Limpiar la consulta generada\n",
        "            cleaned_tabular_query = (\n",
        "                tabular_query.replace('```python', '')\n",
        "                .replace(\"```\", '')\n",
        "                .replace(\"df\", 'self.data_frame')\n",
        "                .strip()\n",
        "            )\n",
        "\n",
        "            # Ejecutar la consulta generada usando eval()\n",
        "            result = eval(f\"{cleaned_tabular_query}\")\n",
        "\n",
        "            # Verificar si hay resultados\n",
        "            if result.empty:\n",
        "                return \"No se encontraron resultados para la consulta.\"\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            result_string = result.to_string(index=False)  # Sin √≠ndices para una salida m√°s limpia\n",
        "            return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda tabular: {e}\")\n",
        "            return f\"Error al procesar la consulta: {e}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYkFO_DZBdO"
      },
      "source": [
        "#### Graph Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "6GM7fw-jZDZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphSearch:\n",
        "    def __init__(self, graph_client, api_key: str):\n",
        "        \"\"\"\n",
        "        Inicializa la clase para b√∫squedas en una base de datos de grafos.\n",
        "\n",
        "        :param graph_client: Cliente conectado a la base de datos de grafos.\n",
        "        :param api_key: Clave de API para autenticar el cliente de Hugging Face.\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.graph_client = graph_client\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_cypher(self, prompt: str, pos_context: str) -> str:\n",
        "        if not prompt:\n",
        "            print(\"Error: El prompt est√° vac√≠o.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system specialized in generating Cypher queries for graph databases. \"\n",
        "                        \"The database contains a single node and its relationships with other entities. The central node is labeld 'The White Castle' but you just have to match it like twc: Entity, \"\n",
        "                        \"Examples of relatiosns can be, HAS_DESIGNER, HAS_COVER_ART_BY, INVOLVES, HASMECHANIC OR HASRULE \"\n",
        "                        \"All nodes in the graph are Entity\"\n",
        "                        \"Your task is to analyze the relationships found in the database and generate Cypher queries based on the user‚Äôs request. \"\n",
        "                        \"Your output must be just cypher code.\"\n",
        "                    ),\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"Relationships extracted from the graph database (raw data): {pos_context}\\n\"\n",
        "                        f\"Prompt: {prompt}\"\n",
        "                    ),\n",
        "                }],\n",
        "                max_tokens=500,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            cypher_query = response.choices[0].message[\"content\"].strip()\n",
        "            return cypher_query.replace(\"```cypher\", \"\").replace(\"```\", \"\")\n",
        "        except KeyError:\n",
        "            print(\"Error: Respuesta mal formada del modelo.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar la consulta Cypher: {e}\")\n",
        "            return None\n",
        "\n",
        "    def graph_search(self, prompt: str, pos_context: str = \"\"):\n",
        "        \"\"\"\n",
        "        Genera y ejecuta una consulta Cypher basada en un prompt y contexto POS.\n",
        "\n",
        "        :param prompt: Prompt proporcionado por el usuario.\n",
        "        :param pos_context: Contexto adicional obtenido a partir de POS.\n",
        "        :return: Resultados formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generar la consulta Cypher\n",
        "            cypher_query = self.query_model_for_cypher(prompt, pos_context)\n",
        "            if not cypher_query:\n",
        "                print(\"Error: No se pudo generar la consulta Cypher.\")\n",
        "                return \"Error: No se pudo generar una consulta v√°lida.\"\n",
        "            # Ejecutar la consulta en el cliente Neo4j\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                records = result.data()\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            if records:\n",
        "                result_string = \"\\n\".join([str(record) for record in records])\n",
        "                return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "            else:\n",
        "                return \"No se encontraron resultados en la base de datos de grafos.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda en grafos: {e}\")\n",
        "            return f\"Error al buscar en la base de datos: {e}\"\n",
        "\n",
        "\n",
        "    def search_relations_by_pos(self, pos_word: str):\n",
        "        \"\"\"\n",
        "        Realiza una consulta para encontrar relaciones cuyo nombre contenga la palabra clave extra√≠da con POS.\n",
        "\n",
        "        :param pos_word: Palabra clave extra√≠da del texto.\n",
        "        :return: Resultado de la b√∫squeda de relaciones o None si ocurre un error.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cypher_query = f\"\"\"\n",
        "              MATCH ()-[r]->()\n",
        "              WHERE type(r) =~ '.*{pos_word.upper()}.*'\n",
        "              RETURN r\n",
        "              \"\"\"\n",
        "\n",
        "            # Ejecutar la consulta en el cliente de grafos\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                # Obtener todos los resultados\n",
        "                records = result.data()\n",
        "                return records  # Retorna los resultados de la consulta\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda de relaciones: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def add_pos_context(prompt: str, graph_search_instance) -> str:\n",
        "    \"\"\"\n",
        "    Toma el prompt, realiza un an√°lisis POS para agregar entidades al contexto y busca relaciones con esas entidades.\n",
        "\n",
        "    :param prompt: Texto de entrada del usuario.\n",
        "    :param graph_search_instance: Instancia de GraphSearch que se utilizar√° para buscar relaciones.\n",
        "    :return: Texto con contexto adicional de las palabras clave y relaciones encontradas.\n",
        "    \"\"\"\n",
        "    # Cargar modelo de spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Procesar el texto y extraer palabras clave con POS\n",
        "    doc = nlp(prompt)\n",
        "    pos_words = [token.text for token in doc if token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"}]\n",
        "    # Agregar contexto para cada palabra clave y realizar la b√∫squeda de relaciones\n",
        "    pos_context = \"\"\n",
        "    for pos_word in pos_words:\n",
        "        # Buscar relaciones en Neo4j que contienen la palabra clave en su nombre\n",
        "        relations = graph_search_instance.search_relations_by_pos(pos_word)\n",
        "        if relations:\n",
        "            pos_context += f\"Found the following relationships containing '{pos_word}': {relations}. \"\n",
        "\n",
        "    # Verifica el contexto POS generado\n",
        "    return pos_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgHZsFxGvDQ"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Inicializar el cliente de Neo4j\n",
        "graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Instancia de la clase GraphSearch\n",
        "graph_search_instance = GraphSearch(graph_client=graph_client, api_key=\"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\")\n",
        "\n",
        "prompt = \"Who created The White Castle??\"\n",
        "pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "print(\"Contexto POS:\", pos_context)\n",
        "\n",
        "# Realizar b√∫squeda en el grafo\n",
        "results = graph_search_instance.graph_search(prompt, pos_context)\n",
        "print(\"Resultados de la b√∫squeda:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kt-mzmWhbqB"
      },
      "source": [
        "## Clasificador Basado en LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "g-sMIbvRZGUS"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen2.5-72B-Instruct\", context=None):\n",
        "        \"\"\"\n",
        "        Inicializa el clasificador con un modelo de Hugging Face y un contexto base.\n",
        "\n",
        "        :param model_name: Nombre del modelo pre-entrenado en Hugging Face.\n",
        "        :param context: Contexto inicial que describe las bases de datos.\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = \"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\"  # Tu token de autenticaci√≥n\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.model_name = model_name\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "\n",
        "\n",
        "        # Usar el cliente de inferencia de Hugging Face\n",
        "        self.qwen_client = InferenceClient(api_key=self.api_key)\n",
        "\n",
        "        # Contexto configurable con valor por defecto\n",
        "        self.context = context or self._default_context()\n",
        "\n",
        "        # Definici√≥n de las categor√≠as\n",
        "        self.labels = [\"Documents\", \"Graph\", \"Table\"]\n",
        "\n",
        "    def _default_context(self):\n",
        "        return (\n",
        "            \"Documents: This category is for any question regarding the rules, strategies, and textual information of the game.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'What are the rules?' => 'Documents'\\n\"\n",
        "            \"  - 'How do you play the game?' => 'Documents'\\n\\n\"\n",
        "\n",
        "            \"Graph: This category is for questions related to game creators, designers, and interactions between people.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'Who designed the game?' => 'Graph'\\n\"\n",
        "            \"  - 'What is the connection between the game designers?' => 'Graph'\\n\\n\"\n",
        "\n",
        "            \"Table: This category includes specific data about the game, such as number of players, price, and other game statistics.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'How much does the game cost?' => 'Table'\\n\"\n",
        "            \"  - 'How long does the game last?' => 'Table'\\n\\n\"\n",
        "        )\n",
        "\n",
        "    def clasificar(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Clasifica un prompt en una de las categor√≠as: 'Documents', 'Graph' o 'Table'.\n",
        "\n",
        "        :param prompt: Consulta del usuario en texto.\n",
        "        :return: Etiqueta de clasificaci√≥n con mayor probabilidad.\n",
        "        \"\"\"\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"El prompt proporcionado est√° vac√≠o.\")\n",
        "\n",
        "        # Combinar contexto y prompt\n",
        "        input_text = f\"{self.context} Prompt: {prompt}\"\n",
        "\n",
        "        try:\n",
        "            # Llamar a Qwen para obtener la clasificaci√≥n (utilizando InferenceClient)\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a classifier that categorizes queries into Documents, Graph, or Table.\"\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": input_text\n",
        "                }],\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "                max_tokens=100  # Limitamos el tama√±o de la respuesta\n",
        "            )\n",
        "\n",
        "            # El modelo Qwen proporcionar√° una respuesta de clasificaci√≥n\n",
        "            classification = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Aqu√≠ asumimos que el modelo devolver√° una respuesta adecuada\n",
        "            return classification\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al clasificar el prompt: {e}\")\n",
        "            return \"unknown\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "-AWBLGaHAJPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb31ba98-7d27-4bd9-a1ba-b31f0487ac3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the rules of the game?\n",
            "Categor√≠a clasificada: Documents\n",
            "\n",
            "Prompt: Who designed the game?\n",
            "Categor√≠a clasificada: Graph\n",
            "\n",
            "Prompt: How much does the game cost?\n",
            "Categor√≠a clasificada: Table\n",
            "\n",
            "Prompt: What is the connection between the game designers?\n",
            "Categor√≠a clasificada: Graph\n",
            "\n",
            "Prompt: How long does the game last?\n",
            "Categor√≠a clasificada: Table\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear una instancia del clasificador\n",
        "classifier = Classifier()\n",
        "\n",
        "# Listado de prompts a probar\n",
        "prompts = [\n",
        "    \"What are the rules of the game?\",  # Deber√≠a ser clasificado como 'Documents'\n",
        "    \"Who designed the game?\",          # Deber√≠a ser clasificado como 'Graph'\n",
        "    \"How much does the game cost?\",    # Deber√≠a ser clasificado como 'Table'\n",
        "    \"What is the connection between the game designers?\",  # Deber√≠a ser 'Graph'\n",
        "    \"How long does the game last?\"     # Deber√≠a ser 'Table'\n",
        "]\n",
        "\n",
        "# Probar clasificaci√≥n para cada prompt\n",
        "for prompt in prompts:\n",
        "    category = classifier.clasificar(prompt)\n",
        "    print(f\"Prompt: {prompt}\\nCategor√≠a clasificada: {category}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfWzpA1ivFW"
      },
      "source": [
        "# ***The White Castle Chatbot***\n",
        "## Chat whith an expert in the famous board game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "vTIsThbtReG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a40d52-e24b-4cfb-cb54-06a876b5648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who designed this game?\n",
            "Categor√≠a del prompt: Graph\n",
            "Respuesta generada: The designers of The White Castle are Isra and Shei.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who covered the art of the game?\n",
            "Categor√≠a del prompt: Graph\n",
            "\n",
            "This question is about the person or people who created or designed the art for the game, which falls under the category of interactions between people involved in the game's creation.\n",
            "Respuesta generada: The art of The White Castle is covered by an artist named Kwanchai Moriuchi.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: hay algun otro artista detras del juego\n",
            "Categor√≠a del prompt: Graph\n",
            "Respuesta generada: El artista detr√°s de El Castillo Blanco es Joan Guardiet.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: what is the rating of this game?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: df['Rating']\n",
            "Respuesta generada: The rating of this game is 8.0.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: cuanto sale el juego?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: Price (USD)\n",
            "Error al realizar la b√∫squeda tabular: name 'Price' is not defined\n",
            "Respuesta generada: El precio de The White Castle no se especifica en la informaci√≥n proporcionada.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: how much does it cost?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: df['Price (USD)']\n",
            "Respuesta generada: 26.31\n",
            "    \n",
            "    Question: what are the available colors for the pawns?\n",
            "\n",
            "    Retrieved Information: \n",
            "    La lista de colores disponibles para las figuritas es la siguiente:\n",
            "    - Rojo\n",
            "    - Azul\n",
            "    - Verde\n",
            "    - Amarillo\n",
            "    - Negro\n",
            "    \n",
            "    Please provide only the available colors, no extra steps or explanations.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: me gustaria aprender a jugar me podrias dar las reglas basicas para comenzar a jugar?\n",
            "Categor√≠a del prompt: Documents\n",
            "Respuesta generada: Para jugar a *The White Castle*, cada jugador comienza con un personaje y un conjunto de recursos. Tu objetivo es llegar al Castillo Blanco y reclamarlo como tuyo. En el camino, te encontrar√°s con obst√°culos y otros jugadores. El combate se resuelve tirando los dados y comparando los resultados. Algunas √°reas requieren que se lance un n√∫mero espec√≠fico para continuar. Los recursos se pueden gastar para obtener ventajas o para curar a tu personaje. El primer jugador en llegar al Castillo Blanco y reclamarlo gana la partida.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: I am experience in the game but not an expert, could you give me some tips to improve in this game, i mean what rules or mechanics do i have to improve in order to become a better player\n",
            "Categor√≠a del prompt: Documents\n",
            "\n",
            "This query is seeking advice and tips on how to improve gameplay, which involves understanding the rules and mechanics of the game. Therefore, it falls under the \"Documents\" category.\n",
            "Respuesta generada: To enhance your gameplay in *The White Castle*, you may want to focus on mastering the combat system, as it plays a significant role in the game. Additionally, paying close attention to resource management and strategy development can also help you improve. Keep practicing and studying the game to continue your growth as a player.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: exit\n",
            "¬°Hasta luego!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from langdetect import detect\n",
        "from jinja2 import Template\n",
        "\n",
        "chat_graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para traducir un texto a ingl√©s\n",
        "def traducir_a_espa√±ol(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='en', to_language='es') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "\n",
        "# Funci√≥n para detectar el idioma del texto\n",
        "def detect_language(text):\n",
        "    return detect(text)\n",
        "\n",
        "# Funci√≥n para generar el template del chat\n",
        "def zephyr_chat_template(messages, add_generation_prompt=True):\n",
        "    template_str  = \"{% for message in messages %}\"\n",
        "    template_str += \"{% if message['role'] == 'user' %}\"\n",
        "    template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
        "    template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'system' %}\"\n",
        "    template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% else %}\"\n",
        "    template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "    template_str += \"{% endfor %}\"\n",
        "    template_str += \"{% if add_generation_prompt %}\"\n",
        "    template_str += \"<|assistant|>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "\n",
        "    # Crear un objeto de plantilla con la cadena de plantilla\n",
        "    template = Template(template_str)\n",
        "\n",
        "    # Renderizar la plantilla con los mensajes proporcionados\n",
        "    return template.render(messages=messages, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "# Funci√≥n para obtener el modelo generativo (en este caso usando Zephyr)\n",
        "def generate_response_with_model(context):\n",
        "    api_key = \"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\"\n",
        "\n",
        "    api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    data = {\n",
        "        \"inputs\": context,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 256,\n",
        "            \"temperature\": 0.68,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        if isinstance(result, list):\n",
        "            return result[0].get('generated_text', 'No se gener√≥ texto.')\n",
        "        else:\n",
        "            return \"Error en la respuesta del modelo: la respuesta no es una lista.\"\n",
        "    else:\n",
        "        return f\"Error en la solicitud: {response.status_code} - {response.text}\"\n",
        "\n",
        "game_state = \"\"\"\n",
        "    You are a chatbot specialized in the famous board game *The White Castle*.\n",
        "    You may want to think and process step by step the information that you have before yoy respond\n",
        "    Your task is to generate responses based on the user's question and the relevant information retrieved from the database.\n",
        "    You should take into account the question, the retrieved information, and the context to provide a detailed and accurate response.\n",
        "    You will rearly recive the exact information to the question but you have to formulate your answer based on what you know.\n",
        "    For instance you may no be provided with the entire rulebook but you can say \"Some of the rules consist of ...\"\n",
        "    Your answers should be clear, concise, and directly related to the game, *The White Castle* and you dont hace to cite any retrieved information, take it as if you already know it.\n",
        "    \"\"\"\n",
        "loop_flag = True\n",
        "while loop_flag:\n",
        "    # Paso 1: Obtener el prompt del usuario\n",
        "    user_prompt = input(\" (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: \")\n",
        "    if user_prompt.lower() == \"exit\":\n",
        "        print(\"¬°Hasta luego!\")\n",
        "        break\n",
        "    esp_flag = False\n",
        "\n",
        "    # Detectar el idioma del texto\n",
        "    if detect_language(user_prompt) == \"es\":\n",
        "        user_prompt = ts.translate_text(user_prompt, translator='bing', from_language='es', to_language='en')\n",
        "        esp_flag = True\n",
        "\n",
        "    # Paso 2: Clasificar el prompt\n",
        "    classifier = Classifier()\n",
        "    category = classifier.clasificar(user_prompt)\n",
        "    print(f\"Categor√≠a del prompt: {category}\")\n",
        "\n",
        "    # Paso 3: Recuperar la informaci√≥n basada en la clasificaci√≥n\n",
        "    if category == \"Documents\":\n",
        "        doc_search = DocSearch(client_castle, collection)\n",
        "        retrieved_info = doc_search.hybrid_search(user_prompt)\n",
        "    elif category == \"Graph\":\n",
        "        graph_search = GraphSearch(graph_client=chat_graph_client, api_key=\"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\")\n",
        "        pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "        # Realizar b√∫squeda en el grafo\n",
        "        retrieved_info = graph_search.graph_search(user_prompt, pos_context)\n",
        "    elif category == \"Table\":\n",
        "        tabular_search = TabularSearch(df_castle)\n",
        "        retrieved_info = tabular_search.tabular_search(user_prompt)\n",
        "    else:\n",
        "        retrieved_info = \"No se pudo clasificar la consulta adecuadamente.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Ajusta el contexto para que sea relevante y claro\n",
        "    context = f\"\"\"\n",
        "    Role: {game_state}\n",
        "    Question: {user_prompt}\n",
        "\n",
        "    Retrieved Information: {retrieved_info}\n",
        "\n",
        "    Please provide only the direct answer, no extra steps or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Paso 5: Generar respuesta con un modelo generativo (Zephyr)\n",
        "    response = generate_response_with_model(context)\n",
        "\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\")[1].strip()\n",
        "    elif \"Response:\" in response:\n",
        "        answer = response.split(\"Response:\")[1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "\n",
        "    # Si el texto original estaba en espa√±ol, traducimos la respuesta generada al espa√±ol\n",
        "    if esp_flag:\n",
        "        answer = traducir_a_espa√±ol(answer)\n",
        "    print(f\"Respuesta generada: {answer}\")\n",
        "    print(f\"{'-' * 50}  \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cClPcYLfIXa"
      },
      "source": [
        "# ReAct Agent\n",
        "\n",
        "Este ejercicio se basa en el ejercicio 1, e incorpora el concepto de Agente, basado en el concepto ReAct. Nuestro agente debe cumplir con los siguientes requisitos:\n",
        "Utilizar al menos 3 herramientas, aprovechando el trabajo anterior:\n",
        "- doc_search(): Busca informaci√≥n en los documentos\n",
        "- graph_search(): Busca informaci√≥n en la base de datos de grafos\n",
        "- table_search(): Busca informaci√≥n sobre los datos tabulares\n",
        "\n",
        "Se puede implementar alguna nueva herramienta que se considere necesaria y que pueda enriquecer las capacidades del agente.\n",
        "Utilizar la librer√≠a Llama-Index para desarrollar el agente:\n",
        "**llama_index.core.agent.ReActAgent**\n",
        "**llama_index.core.tools.FunctionTool**\n",
        "Se debe construir el prompt adecuado para incorporar las herramientas al agente ReAct\n",
        "### Presentar en el informe los resultados:\n",
        "1. Presentar 5 ejemplos de prompts donde se deba recurrir a m√°s de una herramienta para responder al usuario. Evaluar los resultados obtenidos\n",
        "3. Explicar con 3 ejemplos, donde el agente falla o las respuestas no son precisas.\n",
        "4. Explicar cu√°les son las mejoras que ser√≠a conveniente realizar para mejorar los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDOyNEgpwPy8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!ollama pull phi3:medium > ollama.loga\n",
        "\n",
        "!pip install litellm[proxy]\n",
        "!nohup litellm --model ollama/phi3:medium --port 8000 > litellm.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUpS12Xk3IWz"
      },
      "outputs": [],
      "source": [
        "def doc_search(query):\n",
        "    \"\"\"\n",
        "    Simula una b√∫squeda en documentos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la b√∫squeda (simulados).\n",
        "    \"\"\"\n",
        "    # Simulamos que encontramos un documento relevante\n",
        "    return [{\n",
        "        \"document\": f\"Informaci√≥n relevante sobre el tema '{query}' en los documentos.\",\n",
        "        \"metadata\": {\"source\": \"document_1.pdf\", \"date\": \"2023-01-01\"},\n",
        "        \"score\": 0.95\n",
        "    }]\n",
        "\n",
        "def graph_search(query):\n",
        "    \"\"\"\n",
        "    Simula una b√∫squeda en la base de datos de grafos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la b√∫squeda (simulados).\n",
        "    \"\"\"\n",
        "    return [{\n",
        "        \"creator\": {\"name\": \"Israel Cendrero\"}\n",
        "    }]\n",
        "\n",
        "def table_search(query):\n",
        "    \"\"\"\n",
        "    Simula una b√∫squeda en una tabla de datos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la b√∫squeda (simulados).\n",
        "    \"\"\"\n",
        "    # Simulamos que encontramos informaci√≥n tabular\n",
        "    return pd.DataFrame({\n",
        "        \"Player\": [\"Player 1\", \"Player 2\"],\n",
        "        \"Score\": [50, 45]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWxcUuxfwJ2n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class AgentPipeline:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el flujo entre el chatbot, herramientas de b√∫squeda y el agente ReAct.\n",
        "    \"\"\"\n",
        "    def __init__(self, doc_search, graph_search, table_search, llm_model=\"phi3:medium\", temperature=0.2):\n",
        "        self.doc_search = doc_search\n",
        "        self.graph_search = graph_search\n",
        "        self.table_search = table_search\n",
        "\n",
        "        # Inicializar las herramientas para el agente ReAct\n",
        "        self.tools = [\n",
        "            FunctionTool.from_defaults(fn=doc_search, description=\"Busca informaci√≥n en documentos.\"),\n",
        "            FunctionTool.from_defaults(fn=graph_search, description=\"Busca relaciones en bases de datos de grafos.\"),\n",
        "            FunctionTool.from_defaults(fn=table_search, description=\"Busca informaci√≥n en datos tabulares.\"),\n",
        "        ]\n",
        "\n",
        "        # Configuraci√≥n del modelo LLM (phi3:medium de Ollama)\n",
        "        self.llm = Ollama(model=llm_model, temperature=temperature)\n",
        "\n",
        "        # Configurar el sistema de agentes\n",
        "        self.system_prompt = \"\"\"\n",
        "        Eres un agente experto que responde preguntas complejas utilizando herramientas.\n",
        "        Tienes acceso a las siguientes herramientas:\n",
        "        1. doc_search: Busca informaci√≥n en documentos extensos.\n",
        "        2. graph_search: Busca relaciones en una base de datos de grafos.\n",
        "        3. table_search: Busca datos espec√≠ficos en tablas estructuradas.\n",
        "\n",
        "        Para cada pregunta:\n",
        "        1. Analiza qu√© informaci√≥n necesitas.\n",
        "        2. Usa las herramientas con el siguiente formato:\n",
        "\n",
        "        Thought: Pienso en lo que necesito buscar.\n",
        "        Action: [nombre de la herramienta]\n",
        "        Action Input: [entrada para la herramienta]\n",
        "\n",
        "        Observation: [resultado de la herramienta]\n",
        "        Final Answer: [respuesta final]\n",
        "        \"\"\"\n",
        "\n",
        "        # Crear el agente ReAct\n",
        "        self.agent = ReActAgent.from_tools(\n",
        "            tools=self.tools,\n",
        "            llm=self.llm,\n",
        "            system_prompt=self.system_prompt,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def process_query(self, user_query):\n",
        "        \"\"\"\n",
        "        Procesa la consulta del usuario utilizando las herramientas de b√∫squeda.\n",
        "\n",
        "        :param user_query: Consulta proporcionada por el usuario.\n",
        "        :return: Respuesta generada para el usuario.\n",
        "        \"\"\"\n",
        "        response = self.agent.chat(user_query)\n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "LoPI6yJ3ML_D",
        "outputId": "838789db-11de-48a4-a6e6-976a01c55329"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-85-5605d1378c5d>, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-85-5605d1378c5d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    @misc{qwen2.5,\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "#@title Citas\n",
        "@misc{qwen2.5,\n",
        "    title = {Qwen2.5: A Party of Foundation Models},\n",
        "    url = {https://qwenlm.github.io/blog/qwen2.5/},\n",
        "    author = {Qwen Team},\n",
        "    month = {September},\n",
        "    year = {2024}\n",
        "}\n",
        "\n",
        "@article{qwen2,\n",
        "    title={Qwen2 Technical Report},\n",
        "    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n",
        "    journal={arXiv preprint arXiv:2407.10671},\n",
        "    year={2024}\n",
        "}\n",
        "\n",
        "@misc{intfloat2023e5,\n",
        "    title = {Multilingual E5: A Text Embedding Model for Retrieval Tasks},\n",
        "    author = {Intfloat Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/intfloat/multilingual-e5-small}}\n",
        "}\n",
        "\n",
        "\n",
        "@misc{bge2023reranker,\n",
        "    title = {BAAI General Embedding Reranker v2 (BGE ReRanker)},\n",
        "    author = {BAAI Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/BAAI/bge-reranker-v2-m3}}\n",
        "}\n",
        "\n",
        "@misc{phi3ollama2024,\n",
        "    title = {Phi-3: A Series of Lightweight Language Models},\n",
        "    author = {Ollama Team},\n",
        "    year = {2024},\n",
        "    howpublished = {\\url{https://ollama.ai/library/phi3}}\n",
        "}\n",
        "\n",
        "@misc{distilbert2019,\n",
        "    title = {DistilBERT: A distilled version of BERT for faster NLP tasks},\n",
        "    author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n",
        "    year = {2019},\n",
        "    journal = {arXiv preprint arXiv:1910.01108},\n",
        "    url = {https://huggingface.co/distilbert-base-uncased}\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "laUFu54--eTf",
        "2UZAbT5Vc2kB",
        "ZjhPfbqfBX8L",
        "UtNynjmUMVr3",
        "9IomKasVMiEJ",
        "7qPW3Uj9MkTG",
        "1Kf-lcXlhi8n",
        "6GPqLVthBGho",
        "VRqtXwrAY-ZO",
        "eYHHExIeZDx2",
        "3Kt-mzmWhbqB"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}