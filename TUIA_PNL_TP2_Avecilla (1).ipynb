{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaeo3m6civnc"
      },
      "source": [
        "# **Trabajo Práctico Final - TUIA NLP 2024**\n",
        "\n",
        "## **Chatbot experto en Eurogames con RAG y Agentes ReAct**\n",
        "\n",
        "### **Autor**: Tomás Valentino Avecilla\n",
        "### **legajo**: --------\n",
        "### **Fecha**: 16 de diciembre de 2024\n",
        "### **Materia**: Procesamiento del Lenguaje Natural (NLP)  \n",
        "### **Institución**: Facultad de Ciencias Exactas, ingenieria y Agrimensura -- UNR\n",
        "\n",
        "---\n",
        "\n",
        "## **Descripción del Trabajo**\n",
        "Este cuaderno contiene el desarrollo del Trabajo Práctico Final para la materia TUIA NLP 2024. El objetivo es implementar un **chatbot experto** sobre un juego de mesa tipo Eurogame, utilizando las técnicas de **Retrieval-Augmented Generation (RAG)** y **Agentes ReAct**.\n",
        "\n",
        "El proyecto incluye:\n",
        "- Recolección de información desde múltiples fuentes: texto, datos tabulares y bases de grafos.\n",
        "- Construcción de bases de datos vectoriales, tabulares y de grafos.\n",
        "- Implementación de clasificadores y sistemas de recuperación dinámica de información.\n",
        "- Desarrollo de un agente que combina herramientas para responder consultas complejas.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laUFu54--eTf"
      },
      "source": [
        "## Preparación del Entorno de Trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "EF0P5Oln-rDu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# --- 1. Actualización del Sistema ---\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-spa tesseract-ocr-eng  # Herramientas de OCR y dependencias\n",
        "!pip install python-decouple\n",
        "\n",
        "# --- 2. Instalación de Bibliotecas Generales ---\n",
        "!pip install gdown requests python-docx  # Descarga de archivos, solicitudes web, manejo de archivos docx\n",
        "\n",
        "# --- 3. Procesamiento de Imágenes y OCR ---\n",
        "!pip install pdf2image pytesseract  # Extracción de imágenes y OCR desde PDFs\n",
        "\n",
        "# --- 4. Web Scraping y Automatización ---\n",
        "!pip install selenium webdriver-manager  # Automatización de navegación web\n",
        "\n",
        "# --- 5. Procesamiento del Lenguaje Natural ---\n",
        "!pip install transformers  # Modelos de Hugging Face y Sentence Transformers\n",
        "!pip install --upgrade sentence_transformers\n",
        "!python -m spacy download es_core_news_md en_core_web_sm  # Modelo en español para spaCy\n",
        "!pip install translators  # Traducción automática de texto\n",
        "!pip install langdetect\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "!pip install --upgrade chromadb neo4j pydgraph  # Bases de datos vectoriales, de grafos y almacenamiento\n",
        "\n",
        "# --- 7. Modelos de Machine Learning y Deep Learning ---\n",
        "!pip install torch\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install rank_bm25\n",
        "\n",
        "# --- 8. Herramientas para Agentes ReAct ---\n",
        "!pip install llama-index-llms-ollama llama-index pygoogleweather wikipedia  # Agentes y conectores para datos externos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Ym2dmNZf-jDo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# --- 1. Importaciones Básicas y Manejo de Archivos ---\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import uuid\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import logging\n",
        "from time import time\n",
        "import json\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import docx\n",
        "from docx import Document\n",
        "\n",
        "# --- 2. Procesamiento de Imágenes y OCR ---\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "# --- 3. Web Scraping ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# --- 4. Procesamiento del Lenguaje Natural ---\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import translators as ts\n",
        "from langdetect import detect\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- 5. Modelos & Embeddings ---\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from huggingface_hub import InferenceClient\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "import chromadb  # Base de datos vectorial\n",
        "from neo4j import GraphDatabase  # Base de datos de grafos\n",
        "\n",
        "# --- 7. Agentes y Herramientas ReAct ---\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent.react.formatter import ReActChatFormatter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZAbT5Vc2kB"
      },
      "source": [
        "## Recolección de Información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BluNQKmNhYi4"
      },
      "source": [
        "### **1:** 🎮 Reglas y Jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2YVnGhEzNQ0"
      },
      "source": [
        "#### Archivos Descargados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzfzPLRm9Gh5"
      },
      "source": [
        "Primero vamos a usar tres documentos descargados de la seccion ***files*** del sitio BGG _[link](https://boardgamegeek.com/boardgame/371942/the-white-castle/files)_\n",
        "Contamos con 3 documentos PDF\n",
        "\n",
        "\n",
        "1.   Reglamento en Español\n",
        "2.   Reglamento en Ingles\n",
        "3.   Guia Rapida en ingles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGBUyJJX9jHv",
        "outputId": "f5c7c305-ec00-4a6f-a137-183a34f2a567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\n",
            "To: /content/reglamento_en.pdf\n",
            "100% 13.2M/13.2M [00:00<00:00, 23.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\n",
            "To: /content/qs_en.pdf\n",
            "100% 446k/446k [00:00<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\" --output \"reglamento_en.pdf\"\n",
        "!gdown \"1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\" --output \"qs_en.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XvjR5eP_NiT"
      },
      "source": [
        "Los 3 PDFs son imagenes por lo cual vamos a tener que extraer el texto y para eso usaremos un ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th1OJLibBFIB"
      },
      "outputs": [],
      "source": [
        "# Función para crear carpetas si no existen\n",
        "def crear_carpeta(nombre_carpeta):\n",
        "    if not os.path.exists(nombre_carpeta):\n",
        "        os.makedirs(nombre_carpeta)\n",
        "\n",
        "# Carpetas para guardar las imágenes\n",
        "carpeta_en = \"imgs_reglamento_en\"\n",
        "carpeta_qs = \"imgs_qs_en\"\n",
        "\n",
        "# Crear carpetas\n",
        "crear_carpeta(carpeta_en)\n",
        "crear_carpeta(carpeta_qs)\n",
        "\n",
        "# Convertir PDFs en listas de imágenes\n",
        "imgs_reglamento_en = convert_from_path(\"reglamento_en.pdf\")\n",
        "imgs_qs_en = convert_from_path(\"qs_en.pdf\")\n",
        "\n",
        "# Guardar las imágenes en sus carpetas correspondientes\n",
        "for i, imagen in enumerate(imgs_reglamento_en):\n",
        "    imagen.save(os.path.join(carpeta_en, f'pagina_en_{i + 1}.png'), 'PNG')\n",
        "\n",
        "for i, imagen in enumerate(imgs_qs_en):\n",
        "    imagen.save(os.path.join(carpeta_qs, f'qs_en_{i + 1}.png'), 'PNG')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUn_5YrLCKpO",
        "outputId": "36cf0b6c-2b6b-46e7-eb8b-cc2e060dfe4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto procesado y guardado en: texto_reglamento_en\n",
            "Texto procesado y guardado en: texto_qs_en\n"
          ]
        }
      ],
      "source": [
        "# Configurar pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "# Función para procesar imágenes y extraer texto\n",
        "def procesar_imagenes(carpeta_imagenes, carpeta_salida, idioma=\"eng\"):\n",
        "    # Crear carpeta de salida si no existe\n",
        "    if not os.path.exists(carpeta_salida):\n",
        "        os.makedirs(carpeta_salida)\n",
        "\n",
        "    # Iterar sobre las imágenes en la carpeta\n",
        "    for imagen_nombre in sorted(os.listdir(carpeta_imagenes)):  # Ordenar para mantener secuencia\n",
        "        if imagen_nombre.endswith(\".png\"):\n",
        "            ruta_imagen = os.path.join(carpeta_imagenes, imagen_nombre)\n",
        "\n",
        "            # Extraer texto de la imagen\n",
        "            texto = pytesseract.image_to_string(Image.open(ruta_imagen), lang=idioma)\n",
        "\n",
        "            # Guardar texto en un archivo\n",
        "            nombre_txt = os.path.splitext(imagen_nombre)[0] + \".txt\"\n",
        "            ruta_salida = os.path.join(carpeta_salida, nombre_txt)\n",
        "\n",
        "            with open(ruta_salida, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(texto)\n",
        "\n",
        "    print(f\"Texto procesado y guardado en: {carpeta_salida}\")\n",
        "\n",
        "# Procesar las imágenes de cada carpeta\n",
        "procesar_imagenes(\"imgs_reglamento_en\", \"texto_reglamento_en\", idioma=\"eng\")\n",
        "procesar_imagenes(\"imgs_qs_en\", \"texto_qs_en\", idioma=\"eng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeLxKhjxvhwD",
        "outputId": "fdb10be4-ff58-46c6-c88f-f5f483713968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos creados en: documentos\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Función avanzada para limpiar texto y normalizarlo\n",
        "def limpiar_texto(texto):\n",
        "\n",
        "    # Paso 1: Normalizar texto a Unicode estándar (NFKC)\n",
        "    texto = unicodedata.normalize(\"NFKC\", texto)\n",
        "\n",
        "    # Paso 2: Eliminar múltiples espacios, tabulaciones y líneas redundantes\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    texto = re.sub(r'[^a-zA-Z0-9\\s.,!?¿¡]', '', texto)\n",
        "\n",
        "    # Paso 3: Convertir a minúsculas para uniformidad semántica\n",
        "    texto = texto.lower()\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "# Función para crear documentos a partir de los archivos .txt en cada carpeta\n",
        "def crear_documentos(carpeta_lista, carpeta_destino_docs):\n",
        "    # Crear carpeta de destino para los documentos si no existe\n",
        "    if not os.path.exists(carpeta_destino_docs):\n",
        "        os.makedirs(carpeta_destino_docs)\n",
        "\n",
        "    # Iterar sobre las carpetas en la lista proporcionada\n",
        "    for carpeta in sorted(carpeta_lista):  # Ordenar para mantener consistencia\n",
        "        if os.path.isdir(carpeta):  # Procesar solo carpetas\n",
        "            # Crear un documento Word\n",
        "            doc = Document()\n",
        "\n",
        "            # Iterar sobre los archivos .txt en la carpeta\n",
        "            for archivo_txt in sorted(os.listdir(carpeta)):  # Ordenar para mantener secuencia\n",
        "                if archivo_txt.endswith(\".txt\"):\n",
        "                    ruta_txt = os.path.join(carpeta, archivo_txt)\n",
        "\n",
        "                    with open(ruta_txt, \"r\", encoding=\"utf-8\") as file:\n",
        "                        texto = file.read()\n",
        "\n",
        "                    # Limpiar el texto antes de agregarlo al documento\n",
        "                    texto = limpiar_texto(texto)\n",
        "\n",
        "                    doc.add_paragraph(texto)\n",
        "\n",
        "            # Guardar el documento en la carpeta de destino\n",
        "            nombre_doc = f\"{os.path.basename(carpeta)}.docx\"\n",
        "            ruta_doc = os.path.join(carpeta_destino_docs, nombre_doc)\n",
        "            doc.save(ruta_doc)\n",
        "\n",
        "    print(f\"Documentos creados en: {carpeta_destino_docs}\")\n",
        "\n",
        "# Lista de carpetas y carpeta de destino\n",
        "carpetas = [\"texto_reglamento_en\", \"texto_qs_en\"]\n",
        "carpeta_destino_docs = \"documentos\"\n",
        "\n",
        "# Crear documentos\n",
        "crear_documentos(carpetas, carpeta_destino_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdWgfoZPzP56"
      },
      "source": [
        "#### Scrapping de jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ7yO2an5lZX"
      },
      "source": [
        "Haremos un scrapping de la siguiente [reseña](https://misutmeeple.com/2023/11/resena-the-white-castle/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKOKbAVQ1waB",
        "outputId": "39d73380-d5bc-4118-bc37-1b6bd6f0863d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como: reseña.docx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# URL de la página que deseas scrapear\n",
        "url = 'https://misutmeeple.com/2023/11/resena-the-white-castle/'\n",
        "\n",
        "# Hacer la solicitud HTTP para obtener el HTML de la página\n",
        "response = requests.get(url)\n",
        "html = response.text\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Seleccionar el div con la clase específica\n",
        "div = soup.find('div', class_='entry-content single-content')\n",
        "\n",
        "# Asegurarse de que el div existe antes de continuar\n",
        "if div:\n",
        "    # Crear un nuevo documento Word\n",
        "    doc = Document()\n",
        "\n",
        "    # Iterar sobre los elementos h2, h3 y p dentro del div\n",
        "    for elemento in div.find_all(['h2', 'h3', 'p'], recursive=True):\n",
        "        etiqueta = elemento.name\n",
        "        texto = elemento.get_text(strip=True)\n",
        "\n",
        "        # Agregar texto al documento según el tipo de etiqueta\n",
        "        if etiqueta == 'h2':\n",
        "            doc.add_heading(texto, level=1)\n",
        "        elif etiqueta == 'h3':\n",
        "            doc.add_heading(texto, level=2)\n",
        "        elif etiqueta == 'p':\n",
        "            doc.add_paragraph(texto)\n",
        "\n",
        "    # Guardar el documento Word\n",
        "    doc_name = \"reseña.docx\"\n",
        "    doc.save(doc_name)\n",
        "\n",
        "    print(f\"Documento guardado como: {doc_name}\")\n",
        "else:\n",
        "    print(\"Error.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7EsuI7g9hG"
      },
      "source": [
        "### **2:** 🏯 The White Castle Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68I7kA99MF9g"
      },
      "source": [
        "##### Configuracion de drivers para no generar conflictos en el uso de selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCbsuXOtjIJ"
      },
      "source": [
        "Nos aseguramos de que el sistema tiene las bibliotecas necesarias para ejecutar aplicaciones gráficas (como Chrome) en un entorno Linux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckhcQr9tmly",
        "outputId": "c6742294-dcf8-42b2-9435-72005308b261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.19).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxi6 set to manually installed.\n",
            "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-dev set to manually installed.\n",
            "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-xcb1 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  gconf-service gconf-service-backend gconf2-common libdbus-glib-1-2\n",
            "The following NEW packages will be installed:\n",
            "  gconf-service gconf-service-backend gconf2-common libdbus-glib-1-2 libgconf-2-4 libglu1-mesa\n",
            "0 upgraded, 6 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 1,071 kB of archives.\n",
            "After this operation, 8,675 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdbus-glib-1-2 amd64 0.112-2build1 [65.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf2-common all 3.2.6-7ubuntu2 [698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgconf-2-4 amd64 3.2.6-7ubuntu2 [86.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service-backend amd64 3.2.6-7ubuntu2 [59.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service amd64 3.2.6-7ubuntu2 [17.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Fetched 1,071 kB in 2s (583 kB/s)\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 123714 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdbus-glib-1-2_0.112-2build1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Selecting previously unselected package gconf2-common.\n",
            "Preparing to unpack .../1-gconf2-common_3.2.6-7ubuntu2_all.deb ...\n",
            "Unpacking gconf2-common (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package libgconf-2-4:amd64.\n",
            "Preparing to unpack .../2-libgconf-2-4_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service-backend.\n",
            "Preparing to unpack .../3-gconf-service-backend_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service.\n",
            "Preparing to unpack .../4-gconf-service_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../5-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up gconf2-common (3.2.6-7ubuntu2) ...\n",
            "\n",
            "Creating config file /etc/gconf/2/path with new version\n",
            "Setting up libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Actualizar los repositorios\n",
        "!apt-get update\n",
        "!apt-get install -y wget curl unzip\n",
        "!apt-get install -y libx11-dev libx11-xcb1 libglu1-mesa libxi6 libgconf-2-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BD-ThPEtqiU"
      },
      "source": [
        "Descargamos e instalamos Google Chrome para poder usarlo con Selenium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ZiLbvwtsUD",
        "outputId": "22ab6a18-3210-490d-b5a0-28e598bc8a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-15 21:30:57--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.175.136, 142.251.175.190, 142.251.175.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.175.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112421156 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.21M   235MB/s    in 0.5s    \n",
            "\n",
            "2024-12-15 21:30:58 (235 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [112421156/112421156]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123883 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (131.0.6778.139-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 51 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 2s (4,764 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 124000 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (131.0.6778.139-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalar Google Chrome (última versión estable)\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt --fix-broken install -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctEwpcXt3gt"
      },
      "source": [
        "Descargamos ChromeDriver (necesario para Selenium) y lo mueve a una ubicación accesible globalmente en el sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uIIpMdJ8j0rb",
        "outputId": "d500e203-876d-4234-9ba0-d0c4932682a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-15 21:31:17--  https://chromedriver.storage.googleapis.com/131.0.6778.87/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 172.217.194.207, 172.253.118.207, 74.125.200.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|172.217.194.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-12-15 21:31:18 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open chromedriver_linux64.zip, chromedriver_linux64.zip.zip or chromedriver_linux64.zip.ZIP.\n",
            "mv: cannot stat 'chromedriver': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wget https://chromedriver.storage.googleapis.com/131.0.6778.87/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip\n",
        "!mv chromedriver /usr/local/bin/chromedriver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qIhAs2t8mx"
      },
      "source": [
        "Configura el navegador para ejecutarse en segundo plano sin mostrar interfaz gráfica, ideal para entornos como servidores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8-Cg4Kmt9i_"
      },
      "outputs": [],
      "source": [
        "# Configurar las opciones de Chrome para usarlo en modo headless\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')  # Modo sin cabeza\n",
        "chrome_options.add_argument('--no-sandbox')  # Evitar problemas de sandboxing\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Usar en contenedores o entornos con poca memoria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H60n54f-MN3J"
      },
      "source": [
        "#### Datos Tabulares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpcGmlUy6pmu"
      },
      "source": [
        "A continuacion haremos el scrapping de la pagina [board game geek](https://boardgamegeek.com/boardgame/371942/the-white-castle/) para extraer datos numericos e insertarlos en un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V6O609PpgN6"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# URL del juego\n",
        "url = 'https://boardgamegeek.com/boardgame/371942/the-white-castle'\n",
        "\n",
        "# Abrir la página\n",
        "driver.get(url)\n",
        "\n",
        "# Esperar unos segundos para que cargue el contenido dinámico\n",
        "time.sleep(8)\n",
        "\n",
        "# Obtener el HTML completo de la página cargada\n",
        "html = driver.page_source\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Extraer los datos y guardarlos en variables\n",
        "rating_value = soup.find('span', itemprop='ratingValue')\n",
        "rating_value = rating_value.text.strip() if rating_value else 'N/A'\n",
        "\n",
        "year_section = soup.find('span', class_='game-year')\n",
        "game_year = year_section.text.strip() if year_section else 'N/A'\n",
        "game_year = game_year.strip(\"()\").strip()  # Eliminar paréntesis y espacios adicionales\n",
        "game_year = int(game_year) if game_year.isdigit() else 'N/A'\n",
        "\n",
        "review_count_section = soup.find('meta', itemprop='reviewCount')\n",
        "review_count_value = int(review_count_section['content']) if review_count_section else 'N/A'\n",
        "\n",
        "min_players = max_players = play_time = suggested_age = complexity = 'N/A'\n",
        "\n",
        "# Extraer los elementos de gameplay\n",
        "gameplay_section = soup.find('ul', class_='gameplay')\n",
        "if gameplay_section:\n",
        "    gameplay_items = gameplay_section.find_all('li', class_='gameplay-item')\n",
        "    for item in gameplay_items:\n",
        "        title = item.find('h3').text.strip() if item.find('h3') else 'N/A'\n",
        "\n",
        "        if title == \"Number of Players\":\n",
        "            min_players = int(item.find('meta', itemprop='minValue')['content']) if item.find('meta', itemprop='minValue') else 'N/A'\n",
        "            max_players = int(item.find('meta', itemprop='maxValue')['content']) if item.find('meta', itemprop='maxValue') else 'N/A'\n",
        "\n",
        "        elif title == \"Play Time\":\n",
        "            play_time = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Suggested Age\":\n",
        "            suggested_age = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Complexity\":\n",
        "            complexity = float(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "like_count_section = soup.find('a', class_='game-action-play-count')\n",
        "likes = like_count_section.text.strip() if like_count_section else 'N/A'\n",
        "\n",
        "# Extraer precios sugeridos\n",
        "prices = soup.find_all('div', class_='summary-sale-item-price')\n",
        "precio_sugerido = prices[1].text.strip() if len(prices) > 0 else 'N/A'\n",
        "\n",
        "# Cerrar el WebDriver\n",
        "driver.quit()\n",
        "\n",
        "# Crear un diccionario para los datos\n",
        "data = {\n",
        "    \"Rating\": [float(rating_value) if rating_value != 'N/A' else None],\n",
        "    \"Year\": [game_year if game_year != 'N/A' else None],\n",
        "    \"Review Count\": [review_count_value if review_count_value != 'N/A' else None],\n",
        "    \"Min Players\": [min_players if min_players != 'N/A' else None],\n",
        "    \"Max Players\": [max_players if max_players != 'N/A' else None],\n",
        "    \"Play Time (min)\": [play_time if play_time != 'N/A' else None],\n",
        "    \"Suggested Age\": [suggested_age if suggested_age != 'N/A' else None],\n",
        "    \"Complexity\": [complexity if complexity != 'N/A' else None],\n",
        "    \"Likes\": [int(likes.replace('K', '000').replace('.', '')) if 'K' in likes else int(likes) if likes != 'N/A' else None],\n",
        "    \"Price (USD)\": [float(precio_sugerido.replace(\"from $\", \"\").replace(\"$\", \"\")) if precio_sugerido != 'N/A' else None]\n",
        "}\n",
        "# Crear el DataFrame\n",
        "df_castle = pd.DataFrame(data)\n",
        "df_castle.head()\n",
        "\n",
        "# Descargar df en csv\n",
        "df_castle.to_csv('castle.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qG42vAhfh1"
      },
      "source": [
        "### **3:** 💬 Comentarios y opiniones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPtkW3iLnkT"
      },
      "source": [
        "Con el siguiente scrapping traemos a un documento todos lo comentarios en el [foro](https://boardgamegeek.com/boardgame/371942/the-white-castle/forums)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldXKjRVTiTOq",
        "outputId": "ba66b106-ad49-498b-f9b7-f7f033a1c426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como 'comentarios.docx'\n"
          ]
        }
      ],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "# Crear un nuevo documento Word\n",
        "doc = Document()\n",
        "\n",
        "# Función para scrapear los hilos individuales\n",
        "def get_thread_details(thread_url):\n",
        "    driver.get(thread_url)\n",
        "    time.sleep(3)  # Esperar a que se cargue el contenido dinámico\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Buscar los comentarios dentro de las etiquetas <gg-markup-safe-html>\n",
        "    comments = soup.find_all('gg-markup-safe-html')\n",
        "    thread_content = \"\"\n",
        "\n",
        "    for comment in comments:\n",
        "        thread_content += comment.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "    return thread_content\n",
        "\n",
        "# Loop para iterar sobre varias páginas\n",
        "for id in [1, 2, 3, 4]:\n",
        "    url = f'https://boardgamegeek.com/boardgame/371942/the-white-castle/forums/0?pageid={id}'\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Obtener el HTML completo de la página cargada\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Seleccionar todos los <li> con la clase 'summary-item ng-scope'\n",
        "    li_items = soup.find_all('li', class_='summary-item ng-scope')\n",
        "\n",
        "    # Iterar sobre cada elemento <li>\n",
        "    for li in li_items:\n",
        "        # Extraer el título\n",
        "        title = li.find('h3', class_='m-0 fs-sm text-inherit leading-inherit text-inline')\n",
        "        if title:\n",
        "            title_text = title.get_text(strip=True)\n",
        "            doc.add_paragraph(f\"Título: {title_text}\")\n",
        "\n",
        "        # Extraer el enlace del hilo\n",
        "        link = li.find('a', {'ng-href': True})\n",
        "        if link:\n",
        "            thread_url = \"https://boardgamegeek.com\" + link['ng-href']\n",
        "\n",
        "            # Obtener los detalles del hilo (comentarios)\n",
        "            thread_details = get_thread_details(thread_url)\n",
        "            doc.add_paragraph(thread_details)\n",
        "\n",
        "        # Agregar un salto de página después de cada hilo\n",
        "        doc.add_paragraph('')\n",
        "\n",
        "# Guardar el documento Word con el contenido scrapeado\n",
        "doc.save('comentarios.docx')\n",
        "print(\"Documento guardado como 'comentarios.docx'\")\n",
        "\n",
        "# Cerrar el navegador al final\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYUJ1v6OlMi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jeKsDhlNZ_-"
      },
      "source": [
        "Organizados los documentos que usaremos en la carpeta documentos, el archivo reseña.docx y el archivo comentarios.docx vamos a proceder a crear las bdds.\n",
        "Cabe destacar que se accedera a los documentos a partir de una carpeta drive para no tener que repetir el proceso de recoleccion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjhPfbqfBX8L"
      },
      "source": [
        "## Terminada la Recolección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6YqZS3aO8LQ",
        "outputId": "dd534ac2-86c5-4f9f-fa7c-545b0edd08a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\n",
            "To: /content/rulebook_english.docx\n",
            "100% 30.6k/30.6k [00:00<00:00, 41.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\n",
            "To: /content/quick_start_english.docx\n",
            "100% 21.0k/21.0k [00:00<00:00, 31.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\n",
            "To: /content/reseña_español.docx\n",
            "100% 31.1k/31.1k [00:00<00:00, 37.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\n",
            "To: /content/comentarios.docx\n",
            "100% 257k/257k [00:00<00:00, 5.45MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\n",
            "To: /content/castle.csv\n",
            "100% 150/150 [00:00<00:00, 497kB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title Docs\n",
        "!gdown \"1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\" --output \"rulebook_english.docx\"\n",
        "!gdown \"1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\" --output \"quick_start_english.docx\"\n",
        "!gdown \"1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\" --output \"reseña_español.docx\"\n",
        "!gdown \"1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\" --output \"comentarios.docx\"\n",
        "!gdown \"1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\" --output \"castle.csv\"\n",
        "df_castle = pd.read_csv('castle.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-X8cb0NBbPv",
        "outputId": "c4e52a6c-7311-4896-8d73-29517f9913b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo reseña_español.docx procesado y eliminado correctamente. Traducción guardada en translated_reseña_español.docx.\n",
            "Archivo comentarios.docx procesado y eliminado correctamente. Traducción guardada en translated_comentarios.docx.\n"
          ]
        }
      ],
      "source": [
        "# Función para dividir texto en fragmentos más pequeños\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Función para traducir un texto a inglés\n",
        "def traducir_a_ingles(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='auto', to_language='en') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "# Función para extraer texto de un archivo .docx\n",
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Función para guardar texto en un archivo .docx\n",
        "def save_text_to_docx(text, file_path):\n",
        "    try:\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(text)\n",
        "        doc.save(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo {file_path}: {e}\")\n",
        "\n",
        "# Procesar múltiples archivos\n",
        "def procesar_archivos(file_names):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extraer texto del archivo\n",
        "            text = extract_text_from_docx(filename)\n",
        "            if not text.strip():\n",
        "                print(f\"El archivo {filename} está vacío o no se pudo leer.\")\n",
        "                continue\n",
        "\n",
        "            # Traducir texto a inglés\n",
        "            translated_text = traducir_a_ingles(text)\n",
        "\n",
        "            # Guardar el texto traducido en un nuevo archivo\n",
        "            new_filename = f\"translated_{os.path.basename(filename)}\"\n",
        "            save_text_to_docx(translated_text, new_filename)\n",
        "\n",
        "            # Eliminar archivo original\n",
        "            os.remove(filename)\n",
        "            print(f\"Archivo {filename} procesado y eliminado correctamente. Traducción guardada en {new_filename}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando el archivo {filename}: {e}\")\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"reseña_español.docx\", \"comentarios.docx\"]\n",
        "procesar_archivos(archivos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNynjmUMVr3"
      },
      "source": [
        "## Construccion de Bases de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IomKasVMiEJ"
      },
      "source": [
        "### Base de Datos Vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "collapsed": true,
        "id": "pYfLbjDbQI6y"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Función para dividir texto en chunks por oraciones usando SpaCy\n",
        "def dividir_texto_por_oraciones(texto, max_length=1000):\n",
        "    doc = nlp(texto)\n",
        "    oraciones = [sent.text for sent in doc.sents]\n",
        "    fragmentos, fragmento_actual = [], \"\"\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if len(fragmento_actual) + len(oracion) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual.strip())\n",
        "            fragmento_actual = oracion\n",
        "        else:\n",
        "            fragmento_actual += \" \" + oracion if fragmento_actual else oracion\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual.strip())\n",
        "    return fragmentos\n",
        "\n",
        "# Función para extraer metadatos con NER y POS, refinados\n",
        "from collections import Counter\n",
        "\n",
        "def extraer_metadatos(texto, n_keywords=3):\n",
        "    \"\"\"\n",
        "    Extrae metadatos de un texto incluyendo entidades nombradas y las palabras clave más relevantes.\n",
        "\n",
        "    :param texto: Texto del cual extraer los metadatos.\n",
        "    :param n_keywords: Número de palabras clave más relevantes a extraer.\n",
        "    :return: Un diccionario con entidades y palabras clave.\n",
        "    \"\"\"\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Extraer entidades relevantes\n",
        "    entidades = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"TIME\", \"MONEY\"}]\n",
        "\n",
        "    # Extraer palabras clave relevantes: solo sustantivos (NOUN) y verbos (VERB)\n",
        "    palabras_clave = [token.text.lower() for token in doc if token.pos_ in {\"NOUN\", \"VERB\"} and len(token.text) > 2]\n",
        "\n",
        "    # Contar la frecuencia de las palabras clave\n",
        "    palabras_frecuentes = Counter(palabras_clave).most_common(n_keywords)\n",
        "\n",
        "    # Seleccionar las palabras clave más frecuentes\n",
        "    palabras_clave_relevantes = [palabra for palabra, _ in palabras_frecuentes]\n",
        "\n",
        "    # Convertir las listas a strings\n",
        "    return {\n",
        "        \"entities\": \", \".join(entidades),  # Unir las entidades en un string separado por comas\n",
        "        \"keywords\": \", \".join(palabras_clave_relevantes)  # Unir las palabras clave en un string separado por comas\n",
        "    }\n",
        "\n",
        "\n",
        "# Función para calcular embeddings promediados\n",
        "def average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "# Función para generar embeddings normalizados\n",
        "def generar_embeddings(texto, tokenizer, model):\n",
        "    inputs = tokenizer(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "    return F.normalize(embeddings, p=2, dim=1).squeeze().numpy()\n",
        "\n",
        "# Crear la conexión a ChromaDB\n",
        "client_castle = chromadb.Client()\n",
        "\n",
        "#client_castle.delete_collection(name=\"white_castle_embeddings\")\n",
        "collection = client_castle.create_collection(name=\"white_castle_embeddings\")\n",
        "\n",
        "# Procesar archivos y llenar la base de datos\n",
        "def procesar_archivos_y_llenar_bd(file_names, tokenizer, model, collection):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        text = extract_text_from_docx(filename)\n",
        "        if not text.strip():\n",
        "            print(f\"El archivo {filename} está vacío o no se pudo leer.\")\n",
        "            continue\n",
        "\n",
        "        # Dividir el texto en chunks por oraciones\n",
        "        chunks = dividir_texto_por_oraciones(text, max_length=1000)\n",
        "\n",
        "        # Determinar el tipo de archivo basado en el nombre (por ejemplo, por el título del archivo)\n",
        "        if \"rulebook\" in filename:\n",
        "            tipo = \"rules\"\n",
        "        elif \"comentarios\" in filename:\n",
        "            tipo = \"comments\"\n",
        "        elif \"quick_start\" in filename:\n",
        "            tipo = \"quick_start\"\n",
        "        elif \"reseña\" in filename:\n",
        "            tipo = \"review\"\n",
        "        else:\n",
        "            tipo = \"unknown\"  # Si no se reconoce, se marca como \"unknown\"\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            try:\n",
        "                # Generar embeddings\n",
        "                embedding = generar_embeddings(chunk, tokenizer, model)\n",
        "\n",
        "                # Extraer metadatos dinámicos con NER y POS\n",
        "                dynamic_metadata = extraer_metadatos(chunk)\n",
        "\n",
        "                # Combinar metadatos\n",
        "                metadatos = {**dynamic_metadata, \"type\": tipo, \"filename\": filename}\n",
        "\n",
        "                # Agregar a la base de datos\n",
        "                collection.add(\n",
        "                    documents=[chunk],\n",
        "                    metadatas=[metadatos],\n",
        "                    ids=[str(uuid.uuid4())],\n",
        "                    embeddings=[embedding]\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar el chunk {i} del archivo {filename}: {e}\")\n",
        "\n",
        "# Configuración del modelo y tokenizador\n",
        "tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_comentarios.docx\", \"translated_reseña_español.docx\"]\n",
        "\n",
        "procesar_archivos_y_llenar_bd(archivos, tokenizer_e5, model_e5, collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SILkkBm8kD0y"
      },
      "outputs": [],
      "source": [
        "def realizar_consulta(texto, tokenizer, model, collection, k=3):\n",
        "    # Generar el embedding del texto de consulta\n",
        "    embedding = generar_embeddings(texto, tokenizer, model)\n",
        "\n",
        "    # Realizar la búsqueda en la colección de ChromaDB\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=[embedding],\n",
        "        n_results=k  # Número de resultados que deseas obtener\n",
        "    )\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Consulta de ejemplo\n",
        "texto_consulta = \"Explain the rules of the game.\"\n",
        "\n",
        "# Realizar la consulta en la colección\n",
        "r = realizar_consulta(texto_consulta, tokenizer_e5, model_e5, collection)\n",
        "print(r['documents'])\n",
        "# Mostrar los resultados de la consulta, incluyendo el score de similitud\n",
        "for i, doc in enumerate(r['documents'][0]):\n",
        "    similarity_score = r['distances'][0][i]\n",
        "    metadata = r['metadatas'][0][i]  # Metadatos asociados con el documento\n",
        "    print(f\"Resultado {i+1}:\")\n",
        "    print(f\"Texto: {doc}\")  # Texto del documento\n",
        "    print(f\"Metadatos: {metadata}\")  # Metadatos del documento\n",
        "    print(f\"Similitud: {similarity_score:.4f}\")  # Formatear el primer score de similitud\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPW3Uj9MkTG"
      },
      "source": [
        "### Base de Datos de Grafos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vvSIQXrhtq9D"
      },
      "outputs": [],
      "source": [
        "# Configuración de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Crear el cliente de Hugging Face con tu API Key\n",
        "qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "# Función para crear el nodo central \"The White Castle\"\n",
        "def crear_nodo_central(driver):\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            session.run(\"\"\"\n",
        "                MERGE (central:Game {name: 'The White Castle'})\n",
        "            \"\"\")\n",
        "            print(\"Nodo central 'The White Castle' creado o ya existente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al crear el nodo central: {e}\")\n",
        "\n",
        "# Función para leer archivos .docx y dividir el texto en fragmentos\n",
        "def leer_documento(file_path, max_length=1000):\n",
        "    try:\n",
        "        # Leer el documento .docx\n",
        "        doc = docx.Document(file_path)\n",
        "        texto = \"\\n\".join([parrafo.text.strip() for parrafo in doc.paragraphs])\n",
        "\n",
        "        # Dividir el texto en fragmentos si excede el max_length\n",
        "        palabras = texto.split()\n",
        "        fragmentos = []\n",
        "        fragmento_actual = \"\"\n",
        "\n",
        "        for palabra in palabras:\n",
        "            if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "                fragmentos.append(fragmento_actual)\n",
        "                fragmento_actual = palabra\n",
        "            else:\n",
        "                fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "        if fragmento_actual:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "\n",
        "        return fragmentos\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Función para extraer las tríadas RDF de un párrafo\n",
        "def extraer_triadass_rdf(texto_parrafo):\n",
        "    try:\n",
        "        # Usar el cliente de Hugging Face para obtener la respuesta del modelo\n",
        "        response = qwen_client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "            messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system used to generate RDF triplets for a graph in a Neo4j Aura database. \"\n",
        "                        \"The central node 'The White Castle' has already been created in the database. \"\n",
        "                        \"Your priority must be finding relevant information related to the game 'The White Castle', \"\n",
        "                        \"such as the designer, creators, or important relationships. \"\n",
        "                        \"Focus only on triplets where either the subject or object is 'The White Castle'. \"\n",
        "                        \"Do not attempt to recreate 'The White Castle'. \"\n",
        "                        \"Your output must be like this: (subject, predicate, object)\"\n",
        "                    ),\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": f\"extract this in RDF triples: {texto_parrafo}\"}\n",
        "            ],\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        triadass_rdf_texto = response.choices[0].message['content'].strip()\n",
        "        if not triadass_rdf_texto:\n",
        "            print(\"No triples found.\")\n",
        "            return []\n",
        "\n",
        "        print(triadass_rdf_texto)\n",
        "        return triadass_rdf_texto\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener la respuesta del modelo: {e}\")\n",
        "        return []\n",
        "\n",
        "# Función para parsear las tríadas RDF extraídas\n",
        "def parsear_triadass_rdf(texto):\n",
        "    triadass = []\n",
        "    lineas = texto.split(\"\\n\")\n",
        "    for linea in lineas:\n",
        "        if linea.strip():\n",
        "            # Asumimos que las tripletas se presentan entre paréntesis\n",
        "            partes = linea.replace('(', '').replace(')', '').split(\",\")  # Separar por comas y eliminar paréntesis\n",
        "            if len(partes) == 3:\n",
        "                sujeto = partes[0].strip()\n",
        "                predicado = partes[1].strip()\n",
        "                objeto = partes[2].strip()\n",
        "\n",
        "                # Verificar si 'The White Castle' está presente\n",
        "                if \"The White Castle\" in (sujeto, objeto):\n",
        "                    triadass.append((sujeto, predicado, objeto))\n",
        "                else:\n",
        "                    print(f\"Tripleta descartada (no incluye 'The White Castle'): {linea}\")\n",
        "    return triadass\n",
        "\n",
        "# Función para almacenar las tríadas RDF en Neo4j\n",
        "def almacenar_triadass_rdf(triadass, driver):\n",
        "    with driver.session() as session:\n",
        "        for sujeto, predicado, objeto in triadass:\n",
        "            try:\n",
        "                # Asegurarse de que las cadenas no tengan caracteres problemáticos\n",
        "                sujeto = sujeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "                objeto = objeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "                # Crear nodos y relaciones\n",
        "                cypher_query = f\"\"\"\n",
        "                MERGE (sujeto:Entity {{name: '{sujeto}'}})\n",
        "                MERGE (objeto:Entity {{name: '{objeto}'}})\n",
        "                MERGE (sujeto)-[:{predicado.replace(' ', '_').upper()}]->(objeto)\n",
        "                \"\"\"\n",
        "                session.run(cypher_query)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar la tríada ({sujeto}, {predicado}, {objeto}): {e}\")\n",
        "\n",
        "# Función para procesar múltiples archivos y almacenarlos en Neo4j\n",
        "def procesar_documentos(archivos, driver):\n",
        "    for file_path in archivos:\n",
        "        print(f\"Procesando el archivo: {file_path}\")\n",
        "        parrafos = leer_documento(file_path)\n",
        "        for parrafo in parrafos:\n",
        "            print(f\"Extrayendo tríadas para el párrafo: {parrafo}\")\n",
        "            triadass_rdf_texto = extraer_triadass_rdf(parrafo)\n",
        "            if triadass_rdf_texto:  # Solo procesar si se extrajeron tríadas\n",
        "                triadass = parsear_triadass_rdf(triadass_rdf_texto)\n",
        "                if triadass:\n",
        "                    almacenar_triadass_rdf(triadass, driver)\n",
        "                else:\n",
        "                    print(f\"No se encontraron tríadas válidas en el párrafo: {parrafo}\")\n",
        "\n",
        "# Función principal para inicializar la conexión con Neo4j y procesar los documentos\n",
        "def main():\n",
        "    from neo4j import GraphDatabase\n",
        "\n",
        "    # Conectar a Neo4j\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "    # Crear el nodo central\n",
        "    crear_nodo_central(driver)\n",
        "\n",
        "    # Archivos a procesar\n",
        "    archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_reseña_español.docx\"]\n",
        "\n",
        "    # Procesar los documentos y almacenar las tríadas RDF en Neo4j\n",
        "    procesar_documentos(archivos, driver)\n",
        "\n",
        "    # Cerrar la conexión con Neo4j\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kf-lcXlhi8n"
      },
      "source": [
        "## Clasificador Basado en modelo entrenado con ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzMp-Hkhoi8",
        "outputId": "e69ed8d3-30a2-4539-eb21-54519fb4c9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión: 0.9090909090909091\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89         5\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       0.80      1.00      0.89         4\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.93      0.93        11\n",
            "weighted avg       0.93      0.91      0.91        11\n",
            "\n",
            "Prompt: '¿Qué dice el manual sobre el uso de cartas especiales?' - Clasificación: rules\n",
            "Prompt: '¿Es este juego adecuado para jugadores avanzados?' - Clasificación: reviews\n",
            "Prompt: '¿Cómo recomiendan jugar en partidas de 2 jugadores?' - Clasificación: reviews\n",
            "Prompt: '¿Qué pasa si se agotan los recursos en el tablero?' - Clasificación: rules\n",
            "Prompt: '¿Es más divertido jugar en parejas o individualmente?' - Clasificación: reviews\n",
            "Prompt: '¿Qué estrategias funcionan mejor con 4 jugadores?' - Clasificación: reviews\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Dataset ampliado de prompts\n",
        "dataset = [\n",
        "    # Reglas (rules)\n",
        "    (0, \"¿Cuáles son las reglas para construir un edificio?\"),\n",
        "    (0, \"¿Qué pasa si hay un empate en los puntos?\"),\n",
        "    (0, \"¿Cuántas acciones puedo realizar en un turno?\"),\n",
        "    (0, \"¿Qué ocurre si no puedo pagar un recurso necesario?\"),\n",
        "    (0, \"¿Cómo se resuelve un desempate en el final del juego?\"),\n",
        "    (0, \"¿Puedo construir más de un edificio en un solo turno?\"),\n",
        "    (0, \"¿Qué limitaciones existen para colocar fichas?\"),\n",
        "    (0, \"¿Es obligatorio usar todos los recursos en un turno?\"),\n",
        "    (0, \"¿Qué pasa si termino un turno con más de tres cartas?\"),\n",
        "    (0, \"¿Cómo se distribuyen los puntos al final del juego?\"),\n",
        "    (0, \"¿Puedo intercambiar recursos con otros jugadores?\"),\n",
        "    (0, \"¿Qué acciones están permitidas en la fase de preparación?\"),\n",
        "    (0, \"¿Cuántos turnos tiene cada ronda?\"),\n",
        "    (0, \"¿Cuál es el orden para activar habilidades especiales?\"),\n",
        "    (0, \"¿Qué reglas aplican para las cartas especiales?\"),\n",
        "    (0, \"¿Puedo usar habilidades en el turno de otro jugador?\"),\n",
        "    (0, \"¿Qué ocurre si el mazo de cartas se agota?\"),\n",
        "    (0, \"¿Hay un límite de fichas que puedo usar en un turno?\"),\n",
        "\n",
        "    # Reseñas (reviews)\n",
        "    (1, \"¿Qué opinan los jugadores sobre la temática del juego?\"),\n",
        "    (1, \"¿Este juego es recomendado para principiantes?\"),\n",
        "    (1, \"¿Cómo describen los jugadores la complejidad del juego?\"),\n",
        "    (1, \"¿Qué tan rejugable es este juego según las reseñas?\"),\n",
        "    (1, \"¿Es un buen juego para jugar en familia?\"),\n",
        "    (1, \"¿Cuál es la duración típica de una partida?\"),\n",
        "    (1, \"¿Qué tan equilibradas están las estrategias disponibles?\"),\n",
        "    (1, \"¿Cómo se compara este juego con otros del mismo género?\"),\n",
        "    (1, \"¿Los componentes del juego tienen buena calidad?\"),\n",
        "    (1, \"¿Qué aspectos destacan más los jugadores en sus reseñas?\"),\n",
        "    (1, \"¿Hay alguna reseña negativa sobre el juego?\"),\n",
        "    (1, \"¿Este juego es más adecuado para expertos o principiantes?\"),\n",
        "    (1, \"¿Qué tan divertido es jugar con grupos grandes?\"),\n",
        "    (1, \"¿Las reglas son fáciles de aprender según las reseñas?\"),\n",
        "    (1, \"¿Los gráficos del juego ayudan a la inmersión?\"),\n",
        "    (1, \"¿Es un juego más social o estratégico?\"),\n",
        "    (1, \"¿Los jugadores mencionan algún problema recurrente en el diseño?\"),\n",
        "    (1, \"¿Se necesitan expansiones para disfrutar el juego al máximo?\"),\n",
        "\n",
        "    # Comentarios (comments)\n",
        "    (2, \"¿Cuáles son las mejores estrategias iniciales?\"),\n",
        "    (2, \"¿Hay estrategias avanzadas para ganar más puntos?\"),\n",
        "    (2, \"¿Qué tipo de combinaciones de cartas son más efectivas?\"),\n",
        "    (2, \"¿Cómo maximizar los recursos en las primeras rondas?\"),\n",
        "    (2, \"¿Es mejor centrarse en la defensa o en la ofensiva?\"),\n",
        "    (2, \"¿Qué habilidades son más útiles para principiantes?\"),\n",
        "    (2, \"¿Hay estrategias específicas para jugar con 2 jugadores?\"),\n",
        "    (2, \"¿Cuál es la mejor manera de gestionar los recursos limitados?\"),\n",
        "    (2, \"¿Cómo sacar ventaja de los bonos de las cartas especiales?\"),\n",
        "    (2, \"¿Qué tácticas recomiendan los jugadores experimentados?\"),\n",
        "    (2, \"¿Cómo adaptar la estrategia dependiendo de los oponentes?\"),\n",
        "    (2, \"¿Es más beneficioso priorizar los edificios grandes?\"),\n",
        "    (2, \"¿Qué estrategias funcionan mejor en partidas rápidas?\"),\n",
        "    (2, \"¿Cómo influye el orden de turno en la estrategia?\"),\n",
        "    (2, \"¿Cuál es el mejor momento para usar cartas especiales?\"),\n",
        "    (2, \"¿Qué cartas son clave para asegurar la victoria?\"),\n",
        "    (2, \"¿Cómo combinar habilidades de edificios para optimizar el puntaje?\"),\n",
        "    (2, \"¿Qué errores comunes se deben evitar en estrategias avanzadas?\"),\n",
        "]\n",
        "\n",
        "\n",
        "# Mapeo de categorías\n",
        "categories = {0: \"rules\", 1: \"reviews\", 2: \"comments\"}\n",
        "\n",
        "# Preparar datos\n",
        "X = [text for _, text in dataset]\n",
        "y = [label for label, _ in dataset]\n",
        "\n",
        "# Generar embeddings para el dataset\n",
        "X_vectorized = model.encode(X)\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar el modelo\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Precisión:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Nuevos prompts de prueba\n",
        "new_prompts = [\n",
        "    \"¿Qué dice el manual sobre el uso de cartas especiales?\",\n",
        "    \"¿Es este juego adecuado para jugadores avanzados?\",\n",
        "    \"¿Cómo recomiendan jugar en partidas de 2 jugadores?\",\n",
        "    \"¿Qué pasa si se agotan los recursos en el tablero?\",\n",
        "    \"¿Es más divertido jugar en parejas o individualmente?\",\n",
        "    \"¿Qué estrategias funcionan mejor con 4 jugadores?\",\n",
        "]\n",
        "new_embeddings = model.encode(new_prompts)\n",
        "new_predictions = classifier.predict(new_embeddings)\n",
        "\n",
        "# Mostrar resultados\n",
        "for prompt, pred in zip(new_prompts, new_predictions):\n",
        "    print(f\"Prompt: '{prompt}' - Clasificación: {categories[pred]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPqLVthBGho"
      },
      "source": [
        "## Queries Dinamicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqtXwrAY-ZO"
      },
      "source": [
        "#### Doc Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "L9TBVV-BZFnX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DocSearch:\n",
        "    def __init__(self, db_client, collection):\n",
        "        self.db_client = db_client\n",
        "        self.collection = collection\n",
        "        self.tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_e5.to(self.device)\n",
        "\n",
        "        # Usar el pipeline de Hugging Face para el modelo BGE ReRanker\n",
        "        self.reranker = pipeline(\"text-classification\", model=\"BAAI/bge-reranker-v2-m3\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    def average_pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "    def generar_embeddings(self, texto):\n",
        "        inputs = self.tokenizer_e5(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_e5(**inputs)\n",
        "            embeddings = self.average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "        return F.normalize(embeddings, p=2, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    def rerank(self, query, top_documents):\n",
        "        query_prediction = self.reranker(query)\n",
        "        query_score = query_prediction[0]['score']\n",
        "\n",
        "        document_scores = [\n",
        "            self.reranker(doc[\"document\"])[0]['score'] for doc in top_documents\n",
        "        ]\n",
        "\n",
        "        min_score, max_score = min(document_scores), max(document_scores)\n",
        "        document_scores = [(score - min_score) / (max_score - min_score) for score in document_scores]\n",
        "        scores = [0.5 * query_score + 0.5 * doc_score for doc_score in document_scores]\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            doc[\"rerank_score\"] = scores[i]\n",
        "\n",
        "        return sorted(top_documents, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    def realizar_consulta(self, texto, k=3):\n",
        "        embedding = self.generar_embeddings(texto)\n",
        "        resultados = self.collection.query(\n",
        "            query_embeddings=[embedding], n_results=k\n",
        "        )\n",
        "        return resultados\n",
        "\n",
        "    def penalizar_redundancia(self, top_documents, threshold=0.95, penalty_score=0.5):\n",
        "        \"\"\"\n",
        "        Penaliza documentos similares usando similitud coseno entre embeddings.\n",
        "        Garantiza que al menos un documento será devuelto.\n",
        "        También penaliza documentos con el metadato 'filename: translated_comentarios.docx' reduciendo su puntuación.\n",
        "        \"\"\"\n",
        "        if not top_documents:\n",
        "            return top_documents  # Retornar directamente si la lista está vacía\n",
        "\n",
        "        embeddings = [self.generar_embeddings(doc[\"document\"]) for doc in top_documents]\n",
        "        sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "        penalized_docs = []\n",
        "        added_indices = set()\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            score = 1.0  # Puntuación predeterminada\n",
        "\n",
        "            # Penalizar documentos con filename: translated_comentarios.docx\n",
        "            if doc.get(\"metadata\", {}).get(\"filename\") == \"translated_comentarios.docx\":\n",
        "                score -= penalty_score  # Reducir la puntuación de estos documentos\n",
        "\n",
        "            # Incluir documentos solo si no son demasiado similares a los ya seleccionados\n",
        "            if all(sim_matrix[i][j] < threshold for j in range(len(top_documents)) if i != j and j in added_indices):\n",
        "                penalized_docs.append({**doc, \"penalized_score\": score})\n",
        "                added_indices.add(i)\n",
        "\n",
        "        # Garantizar que al menos un documento esté presente\n",
        "        if not penalized_docs:\n",
        "            penalized_docs.append(top_documents[0])  # Incluir el primer documento por defecto\n",
        "\n",
        "        # Ordenar los documentos penalizados por su puntuación\n",
        "        penalized_docs.sort(key=lambda x: x.get(\"penalized_score\", 1.0), reverse=True)\n",
        "\n",
        "        return penalized_docs\n",
        "\n",
        "\n",
        "    def hybrid_search(self, prompt, n_results=3, n_rerank=8, redundancy_threshold=0.95) -> str:\n",
        "        try:\n",
        "            resultados = self.realizar_consulta(prompt, k=n_rerank)\n",
        "            documents = resultados['documents'][0]\n",
        "            metadatas = resultados['metadatas'][0]\n",
        "            distances = resultados['distances'][0]\n",
        "\n",
        "            tokenized_documents = [doc.split() for doc in documents]\n",
        "            tokenized_query = prompt.split()\n",
        "            bm25 = BM25Okapi(tokenized_documents)\n",
        "            keyword_scores = bm25.get_scores(tokenized_query)\n",
        "            keyword_scores = (np.array(keyword_scores) - np.min(keyword_scores)) / (np.max(keyword_scores) - np.min(keyword_scores))\n",
        "\n",
        "            semantic_scores = 1 - (np.array(distances) - np.min(distances)) / (np.max(distances) - np.min(distances))\n",
        "            combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
        "\n",
        "            top_documents = [\n",
        "                {\"document\": doc, \"metadata\": meta, \"score\": score}\n",
        "                for doc, meta, score in zip(documents, metadatas, combined_scores)\n",
        "            ]\n",
        "            top_documents = sorted(top_documents, key=lambda x: x[\"score\"], reverse=True)[:n_rerank]\n",
        "\n",
        "            # Aplicar penalización de redundancia\n",
        "            top_documents = self.penalizar_redundancia(top_documents, threshold=redundancy_threshold)\n",
        "\n",
        "            # Reordenar con el modelo ReRanker\n",
        "            reranked_results = self.rerank(prompt, top_documents)[:n_results]\n",
        "\n",
        "            result_string = \"\\n\\n\".join(\n",
        "                [\n",
        "                    f\"{i+1}. Document:\\n\\\"{res['document'][:1000]}...\\\"\\n\"\n",
        "                ]\n",
        "            )\n",
        "            return f\"Results:\\n\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error en hybrid_search: {e}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmeZ9GChkBHq",
        "outputId": "d35be216-0437-4e6f-9759-ad38d10dacb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Results:\n",
            "\n",
            "1. Document:\n",
            "\"I kind of want to tell someone \"we're playing White Castle tonight\" and then bring that thing out. That would actually be funny! Enlace al hilo: https://boardgamegeek.com/thread/3220134/number-of-courtiers-in-the-rooms Hi all, at the gate field of the castle there is no limit of courtiers. This rule is mentioned in the rulebook. But how is the situation in the rooms of the castle? thanks, rolwin https://boardgamegeek.com/thread/3168240/can-there-be-two-co... No limit There is no limit of courtiers in the rooms. btw: We thought about this and tried a house rule of 1 meeple per color per room to push players to move upwards in the castle. Didn't change anything in the game experience Enlace al hilo:\n",
            "https://boardgamegeek.com/thread/3220104/daimyo-seals-for-passing-checkpoint...\"\n",
            "Metadata: {'entities': 'tonight, Enlace al, Enlace al', 'filename': 'translated_comentarios.docx', 'keywords': 'castle, limit, courtiers', 'type': 'comments'}\n",
            "Score: 0.5001\n",
            "\n",
            "2. Document:\n",
            "\"Here flies the flag of the Sakai clan, commanded by Daimio Sakai Tadakiyo. The\n",
            "White Castle is a Euro-like game with resource management mechanisms, worker placement, and dice placement to perform actions. During the game, over the course of three rounds (in which you only have 9 turns, 3 times per round), players send members of their clan to tend the gardens, defend the castle or move up the social ladder of the nobility. At the end of the game, players are awarded victory points in a variety of ways. The first thing you notice is that the box in which the game comes is much smaller than similar games of this category. It's the size of a Carcassonne box, which would almost give you the impression that The White Castle is a game of the same quality/difficulty. And that is not the case. You get a lot more for your thirty euros (which is a real bargain). Once the box is open, you notice that there is no insert and that all the space is filled with ... the game. And that's darn clever!...\"\n",
            "Metadata: {'entities': 'Sakai, Daimio Sakai Tadakiyo, Carcassonne', 'filename': 'translated_comentarios.docx', 'keywords': 'game, box, clan', 'type': 'comments'}\n",
            "Score: 0.0976\n",
            "\n",
            "3. Document:\n",
            "\"The White Castle box 77 cards 82 tiles 5 boards 85 wooden pieces 15 dice 1 rulebook The\n",
            "components are high quality with wooden meeples used, dice of 3 different colors, and cardboard tokens. The game comes with 3 cardboard bridges that you will construct when first opening the game. Blocks are used to keep track of resources on player boards, and cards are used to change actions throughout the game. How’s It Play? Players will be taking turns choosing one of the dice on one of the three bridges to take actions with. The game consists of 3 rounds where players will take 3 turns in each round, so 9 turns total. When choosing a die to perform your action with, you can choose the higher value die of a certain color, or the lower valued die of a certain color. The color will determine what actions you can perform on the board, and how much money you will gain or possibly pay....\"\n",
            "Metadata: {'entities': '', 'filename': 'translated_comentarios.docx', 'keywords': 'game, dice, used', 'type': 'comments'}\n",
            "Score: 0.0343\n"
          ]
        }
      ],
      "source": [
        "doc_search = DocSearch(client_castle, collection)\n",
        "query = \"What are the rules of the white castel\"\n",
        "results = doc_search.hybrid_search(query, n_results=3, n_rerank=8, redundancy_threshold=0.9)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYHHExIeZDx2"
      },
      "source": [
        "#### Tabular Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "OCwISpD-ZFQ-"
      },
      "outputs": [],
      "source": [
        "class TabularSearch:\n",
        "    def __init__(self, data_frame: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Inicializa la clase de búsqueda tabular con parámetros fijos.\n",
        "\n",
        "        :param data_frame: DataFrame de Pandas donde se realizará la búsqueda.\n",
        "        \"\"\"\n",
        "        self.data_frame = data_frame\n",
        "        self.temperature = 0.4  # Configuración fija\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]  # Configuración fija\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_tabular(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Envía un prompt al modelo Qwen para generar la consulta tabular.\n",
        "\n",
        "        :param prompt: Prompt que incluye instrucción, contexto, entrada, y salida deseada.\n",
        "        :return: Consulta generada por el modelo.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are a system used to generate a query to a tabular search into a dataframe. \"\n",
        "                            \"The dataframe contains the following columns: \"\n",
        "                            \"'Rating', 'Year', 'Review Count', 'Min Players', 'Max Players', 'Play Time (min)', \"\n",
        "                            \"'Suggested Age', 'Complexity', 'Likes', 'Price (USD)'. \"\n",
        "                            \"Your queries should be simple and direct. You just need to give the column content\"\n",
        "                            \"You have only one observation so dont extend the query\"\n",
        "                            \"Your Output must be just the code to access the column\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            return response.choices[0].message[\"content\"].strip()\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error al llamar al modelo Qwen: {e}\")\n",
        "\n",
        "    def tabular_search(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Realiza una búsqueda tabular basada en el prompt generado y devuelve los resultados como una cadena.\n",
        "\n",
        "        :param prompt: Prompt que el modelo usará para generar una consulta.\n",
        "        :return: Resultados filtrados del DataFrame formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Llamar al modelo para generar la consulta tabular\n",
        "            tabular_query = self.query_model_for_tabular(prompt)\n",
        "            print(f\"Consulta generada: {tabular_query}\")\n",
        "\n",
        "            # Limpiar la consulta generada\n",
        "            cleaned_tabular_query = (\n",
        "                tabular_query.replace('```python', '')\n",
        "                .replace(\"```\", '')\n",
        "                .replace(\"df\", 'self.data_frame')\n",
        "                .strip()\n",
        "            )\n",
        "\n",
        "            # Ejecutar la consulta generada usando eval()\n",
        "            result = eval(f\"{cleaned_tabular_query}\")\n",
        "\n",
        "            # Verificar si hay resultados\n",
        "            if result.empty:\n",
        "                return \"No se encontraron resultados para la consulta.\"\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            result_string = result.to_string(index=False)  # Sin índices para una salida más limpia\n",
        "            return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la búsqueda tabular: {e}\")\n",
        "            return f\"Error al procesar la consulta: {e}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYkFO_DZBdO"
      },
      "source": [
        "#### Graph Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "6GM7fw-jZDZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphSearch:\n",
        "    def __init__(self, graph_client, api_key: str):\n",
        "        \"\"\"\n",
        "        Inicializa la clase para búsquedas en una base de datos de grafos.\n",
        "\n",
        "        :param graph_client: Cliente conectado a la base de datos de grafos.\n",
        "        :param api_key: Clave de API para autenticar el cliente de Hugging Face.\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.graph_client = graph_client\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_cypher(self, prompt: str, pos_context: str) -> str:\n",
        "        if not prompt:\n",
        "            print(\"Error: El prompt está vacío.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system specialized in generating Cypher queries for graph databases. \"\n",
        "                        \"The database contains a single node and its relationships with other entities. The central node is labeld 'The White Castle' but you just have to match it like twc: Entity, \"\n",
        "                        \"Examples of relatiosns can be, HAS_DESIGNER, HAS_COVER_ART_BY, INVOLVES, HASMECHANIC OR HASRULE \"\n",
        "                        \"All nodes in the graph are Entity\"\n",
        "                        \"Your task is to analyze the relationships found in the database and generate Cypher queries based on the user’s request. \"\n",
        "                        \"Your output must be just cypher code.\"\n",
        "                    ),\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"Relationships extracted from the graph database (raw data): {pos_context}\\n\"\n",
        "                        f\"Prompt: {prompt}\"\n",
        "                    ),\n",
        "                }],\n",
        "                max_tokens=500,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            cypher_query = response.choices[0].message[\"content\"].strip()\n",
        "            return cypher_query.replace(\"```cypher\", \"\").replace(\"```\", \"\")\n",
        "        except KeyError:\n",
        "            print(\"Error: Respuesta mal formada del modelo.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar la consulta Cypher: {e}\")\n",
        "            return None\n",
        "\n",
        "    def graph_search(self, prompt: str, pos_context: str = \"\"):\n",
        "        \"\"\"\n",
        "        Genera y ejecuta una consulta Cypher basada en un prompt y contexto POS.\n",
        "\n",
        "        :param prompt: Prompt proporcionado por el usuario.\n",
        "        :param pos_context: Contexto adicional obtenido a partir de POS.\n",
        "        :return: Resultados formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generar la consulta Cypher\n",
        "            cypher_query = self.query_model_for_cypher(prompt, pos_context)\n",
        "            if not cypher_query:\n",
        "                print(\"Error: No se pudo generar la consulta Cypher.\")\n",
        "                return \"Error: No se pudo generar una consulta válida.\"\n",
        "            # Ejecutar la consulta en el cliente Neo4j\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                records = result.data()\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            if records:\n",
        "                result_string = \"\\n\".join([str(record) for record in records])\n",
        "                return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "            else:\n",
        "                return \"No se encontraron resultados en la base de datos de grafos.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la búsqueda en grafos: {e}\")\n",
        "            return f\"Error al buscar en la base de datos: {e}\"\n",
        "\n",
        "\n",
        "    def search_relations_by_pos(self, pos_word: str):\n",
        "        \"\"\"\n",
        "        Realiza una consulta para encontrar relaciones cuyo nombre contenga la palabra clave extraída con POS.\n",
        "\n",
        "        :param pos_word: Palabra clave extraída del texto.\n",
        "        :return: Resultado de la búsqueda de relaciones o None si ocurre un error.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cypher_query = f\"\"\"\n",
        "              MATCH ()-[r]->()\n",
        "              WHERE type(r) =~ '.*{pos_word.upper()}.*'\n",
        "              RETURN r\n",
        "              \"\"\"\n",
        "\n",
        "            # Ejecutar la consulta en el cliente de grafos\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                # Obtener todos los resultados\n",
        "                records = result.data()\n",
        "                return records  # Retorna los resultados de la consulta\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la búsqueda de relaciones: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def add_pos_context(prompt: str, graph_search_instance) -> str:\n",
        "    \"\"\"\n",
        "    Toma el prompt, realiza un análisis POS para agregar entidades al contexto y busca relaciones con esas entidades.\n",
        "\n",
        "    :param prompt: Texto de entrada del usuario.\n",
        "    :param graph_search_instance: Instancia de GraphSearch que se utilizará para buscar relaciones.\n",
        "    :return: Texto con contexto adicional de las palabras clave y relaciones encontradas.\n",
        "    \"\"\"\n",
        "    # Cargar modelo de spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Procesar el texto y extraer palabras clave con POS\n",
        "    doc = nlp(prompt)\n",
        "    pos_words = [token.text for token in doc if token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"}]\n",
        "    # Agregar contexto para cada palabra clave y realizar la búsqueda de relaciones\n",
        "    pos_context = \"\"\n",
        "    for pos_word in pos_words:\n",
        "        # Buscar relaciones en Neo4j que contienen la palabra clave en su nombre\n",
        "        relations = graph_search_instance.search_relations_by_pos(pos_word)\n",
        "        if relations:\n",
        "            pos_context += f\"Found the following relationships containing '{pos_word}': {relations}. \"\n",
        "\n",
        "    # Verifica el contexto POS generado\n",
        "    return pos_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgHZsFxGvDQ"
      },
      "outputs": [],
      "source": [
        "# Configuración de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Inicializar el cliente de Neo4j\n",
        "graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Instancia de la clase GraphSearch\n",
        "graph_search_instance = GraphSearch(graph_client=graph_client, api_key=\"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\")\n",
        "\n",
        "prompt = \"Who created The White Castle??\"\n",
        "pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "print(\"Contexto POS:\", pos_context)\n",
        "\n",
        "# Realizar búsqueda en el grafo\n",
        "results = graph_search_instance.graph_search(prompt, pos_context)\n",
        "print(\"Resultados de la búsqueda:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kt-mzmWhbqB"
      },
      "source": [
        "## Clasificador Basado en LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "g-sMIbvRZGUS"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen2.5-72B-Instruct\", context=None):\n",
        "        \"\"\"\n",
        "        Inicializa el clasificador con un modelo de Hugging Face y un contexto base.\n",
        "\n",
        "        :param model_name: Nombre del modelo pre-entrenado en Hugging Face.\n",
        "        :param context: Contexto inicial que describe las bases de datos.\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = \"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\"  # Tu token de autenticación\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.model_name = model_name\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "\n",
        "\n",
        "        # Usar el cliente de inferencia de Hugging Face\n",
        "        self.qwen_client = InferenceClient(api_key=self.api_key)\n",
        "\n",
        "        # Contexto configurable con valor por defecto\n",
        "        self.context = context or self._default_context()\n",
        "\n",
        "        # Definición de las categorías\n",
        "        self.labels = [\"Documents\", \"Graph\", \"Table\"]\n",
        "\n",
        "    def _default_context(self):\n",
        "        return (\n",
        "            \"Documents: This category is for any question regarding the rules, strategies, and textual information of the game.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'What are the rules?' => 'Documents'\\n\"\n",
        "            \"  - 'How do you play the game?' => 'Documents'\\n\\n\"\n",
        "\n",
        "            \"Graph: This category is for questions related to game creators, designers, and interactions between people.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'Who designed the game?' => 'Graph'\\n\"\n",
        "            \"  - 'What is the connection between the game designers?' => 'Graph'\\n\\n\"\n",
        "\n",
        "            \"Table: This category includes specific data about the game, such as number of players, price, and other game statistics.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'How much does the game cost?' => 'Table'\\n\"\n",
        "            \"  - 'How long does the game last?' => 'Table'\\n\\n\"\n",
        "        )\n",
        "\n",
        "    def clasificar(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Clasifica un prompt en una de las categorías: 'Documents', 'Graph' o 'Table'.\n",
        "\n",
        "        :param prompt: Consulta del usuario en texto.\n",
        "        :return: Etiqueta de clasificación con mayor probabilidad.\n",
        "        \"\"\"\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"El prompt proporcionado está vacío.\")\n",
        "\n",
        "        # Combinar contexto y prompt\n",
        "        input_text = f\"{self.context} Prompt: {prompt}\"\n",
        "\n",
        "        try:\n",
        "            # Llamar a Qwen para obtener la clasificación (utilizando InferenceClient)\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a classifier that categorizes queries into Documents, Graph, or Table.\"\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": input_text\n",
        "                }],\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "                max_tokens=100  # Limitamos el tamaño de la respuesta\n",
        "            )\n",
        "\n",
        "            # El modelo Qwen proporcionará una respuesta de clasificación\n",
        "            classification = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Aquí asumimos que el modelo devolverá una respuesta adecuada\n",
        "            return classification\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al clasificar el prompt: {e}\")\n",
        "            return \"unknown\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "-AWBLGaHAJPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb31ba98-7d27-4bd9-a1ba-b31f0487ac3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the rules of the game?\n",
            "Categoría clasificada: Documents\n",
            "\n",
            "Prompt: Who designed the game?\n",
            "Categoría clasificada: Graph\n",
            "\n",
            "Prompt: How much does the game cost?\n",
            "Categoría clasificada: Table\n",
            "\n",
            "Prompt: What is the connection between the game designers?\n",
            "Categoría clasificada: Graph\n",
            "\n",
            "Prompt: How long does the game last?\n",
            "Categoría clasificada: Table\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear una instancia del clasificador\n",
        "classifier = Classifier()\n",
        "\n",
        "# Listado de prompts a probar\n",
        "prompts = [\n",
        "    \"What are the rules of the game?\",  # Debería ser clasificado como 'Documents'\n",
        "    \"Who designed the game?\",          # Debería ser clasificado como 'Graph'\n",
        "    \"How much does the game cost?\",    # Debería ser clasificado como 'Table'\n",
        "    \"What is the connection between the game designers?\",  # Debería ser 'Graph'\n",
        "    \"How long does the game last?\"     # Debería ser 'Table'\n",
        "]\n",
        "\n",
        "# Probar clasificación para cada prompt\n",
        "for prompt in prompts:\n",
        "    category = classifier.clasificar(prompt)\n",
        "    print(f\"Prompt: {prompt}\\nCategoría clasificada: {category}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfWzpA1ivFW"
      },
      "source": [
        "# ***The White Castle Chatbot***\n",
        "## Chat whith an expert in the famous board game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "vTIsThbtReG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a40d52-e24b-4cfb-cb54-06a876b5648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who designed this game?\n",
            "Categoría del prompt: Graph\n",
            "Respuesta generada: The designers of The White Castle are Isra and Shei.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who covered the art of the game?\n",
            "Categoría del prompt: Graph\n",
            "\n",
            "This question is about the person or people who created or designed the art for the game, which falls under the category of interactions between people involved in the game's creation.\n",
            "Respuesta generada: The art of The White Castle is covered by an artist named Kwanchai Moriuchi.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: hay algun otro artista detras del juego\n",
            "Categoría del prompt: Graph\n",
            "Respuesta generada: El artista detrás de El Castillo Blanco es Joan Guardiet.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: what is the rating of this game?\n",
            "Categoría del prompt: Table\n",
            "Consulta generada: df['Rating']\n",
            "Respuesta generada: The rating of this game is 8.0.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: cuanto sale el juego?\n",
            "Categoría del prompt: Table\n",
            "Consulta generada: Price (USD)\n",
            "Error al realizar la búsqueda tabular: name 'Price' is not defined\n",
            "Respuesta generada: El precio de The White Castle no se especifica en la información proporcionada.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: how much does it cost?\n",
            "Categoría del prompt: Table\n",
            "Consulta generada: df['Price (USD)']\n",
            "Respuesta generada: 26.31\n",
            "    \n",
            "    Question: what are the available colors for the pawns?\n",
            "\n",
            "    Retrieved Information: \n",
            "    La lista de colores disponibles para las figuritas es la siguiente:\n",
            "    - Rojo\n",
            "    - Azul\n",
            "    - Verde\n",
            "    - Amarillo\n",
            "    - Negro\n",
            "    \n",
            "    Please provide only the available colors, no extra steps or explanations.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: me gustaria aprender a jugar me podrias dar las reglas basicas para comenzar a jugar?\n",
            "Categoría del prompt: Documents\n",
            "Respuesta generada: Para jugar a *The White Castle*, cada jugador comienza con un personaje y un conjunto de recursos. Tu objetivo es llegar al Castillo Blanco y reclamarlo como tuyo. En el camino, te encontrarás con obstáculos y otros jugadores. El combate se resuelve tirando los dados y comparando los resultados. Algunas áreas requieren que se lance un número específico para continuar. Los recursos se pueden gastar para obtener ventajas o para curar a tu personaje. El primer jugador en llegar al Castillo Blanco y reclamarlo gana la partida.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: I am experience in the game but not an expert, could you give me some tips to improve in this game, i mean what rules or mechanics do i have to improve in order to become a better player\n",
            "Categoría del prompt: Documents\n",
            "\n",
            "This query is seeking advice and tips on how to improve gameplay, which involves understanding the rules and mechanics of the game. Therefore, it falls under the \"Documents\" category.\n",
            "Respuesta generada: To enhance your gameplay in *The White Castle*, you may want to focus on mastering the combat system, as it plays a significant role in the game. Additionally, paying close attention to resource management and strategy development can also help you improve. Keep practicing and studying the game to continue your growth as a player.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: exit\n",
            "¡Hasta luego!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from langdetect import detect\n",
        "from jinja2 import Template\n",
        "\n",
        "chat_graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Función para traducir un texto a inglés\n",
        "def traducir_a_español(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='en', to_language='es') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "\n",
        "# Función para detectar el idioma del texto\n",
        "def detect_language(text):\n",
        "    return detect(text)\n",
        "\n",
        "# Función para generar el template del chat\n",
        "def zephyr_chat_template(messages, add_generation_prompt=True):\n",
        "    template_str  = \"{% for message in messages %}\"\n",
        "    template_str += \"{% if message['role'] == 'user' %}\"\n",
        "    template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
        "    template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'system' %}\"\n",
        "    template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% else %}\"\n",
        "    template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "    template_str += \"{% endfor %}\"\n",
        "    template_str += \"{% if add_generation_prompt %}\"\n",
        "    template_str += \"<|assistant|>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "\n",
        "    # Crear un objeto de plantilla con la cadena de plantilla\n",
        "    template = Template(template_str)\n",
        "\n",
        "    # Renderizar la plantilla con los mensajes proporcionados\n",
        "    return template.render(messages=messages, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "# Función para obtener el modelo generativo (en este caso usando Zephyr)\n",
        "def generate_response_with_model(context):\n",
        "    api_key = \"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\"\n",
        "\n",
        "    api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    data = {\n",
        "        \"inputs\": context,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 256,\n",
        "            \"temperature\": 0.68,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        if isinstance(result, list):\n",
        "            return result[0].get('generated_text', 'No se generó texto.')\n",
        "        else:\n",
        "            return \"Error en la respuesta del modelo: la respuesta no es una lista.\"\n",
        "    else:\n",
        "        return f\"Error en la solicitud: {response.status_code} - {response.text}\"\n",
        "\n",
        "game_state = \"\"\"\n",
        "    You are a chatbot specialized in the famous board game *The White Castle*.\n",
        "    You may want to think and process step by step the information that you have before yoy respond\n",
        "    Your task is to generate responses based on the user's question and the relevant information retrieved from the database.\n",
        "    You should take into account the question, the retrieved information, and the context to provide a detailed and accurate response.\n",
        "    You will rearly recive the exact information to the question but you have to formulate your answer based on what you know.\n",
        "    For instance you may no be provided with the entire rulebook but you can say \"Some of the rules consist of ...\"\n",
        "    Your answers should be clear, concise, and directly related to the game, *The White Castle* and you dont hace to cite any retrieved information, take it as if you already know it.\n",
        "    \"\"\"\n",
        "loop_flag = True\n",
        "while loop_flag:\n",
        "    # Paso 1: Obtener el prompt del usuario\n",
        "    user_prompt = input(\" (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: \")\n",
        "    if user_prompt.lower() == \"exit\":\n",
        "        print(\"¡Hasta luego!\")\n",
        "        break\n",
        "    esp_flag = False\n",
        "\n",
        "    # Detectar el idioma del texto\n",
        "    if detect_language(user_prompt) == \"es\":\n",
        "        user_prompt = ts.translate_text(user_prompt, translator='bing', from_language='es', to_language='en')\n",
        "        esp_flag = True\n",
        "\n",
        "    # Paso 2: Clasificar el prompt\n",
        "    classifier = Classifier()\n",
        "    category = classifier.clasificar(user_prompt)\n",
        "    print(f\"Categoría del prompt: {category}\")\n",
        "\n",
        "    # Paso 3: Recuperar la información basada en la clasificación\n",
        "    if category == \"Documents\":\n",
        "        doc_search = DocSearch(client_castle, collection)\n",
        "        retrieved_info = doc_search.hybrid_search(user_prompt)\n",
        "    elif category == \"Graph\":\n",
        "        graph_search = GraphSearch(graph_client=chat_graph_client, api_key=\"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\")\n",
        "        pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "        # Realizar búsqueda en el grafo\n",
        "        retrieved_info = graph_search.graph_search(user_prompt, pos_context)\n",
        "    elif category == \"Table\":\n",
        "        tabular_search = TabularSearch(df_castle)\n",
        "        retrieved_info = tabular_search.tabular_search(user_prompt)\n",
        "    else:\n",
        "        retrieved_info = \"No se pudo clasificar la consulta adecuadamente.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Ajusta el contexto para que sea relevante y claro\n",
        "    context = f\"\"\"\n",
        "    Role: {game_state}\n",
        "    Question: {user_prompt}\n",
        "\n",
        "    Retrieved Information: {retrieved_info}\n",
        "\n",
        "    Please provide only the direct answer, no extra steps or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Paso 5: Generar respuesta con un modelo generativo (Zephyr)\n",
        "    response = generate_response_with_model(context)\n",
        "\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\")[1].strip()\n",
        "    elif \"Response:\" in response:\n",
        "        answer = response.split(\"Response:\")[1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "\n",
        "    # Si el texto original estaba en español, traducimos la respuesta generada al español\n",
        "    if esp_flag:\n",
        "        answer = traducir_a_español(answer)\n",
        "    print(f\"Respuesta generada: {answer}\")\n",
        "    print(f\"{'-' * 50}  \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cClPcYLfIXa"
      },
      "source": [
        "# ReAct Agent\n",
        "\n",
        "Este ejercicio se basa en el ejercicio 1, e incorpora el concepto de Agente, basado en el concepto ReAct. Nuestro agente debe cumplir con los siguientes requisitos:\n",
        "Utilizar al menos 3 herramientas, aprovechando el trabajo anterior:\n",
        "- doc_search(): Busca información en los documentos\n",
        "- graph_search(): Busca información en la base de datos de grafos\n",
        "- table_search(): Busca información sobre los datos tabulares\n",
        "\n",
        "Se puede implementar alguna nueva herramienta que se considere necesaria y que pueda enriquecer las capacidades del agente.\n",
        "Utilizar la librería Llama-Index para desarrollar el agente:\n",
        "**llama_index.core.agent.ReActAgent**\n",
        "**llama_index.core.tools.FunctionTool**\n",
        "Se debe construir el prompt adecuado para incorporar las herramientas al agente ReAct\n",
        "### Presentar en el informe los resultados:\n",
        "1. Presentar 5 ejemplos de prompts donde se deba recurrir a más de una herramienta para responder al usuario. Evaluar los resultados obtenidos\n",
        "3. Explicar con 3 ejemplos, donde el agente falla o las respuestas no son precisas.\n",
        "4. Explicar cuáles son las mejoras que sería conveniente realizar para mejorar los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDOyNEgpwPy8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!ollama pull phi3:medium > ollama.loga\n",
        "\n",
        "!pip install litellm[proxy]\n",
        "!nohup litellm --model ollama/phi3:medium --port 8000 > litellm.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUpS12Xk3IWz"
      },
      "outputs": [],
      "source": [
        "def doc_search(query):\n",
        "    \"\"\"\n",
        "    Simula una búsqueda en documentos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la búsqueda (simulados).\n",
        "    \"\"\"\n",
        "    # Simulamos que encontramos un documento relevante\n",
        "    return [{\n",
        "        \"document\": f\"Información relevante sobre el tema '{query}' en los documentos.\",\n",
        "        \"metadata\": {\"source\": \"document_1.pdf\", \"date\": \"2023-01-01\"},\n",
        "        \"score\": 0.95\n",
        "    }]\n",
        "\n",
        "def graph_search(query):\n",
        "    \"\"\"\n",
        "    Simula una búsqueda en la base de datos de grafos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la búsqueda (simulados).\n",
        "    \"\"\"\n",
        "    return [{\n",
        "        \"creator\": {\"name\": \"Israel Cendrero\"}\n",
        "    }]\n",
        "\n",
        "def table_search(query):\n",
        "    \"\"\"\n",
        "    Simula una búsqueda en una tabla de datos.\n",
        "    :param query: Pregunta del usuario.\n",
        "    :return: Resultados de la búsqueda (simulados).\n",
        "    \"\"\"\n",
        "    # Simulamos que encontramos información tabular\n",
        "    return pd.DataFrame({\n",
        "        \"Player\": [\"Player 1\", \"Player 2\"],\n",
        "        \"Score\": [50, 45]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWxcUuxfwJ2n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class AgentPipeline:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el flujo entre el chatbot, herramientas de búsqueda y el agente ReAct.\n",
        "    \"\"\"\n",
        "    def __init__(self, doc_search, graph_search, table_search, llm_model=\"phi3:medium\", temperature=0.2):\n",
        "        self.doc_search = doc_search\n",
        "        self.graph_search = graph_search\n",
        "        self.table_search = table_search\n",
        "\n",
        "        # Inicializar las herramientas para el agente ReAct\n",
        "        self.tools = [\n",
        "            FunctionTool.from_defaults(fn=doc_search, description=\"Busca información en documentos.\"),\n",
        "            FunctionTool.from_defaults(fn=graph_search, description=\"Busca relaciones en bases de datos de grafos.\"),\n",
        "            FunctionTool.from_defaults(fn=table_search, description=\"Busca información en datos tabulares.\"),\n",
        "        ]\n",
        "\n",
        "        # Configuración del modelo LLM (phi3:medium de Ollama)\n",
        "        self.llm = Ollama(model=llm_model, temperature=temperature)\n",
        "\n",
        "        # Configurar el sistema de agentes\n",
        "        self.system_prompt = \"\"\"\n",
        "        Eres un agente experto que responde preguntas complejas utilizando herramientas.\n",
        "        Tienes acceso a las siguientes herramientas:\n",
        "        1. doc_search: Busca información en documentos extensos.\n",
        "        2. graph_search: Busca relaciones en una base de datos de grafos.\n",
        "        3. table_search: Busca datos específicos en tablas estructuradas.\n",
        "\n",
        "        Para cada pregunta:\n",
        "        1. Analiza qué información necesitas.\n",
        "        2. Usa las herramientas con el siguiente formato:\n",
        "\n",
        "        Thought: Pienso en lo que necesito buscar.\n",
        "        Action: [nombre de la herramienta]\n",
        "        Action Input: [entrada para la herramienta]\n",
        "\n",
        "        Observation: [resultado de la herramienta]\n",
        "        Final Answer: [respuesta final]\n",
        "        \"\"\"\n",
        "\n",
        "        # Crear el agente ReAct\n",
        "        self.agent = ReActAgent.from_tools(\n",
        "            tools=self.tools,\n",
        "            llm=self.llm,\n",
        "            system_prompt=self.system_prompt,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def process_query(self, user_query):\n",
        "        \"\"\"\n",
        "        Procesa la consulta del usuario utilizando las herramientas de búsqueda.\n",
        "\n",
        "        :param user_query: Consulta proporcionada por el usuario.\n",
        "        :return: Respuesta generada para el usuario.\n",
        "        \"\"\"\n",
        "        response = self.agent.chat(user_query)\n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "LoPI6yJ3ML_D",
        "outputId": "838789db-11de-48a4-a6e6-976a01c55329"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-85-5605d1378c5d>, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-85-5605d1378c5d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    @misc{qwen2.5,\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "#@title Citas\n",
        "@misc{qwen2.5,\n",
        "    title = {Qwen2.5: A Party of Foundation Models},\n",
        "    url = {https://qwenlm.github.io/blog/qwen2.5/},\n",
        "    author = {Qwen Team},\n",
        "    month = {September},\n",
        "    year = {2024}\n",
        "}\n",
        "\n",
        "@article{qwen2,\n",
        "    title={Qwen2 Technical Report},\n",
        "    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n",
        "    journal={arXiv preprint arXiv:2407.10671},\n",
        "    year={2024}\n",
        "}\n",
        "\n",
        "@misc{intfloat2023e5,\n",
        "    title = {Multilingual E5: A Text Embedding Model for Retrieval Tasks},\n",
        "    author = {Intfloat Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/intfloat/multilingual-e5-small}}\n",
        "}\n",
        "\n",
        "\n",
        "@misc{bge2023reranker,\n",
        "    title = {BAAI General Embedding Reranker v2 (BGE ReRanker)},\n",
        "    author = {BAAI Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/BAAI/bge-reranker-v2-m3}}\n",
        "}\n",
        "\n",
        "@misc{phi3ollama2024,\n",
        "    title = {Phi-3: A Series of Lightweight Language Models},\n",
        "    author = {Ollama Team},\n",
        "    year = {2024},\n",
        "    howpublished = {\\url{https://ollama.ai/library/phi3}}\n",
        "}\n",
        "\n",
        "@misc{distilbert2019,\n",
        "    title = {DistilBERT: A distilled version of BERT for faster NLP tasks},\n",
        "    author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n",
        "    year = {2019},\n",
        "    journal = {arXiv preprint arXiv:1910.01108},\n",
        "    url = {https://huggingface.co/distilbert-base-uncased}\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "laUFu54--eTf",
        "2UZAbT5Vc2kB",
        "ZjhPfbqfBX8L",
        "UtNynjmUMVr3",
        "9IomKasVMiEJ",
        "7qPW3Uj9MkTG",
        "1Kf-lcXlhi8n",
        "6GPqLVthBGho",
        "VRqtXwrAY-ZO",
        "eYHHExIeZDx2",
        "3Kt-mzmWhbqB"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}