{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaeo3m6civnc"
      },
      "source": [
        "# **Trabajo Pr√°ctico Final - TUIA NLP 2024**\n",
        "\n",
        "### **Autor**: Tom√°s Valentino Avecilla\n",
        "### **legajo**: A-4239/9\n",
        "### **Fecha**: 18 de diciembre de 2024\n",
        "### **Materia**: Procesamiento del Lenguaje Natural (NLP)  \n",
        "### **Instituci√≥n**: Facultad de Ciencias Exactas, ingenieria y Agrimensura - UNR\n",
        "\n",
        "---\n",
        "\n",
        "## **Descripci√≥n del Trabajo**\n",
        "Este cuaderno contiene el desarrollo del Trabajo Pr√°ctico Final para la materia NLP. El objetivo es implementar un **chatbot experto** sobre un juego de mesa tipo Eurogame, utilizando las t√©cnicas de **Retrieval-Augmented Generation (RAG)** y **Agentes ReAct**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laUFu54--eTf"
      },
      "source": [
        "## Preparaci√≥n del Entorno de Trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EF0P5Oln-rDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dbde86-2104-4817-ec01-91cc8f18e18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?25lpulling manifest ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ‚ñï‚ñè 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ‚ñï‚ñè 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ‚ñï‚ñè 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ‚ñï‚ñè 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ‚ñï‚ñè   96 B                         \n",
            "pulling 34bb5ab01051... 100% ‚ñï‚ñè  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "NAME               ID              SIZE      MODIFIED               \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    Less than a second ago    \n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "# --- 1. Actualizaci√≥n del Sistema ---\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-spa tesseract-ocr-eng  # Herramientas de OCR y dependencias\n",
        "!pip install python-decouple\n",
        "\n",
        "# --- 2. Instalaci√≥n de Bibliotecas Generales ---\n",
        "!pip install gdown requests python-docx  # Descarga de archivos, solicitudes web, manejo de archivos docx\n",
        "\n",
        "# --- 3. Procesamiento de Im√°genes y OCR ---\n",
        "!pip install pdf2image pytesseract  # Extracci√≥n de im√°genes y OCR desde PDFs\n",
        "\n",
        "# --- 4. Web Scraping y Automatizaci√≥n ---\n",
        "!pip install selenium webdriver-manager  # Automatizaci√≥n de navegaci√≥n web\n",
        "\n",
        "# --- 5. Procesamiento del Lenguaje Natural ---\n",
        "!pip install transformers  # Modelos de Hugging Face y Sentence Transformers\n",
        "!pip install --upgrade sentence_transformers\n",
        "!python -m spacy download es_core_news_md en_core_web_sm  # Modelo en espa√±ol para spaCy\n",
        "!pip install translators  # Traducci√≥n autom√°tica de texto\n",
        "!pip install langdetect\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "!pip install --upgrade chromadb neo4j pydgraph  # Bases de datos vectoriales, de grafos y almacenamiento\n",
        "\n",
        "# --- 7. Modelos de Machine Learning y Deep Learning ---\n",
        "!pip install torch\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install rank_bm25\n",
        "!pip install --upgrade tokenizers\n",
        "\n",
        "# --- 8. Herramientas para Agentes ReAct ---\n",
        "\n",
        "!pip install llama-index\n",
        "!pip install llama-index-llms-ollama\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!rm -f ollama_start.sh\n",
        "!echo '#!/bin/bash' > ollama_start.sh\n",
        "!echo 'ollama serve' >> ollama_start.sh\n",
        "!chmod +x ollama_start.sh\n",
        "!nohup ./ollama_start.sh &\n",
        "!ollama pull llama3.2 > ollama.log\n",
        "!ollama list\n",
        "\n",
        "\n",
        "!nohup litellm --model ollama/llama3.2:latest --port 8000 > litellm.log 2>&1 &\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ym2dmNZf-jDo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# --- 1. Importaciones B√°sicas y Manejo de Archivos ---\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import uuid\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import logging\n",
        "from time import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import docx\n",
        "from docx import Document\n",
        "\n",
        "# --- 2. Procesamiento de Im√°genes y OCR ---\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "# --- 3. Web Scraping ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# --- 4. Procesamiento del Lenguaje Natural ---\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import translators as ts\n",
        "from langdetect import detect\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- 5. Modelos & Embeddings ---\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from huggingface_hub import InferenceClient\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# --- 6. Bases de Datos ---\n",
        "import chromadb  # Base de datos vectorial\n",
        "from neo4j import GraphDatabase  # Base de datos de grafos\n",
        "\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent.react.formatter import ReActChatFormatter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZAbT5Vc2kB"
      },
      "source": [
        "## Recolecci√≥n de Informaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BluNQKmNhYi4"
      },
      "source": [
        "### **1:** üéÆ Reglas y Jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2YVnGhEzNQ0"
      },
      "source": [
        "#### Archivos Descargados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzfzPLRm9Gh5"
      },
      "source": [
        "Primero vamos a usar dos documentos descargados de la seccion ***files*** del sitio BGG _[link](https://boardgamegeek.com/boardgame/371942/the-white-castle/files)_\n",
        "Contamos con 3 documentos PDF\n",
        "\n",
        "\n",
        "1.   Reglamento en Ingles\n",
        "2.   Guia Rapida en ingles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGBUyJJX9jHv",
        "outputId": "f5c7c305-ec00-4a6f-a137-183a34f2a567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\n",
            "To: /content/reglamento_en.pdf\n",
            "100% 13.2M/13.2M [00:00<00:00, 23.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\n",
            "To: /content/qs_en.pdf\n",
            "100% 446k/446k [00:00<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1UnwbtxzLsqUMgN2JT-xGayOHgm1B6ZGi\" --output \"reglamento_en.pdf\"\n",
        "!gdown \"1k_cyEVOoBqrP3od8dYUKFAQ4sLPaRzVI\" --output \"qs_en.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XvjR5eP_NiT"
      },
      "source": [
        "Los 2 PDFs son imagenes por lo cual vamos a tener que extraer el texto y para eso usaremos un ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th1OJLibBFIB"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n para crear carpetas si no existen\n",
        "def crear_carpeta(nombre_carpeta):\n",
        "    if not os.path.exists(nombre_carpeta):\n",
        "        os.makedirs(nombre_carpeta)\n",
        "\n",
        "# Carpetas para guardar las im√°genes\n",
        "carpeta_en = \"imgs_reglamento_en\"\n",
        "carpeta_qs = \"imgs_qs_en\"\n",
        "\n",
        "# Crear carpetas\n",
        "crear_carpeta(carpeta_en)\n",
        "crear_carpeta(carpeta_qs)\n",
        "\n",
        "# Convertir PDFs en listas de im√°genes\n",
        "imgs_reglamento_en = convert_from_path(\"reglamento_en.pdf\")\n",
        "imgs_qs_en = convert_from_path(\"qs_en.pdf\")\n",
        "\n",
        "# Guardar las im√°genes en sus carpetas correspondientes\n",
        "for i, imagen in enumerate(imgs_reglamento_en):\n",
        "    imagen.save(os.path.join(carpeta_en, f'pagina_en_{i + 1}.png'), 'PNG')\n",
        "\n",
        "for i, imagen in enumerate(imgs_qs_en):\n",
        "    imagen.save(os.path.join(carpeta_qs, f'qs_en_{i + 1}.png'), 'PNG')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUn_5YrLCKpO",
        "outputId": "36cf0b6c-2b6b-46e7-eb8b-cc2e060dfe4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto procesado y guardado en: texto_reglamento_en\n",
            "Texto procesado y guardado en: texto_qs_en\n"
          ]
        }
      ],
      "source": [
        "# Configurar pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "# Funci√≥n para procesar im√°genes y extraer texto\n",
        "def procesar_imagenes(carpeta_imagenes, carpeta_salida, idioma=\"eng\"):\n",
        "    # Crear carpeta de salida si no existe\n",
        "    if not os.path.exists(carpeta_salida):\n",
        "        os.makedirs(carpeta_salida)\n",
        "\n",
        "    # Iterar sobre las im√°genes en la carpeta\n",
        "    for imagen_nombre in sorted(os.listdir(carpeta_imagenes)):  # Ordenar para mantener secuencia\n",
        "        if imagen_nombre.endswith(\".png\"):\n",
        "            ruta_imagen = os.path.join(carpeta_imagenes, imagen_nombre)\n",
        "\n",
        "            # Extraer texto de la imagen\n",
        "            texto = pytesseract.image_to_string(Image.open(ruta_imagen), lang=idioma)\n",
        "\n",
        "            # Guardar texto en un archivo\n",
        "            nombre_txt = os.path.splitext(imagen_nombre)[0] + \".txt\"\n",
        "            ruta_salida = os.path.join(carpeta_salida, nombre_txt)\n",
        "\n",
        "            with open(ruta_salida, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(texto)\n",
        "\n",
        "    print(f\"Texto procesado y guardado en: {carpeta_salida}\")\n",
        "\n",
        "# Procesar las im√°genes de cada carpeta\n",
        "procesar_imagenes(\"imgs_reglamento_en\", \"texto_reglamento_en\", idioma=\"eng\")\n",
        "procesar_imagenes(\"imgs_qs_en\", \"texto_qs_en\", idioma=\"eng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeLxKhjxvhwD",
        "outputId": "fdb10be4-ff58-46c6-c88f-f5f483713968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos creados en: documentos\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Funci√≥n avanzada para limpiar texto y normalizarlo\n",
        "def limpiar_texto(texto):\n",
        "\n",
        "    # Paso 1: Normalizar texto a Unicode est√°ndar (NFKC)\n",
        "    texto = unicodedata.normalize(\"NFKC\", texto)\n",
        "\n",
        "    # Paso 2: Eliminar m√∫ltiples espacios, tabulaciones y l√≠neas redundantes\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    texto = re.sub(r'[^a-zA-Z0-9\\s.,!?¬ø¬°]', '', texto)\n",
        "\n",
        "    # Paso 3: Convertir a min√∫sculas para uniformidad sem√°ntica\n",
        "    texto = texto.lower()\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "# Funci√≥n para crear documentos a partir de los archivos .txt en cada carpeta\n",
        "def crear_documentos(carpeta_lista, carpeta_destino_docs):\n",
        "    # Crear carpeta de destino para los documentos si no existe\n",
        "    if not os.path.exists(carpeta_destino_docs):\n",
        "        os.makedirs(carpeta_destino_docs)\n",
        "\n",
        "    # Iterar sobre las carpetas en la lista proporcionada\n",
        "    for carpeta in sorted(carpeta_lista):  # Ordenar para mantener consistencia\n",
        "        if os.path.isdir(carpeta):  # Procesar solo carpetas\n",
        "            # Crear un documento Word\n",
        "            doc = Document()\n",
        "\n",
        "            # Iterar sobre los archivos .txt en la carpeta\n",
        "            for archivo_txt in sorted(os.listdir(carpeta)):  # Ordenar para mantener secuencia\n",
        "                if archivo_txt.endswith(\".txt\"):\n",
        "                    ruta_txt = os.path.join(carpeta, archivo_txt)\n",
        "\n",
        "                    with open(ruta_txt, \"r\", encoding=\"utf-8\") as file:\n",
        "                        texto = file.read()\n",
        "\n",
        "                    # Limpiar el texto antes de agregarlo al documento\n",
        "                    texto = limpiar_texto(texto)\n",
        "\n",
        "                    doc.add_paragraph(texto)\n",
        "\n",
        "            # Guardar el documento en la carpeta de destino\n",
        "            nombre_doc = f\"{os.path.basename(carpeta)}.docx\"\n",
        "            ruta_doc = os.path.join(carpeta_destino_docs, nombre_doc)\n",
        "            doc.save(ruta_doc)\n",
        "\n",
        "    print(f\"Documentos creados en: {carpeta_destino_docs}\")\n",
        "\n",
        "# Lista de carpetas y carpeta de destino\n",
        "carpetas = [\"texto_reglamento_en\", \"texto_qs_en\"]\n",
        "carpeta_destino_docs = \"documentos\"\n",
        "\n",
        "# Crear documentos\n",
        "crear_documentos(carpetas, carpeta_destino_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7EsuI7g9hG"
      },
      "source": [
        "### **2:** üèØ The White Castle Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdWgfoZPzP56"
      },
      "source": [
        "#### Scrapping de jugabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ7yO2an5lZX"
      },
      "source": [
        "Haremos un scrapping de la siguiente [rese√±a](https://misutmeeple.com/2023/11/resena-the-white-castle/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKOKbAVQ1waB",
        "outputId": "39d73380-d5bc-4118-bc37-1b6bd6f0863d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como: rese√±a.docx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# URL de la p√°gina que deseas scrapear\n",
        "url = 'https://misutmeeple.com/2023/11/resena-the-white-castle/'\n",
        "\n",
        "# Hacer la solicitud HTTP para obtener el HTML de la p√°gina\n",
        "response = requests.get(url)\n",
        "html = response.text\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Seleccionar el div con la clase espec√≠fica\n",
        "div = soup.find('div', class_='entry-content single-content')\n",
        "\n",
        "# Asegurarse de que el div existe antes de continuar\n",
        "if div:\n",
        "    # Crear un nuevo documento Word\n",
        "    doc = Document()\n",
        "\n",
        "    # Iterar sobre los elementos h2, h3 y p dentro del div\n",
        "    for elemento in div.find_all(['h2', 'h3', 'p'], recursive=True):\n",
        "        etiqueta = elemento.name\n",
        "        texto = elemento.get_text(strip=True)\n",
        "\n",
        "        # Agregar texto al documento seg√∫n el tipo de etiqueta\n",
        "        if etiqueta == 'h2':\n",
        "            doc.add_heading(texto, level=1)\n",
        "        elif etiqueta == 'h3':\n",
        "            doc.add_heading(texto, level=2)\n",
        "        elif etiqueta == 'p':\n",
        "            doc.add_paragraph(texto)\n",
        "\n",
        "    # Guardar el documento Word\n",
        "    doc_name = \"rese√±a.docx\"\n",
        "    doc.save(doc_name)\n",
        "\n",
        "    print(f\"Documento guardado como: {doc_name}\")\n",
        "else:\n",
        "    print(\"Error.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68I7kA99MF9g"
      },
      "source": [
        "#### Configuracion de drivers para no generar conflictos en el uso de selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCbsuXOtjIJ"
      },
      "source": [
        "Nos aseguramos de que el sistema tiene las bibliotecas necesarias para ejecutar aplicaciones gr√°ficas (como Chrome) en un entorno Linux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QckhcQr9tmly"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Actualizar los repositorios\n",
        "!apt-get update\n",
        "!apt-get install -y wget curl unzip\n",
        "!apt-get install -y libx11-dev libx11-xcb1 libglu1-mesa libxi6 libgconf-2-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BD-ThPEtqiU"
      },
      "source": [
        "Descargamos e instalamos Google Chrome para poder usarlo con Selenium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ZiLbvwtsUD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Instalar Google Chrome (√∫ltima versi√≥n estable)\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt --fix-broken install -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctEwpcXt3gt"
      },
      "source": [
        "Descargamos ChromeDriver (necesario para Selenium) y lo mueve a una ubicaci√≥n accesible globalmente en el sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uIIpMdJ8j0rb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://chromedriver.storage.googleapis.com/131.0.6778.87/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip\n",
        "!mv chromedriver /usr/local/bin/chromedriver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qIhAs2t8mx"
      },
      "source": [
        "Configura el navegador para ejecutarse en segundo plano sin mostrar interfaz gr√°fica, ideal para entornos como servidores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8-Cg4Kmt9i_"
      },
      "outputs": [],
      "source": [
        "# Configurar las opciones de Chrome para usarlo en modo headless\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')  # Modo sin cabeza\n",
        "chrome_options.add_argument('--no-sandbox')  # Evitar problemas de sandboxing\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Usar en contenedores o entornos con poca memoria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H60n54f-MN3J"
      },
      "source": [
        "#### Datos Tabulares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpcGmlUy6pmu"
      },
      "source": [
        "A continuacion haremos el scrapping de la pagina [board game geek](https://boardgamegeek.com/boardgame/371942/the-white-castle/) para extraer datos numericos e insertarlos en un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V6O609PpgN6"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# URL del juego\n",
        "url = 'https://boardgamegeek.com/boardgame/371942/the-white-castle'\n",
        "\n",
        "# Abrir la p√°gina\n",
        "driver.get(url)\n",
        "\n",
        "# Esperar unos segundos para que cargue el contenido din√°mico\n",
        "time.sleep(8)\n",
        "\n",
        "# Obtener el HTML completo de la p√°gina cargada\n",
        "html = driver.page_source\n",
        "\n",
        "# Parsear el HTML con BeautifulSoup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Extraer los datos y guardarlos en variables\n",
        "rating_value = soup.find('span', itemprop='ratingValue')\n",
        "rating_value = rating_value.text.strip() if rating_value else 'N/A'\n",
        "\n",
        "year_section = soup.find('span', class_='game-year')\n",
        "game_year = year_section.text.strip() if year_section else 'N/A'\n",
        "game_year = game_year.strip(\"()\").strip()  # Eliminar par√©ntesis y espacios adicionales\n",
        "game_year = int(game_year) if game_year.isdigit() else 'N/A'\n",
        "\n",
        "review_count_section = soup.find('meta', itemprop='reviewCount')\n",
        "review_count_value = int(review_count_section['content']) if review_count_section else 'N/A'\n",
        "\n",
        "min_players = max_players = play_time = suggested_age = complexity = 'N/A'\n",
        "\n",
        "# Extraer los elementos de gameplay\n",
        "gameplay_section = soup.find('ul', class_='gameplay')\n",
        "if gameplay_section:\n",
        "    gameplay_items = gameplay_section.find_all('li', class_='gameplay-item')\n",
        "    for item in gameplay_items:\n",
        "        title = item.find('h3').text.strip() if item.find('h3') else 'N/A'\n",
        "\n",
        "        if title == \"Number of Players\":\n",
        "            min_players = int(item.find('meta', itemprop='minValue')['content']) if item.find('meta', itemprop='minValue') else 'N/A'\n",
        "            max_players = int(item.find('meta', itemprop='maxValue')['content']) if item.find('meta', itemprop='maxValue') else 'N/A'\n",
        "\n",
        "        elif title == \"Play Time\":\n",
        "            play_time = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Suggested Age\":\n",
        "            suggested_age = int(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "        elif title == \"Complexity\":\n",
        "            complexity = float(item.find('span', class_='ng-binding').text.strip()) if item.find('span', class_='ng-binding') else 'N/A'\n",
        "\n",
        "like_count_section = soup.find('a', class_='game-action-play-count')\n",
        "likes = like_count_section.text.strip() if like_count_section else 'N/A'\n",
        "\n",
        "# Extraer precios sugeridos\n",
        "prices = soup.find_all('div', class_='summary-sale-item-price')\n",
        "precio_sugerido = prices[1].text.strip() if len(prices) > 0 else 'N/A'\n",
        "\n",
        "# Cerrar el WebDriver\n",
        "driver.quit()\n",
        "\n",
        "# Crear un diccionario para los datos\n",
        "data = {\n",
        "    \"Rating\": [float(rating_value) if rating_value != 'N/A' else None],\n",
        "    \"Year\": [game_year if game_year != 'N/A' else None],\n",
        "    \"Review Count\": [review_count_value if review_count_value != 'N/A' else None],\n",
        "    \"Min Players\": [min_players if min_players != 'N/A' else None],\n",
        "    \"Max Players\": [max_players if max_players != 'N/A' else None],\n",
        "    \"Play Time (min)\": [play_time if play_time != 'N/A' else None],\n",
        "    \"Suggested Age\": [suggested_age if suggested_age != 'N/A' else None],\n",
        "    \"Complexity\": [complexity if complexity != 'N/A' else None],\n",
        "    \"Likes\": [int(likes.replace('K', '000').replace('.', '')) if 'K' in likes else int(likes) if likes != 'N/A' else None],\n",
        "    \"Price (USD)\": [float(precio_sugerido.replace(\"from $\", \"\").replace(\"$\", \"\")) if precio_sugerido != 'N/A' else None]\n",
        "}\n",
        "# Crear el DataFrame\n",
        "df_castle = pd.DataFrame(data)\n",
        "df_castle.head()\n",
        "\n",
        "# Descargar df en csv\n",
        "df_castle.to_csv('castle.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qG42vAhfh1"
      },
      "source": [
        "### **3:** üí¨ Comentarios y opiniones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPtkW3iLnkT"
      },
      "source": [
        "Con el siguiente scrapping traemos a un documento todos lo comentarios en el [foro](https://boardgamegeek.com/boardgame/371942/the-white-castle/forums)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldXKjRVTiTOq",
        "outputId": "ba66b106-ad49-498b-f9b7-f7f033a1c426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento guardado como 'comentarios.docx'\n"
          ]
        }
      ],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "# Crear un nuevo documento Word\n",
        "doc = Document()\n",
        "\n",
        "# Funci√≥n para scrapear los hilos individuales\n",
        "def get_thread_details(thread_url):\n",
        "    driver.get(thread_url)\n",
        "    time.sleep(3)  # Esperar a que se cargue el contenido din√°mico\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Buscar los comentarios dentro de las etiquetas <gg-markup-safe-html>\n",
        "    comments = soup.find_all('gg-markup-safe-html')\n",
        "    thread_content = \"\"\n",
        "\n",
        "    for comment in comments:\n",
        "        thread_content += comment.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "    return thread_content\n",
        "\n",
        "# Loop para iterar sobre varias p√°ginas\n",
        "for id in [1, 2, 3, 4]:\n",
        "    url = f'https://boardgamegeek.com/boardgame/371942/the-white-castle/forums/0?pageid={id}'\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Obtener el HTML completo de la p√°gina cargada\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Seleccionar todos los <li> con la clase 'summary-item ng-scope'\n",
        "    li_items = soup.find_all('li', class_='summary-item ng-scope')\n",
        "\n",
        "    # Iterar sobre cada elemento <li>\n",
        "    for li in li_items:\n",
        "        # Extraer el t√≠tulo\n",
        "        title = li.find('h3', class_='m-0 fs-sm text-inherit leading-inherit text-inline')\n",
        "        if title:\n",
        "            title_text = title.get_text(strip=True)\n",
        "            doc.add_paragraph(f\"T√≠tulo: {title_text}\")\n",
        "\n",
        "        # Extraer el enlace del hilo\n",
        "        link = li.find('a', {'ng-href': True})\n",
        "        if link:\n",
        "            thread_url = \"https://boardgamegeek.com\" + link['ng-href']\n",
        "\n",
        "            # Obtener los detalles del hilo (comentarios)\n",
        "            thread_details = get_thread_details(thread_url)\n",
        "            doc.add_paragraph(thread_details)\n",
        "\n",
        "        # Agregar un salto de p√°gina despu√©s de cada hilo\n",
        "        doc.add_paragraph('')\n",
        "\n",
        "# Guardar el documento Word con el contenido scrapeado\n",
        "doc.save('comentarios.docx')\n",
        "print(\"Documento guardado como 'comentarios.docx'\")\n",
        "\n",
        "# Cerrar el navegador al final\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYUJ1v6OlMi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jeKsDhlNZ_-"
      },
      "source": [
        "Organizados los documentos que usaremos en la carpeta documentos, el archivo rese√±a.docx y el archivo comentarios.docx vamos a proceder a crear las bdds.\n",
        "Cabe destacar que se accedera a los documentos a partir de una carpeta drive para no tener que repetir el proceso de recoleccion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjhPfbqfBX8L"
      },
      "source": [
        "## Terminada la Recolecci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q6YqZS3aO8LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb56488b-5c71-49db-93e7-518f85d35db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\n",
            "To: /content/rulebook_english.docx\n",
            "100% 30.6k/30.6k [00:00<00:00, 55.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\n",
            "To: /content/quick_start_english.docx\n",
            "100% 21.0k/21.0k [00:00<00:00, 44.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\n",
            "To: /content/rese√±a_espa√±ol.docx\n",
            "100% 31.1k/31.1k [00:00<00:00, 59.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\n",
            "From (redirected): https://drive.google.com/uc?id=1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN&confirm=t&uuid=bcdcd713-41bb-4499-aa29-e9f9f40c33c2\n",
            "To: /content/comentarios.docx\n",
            "100% 257k/257k [00:00<00:00, 106MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\n",
            "To: /content/castle.csv\n",
            "100% 150/150 [00:00<00:00, 689kB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title Docs\n",
        "!gdown \"1DGso0ohjApz582O2Csbn9-2mRvGW2p_0\" --output \"rulebook_english.docx\"\n",
        "!gdown \"1mS1vO-bzjAC2zaLRZtStZ4rW-e_tbA4z\" --output \"quick_start_english.docx\"\n",
        "!gdown \"1fzsEmSo3z-y3pocef1tpTx2hnCr28qF1\" --output \"rese√±a_espa√±ol.docx\"\n",
        "!gdown \"1c3_yLh7TluobQvNxZRdz_DaONxZbWyjN\" --output \"comentarios.docx\"\n",
        "!gdown \"1K7yXNjC3yZyYIruafLb-93sWn8tkKtYX\" --output \"castle.csv\"\n",
        "df_castle = pd.read_csv('castle.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-X8cb0NBbPv",
        "outputId": "2db89746-5c2e-4662-8363-21d9fe12212c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo rese√±a_espa√±ol.docx procesado y eliminado correctamente. Traducci√≥n guardada en translated_rese√±a_espa√±ol.docx.\n",
            "Archivo comentarios.docx procesado y eliminado correctamente. Traducci√≥n guardada en translated_comentarios.docx.\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n para dividir texto en fragmentos m√°s peque√±os\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para traducir un texto a ingl√©s\n",
        "def traducir_a_ingles(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='auto', to_language='en') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "# Funci√≥n para extraer texto de un archivo .docx\n",
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Funci√≥n para guardar texto en un archivo .docx\n",
        "def save_text_to_docx(text, file_path):\n",
        "    try:\n",
        "        doc = Document()\n",
        "        doc.add_paragraph(text)\n",
        "        doc.save(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo {file_path}: {e}\")\n",
        "\n",
        "# Procesar m√∫ltiples archivos\n",
        "def procesar_archivos(file_names):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extraer texto del archivo\n",
        "            text = extract_text_from_docx(filename)\n",
        "            if not text.strip():\n",
        "                print(f\"El archivo {filename} est√° vac√≠o o no se pudo leer.\")\n",
        "                continue\n",
        "\n",
        "            # Traducir texto a ingl√©s\n",
        "            translated_text = traducir_a_ingles(text)\n",
        "\n",
        "            # Guardar el texto traducido en un nuevo archivo\n",
        "            new_filename = f\"translated_{os.path.basename(filename)}\"\n",
        "            save_text_to_docx(translated_text, new_filename)\n",
        "\n",
        "            # Eliminar archivo original\n",
        "            os.remove(filename)\n",
        "            print(f\"Archivo {filename} procesado y eliminado correctamente. Traducci√≥n guardada en {new_filename}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando el archivo {filename}: {e}\")\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"rese√±a_espa√±ol.docx\", \"comentarios.docx\"]\n",
        "procesar_archivos(archivos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNynjmUMVr3"
      },
      "source": [
        "## Construccion de Bases de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IomKasVMiEJ"
      },
      "source": [
        "### Base de Datos Vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "pYfLbjDbQI6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69524cf6-01dc-4551-b693-08a6f054172c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Funci√≥n para dividir texto en chunks por oraciones usando SpaCy\n",
        "def dividir_texto_por_oraciones(texto, max_length=1000):\n",
        "    doc = nlp(texto)\n",
        "    oraciones = [sent.text for sent in doc.sents]\n",
        "    fragmentos, fragmento_actual = [], \"\"\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if len(fragmento_actual) + len(oracion) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual.strip())\n",
        "            fragmento_actual = oracion\n",
        "        else:\n",
        "            fragmento_actual += \" \" + oracion if fragmento_actual else oracion\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual.strip())\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para extraer metadatos con NER y POS, refinados\n",
        "from collections import Counter\n",
        "\n",
        "def extraer_metadatos(texto, n_keywords=3):\n",
        "    \"\"\"\n",
        "    Extrae metadatos de un texto incluyendo entidades nombradas y las palabras clave m√°s relevantes.\n",
        "\n",
        "    :param texto: Texto del cual extraer los metadatos.\n",
        "    :param n_keywords: N√∫mero de palabras clave m√°s relevantes a extraer.\n",
        "    :return: Un diccionario con entidades y palabras clave.\n",
        "    \"\"\"\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Extraer entidades relevantes\n",
        "    entidades = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"TIME\", \"MONEY\"}]\n",
        "\n",
        "    # Extraer palabras clave relevantes: solo sustantivos (NOUN) y verbos (VERB)\n",
        "    palabras_clave = [token.text.lower() for token in doc if token.pos_ in {\"NOUN\", \"VERB\"} and len(token.text) > 2]\n",
        "\n",
        "    # Contar la frecuencia de las palabras clave\n",
        "    palabras_frecuentes = Counter(palabras_clave).most_common(n_keywords)\n",
        "\n",
        "    # Seleccionar las palabras clave m√°s frecuentes\n",
        "    palabras_clave_relevantes = [palabra for palabra, _ in palabras_frecuentes]\n",
        "\n",
        "    # Convertir las listas a strings\n",
        "    return {\n",
        "        \"entities\": \", \".join(entidades),  # Unir las entidades en un string separado por comas\n",
        "        \"keywords\": \", \".join(palabras_clave_relevantes)  # Unir las palabras clave en un string separado por comas\n",
        "    }\n",
        "\n",
        "\n",
        "# Funci√≥n para calcular embeddings promediados\n",
        "def average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "# Funci√≥n para generar embeddings normalizados\n",
        "def generar_embeddings(texto, tokenizer, model):\n",
        "    inputs = tokenizer(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "    return F.normalize(embeddings, p=2, dim=1).squeeze().numpy()\n",
        "\n",
        "# Crear la conexi√≥n a ChromaDB\n",
        "client_castle = chromadb.Client()\n",
        "\n",
        "#client_castle.delete_collection(name=\"white_castle_embeddings\")\n",
        "collection = client_castle.create_collection(name=\"white_castle_embeddings\")\n",
        "\n",
        "# Procesar archivos y llenar la base de datos\n",
        "def procesar_archivos_y_llenar_bd(file_names, tokenizer, model, collection):\n",
        "    for filename in file_names:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"El archivo {filename} no existe. Verifica la ruta.\")\n",
        "            continue\n",
        "\n",
        "        text = extract_text_from_docx(filename)\n",
        "        if not text.strip():\n",
        "            print(f\"El archivo {filename} est√° vac√≠o o no se pudo leer.\")\n",
        "            continue\n",
        "\n",
        "        # Dividir el texto en chunks por oraciones\n",
        "        chunks = dividir_texto_por_oraciones(text, max_length=1000)\n",
        "\n",
        "        # Determinar el tipo de archivo basado en el nombre (por ejemplo, por el t√≠tulo del archivo)\n",
        "        if \"rulebook\" in filename:\n",
        "            tipo = \"rules\"\n",
        "        elif \"comentarios\" in filename:\n",
        "            tipo = \"comments\"\n",
        "        elif \"quick_start\" in filename:\n",
        "            tipo = \"quick_start\"\n",
        "        elif \"rese√±a\" in filename:\n",
        "            tipo = \"review\"\n",
        "        else:\n",
        "            tipo = \"unknown\"  # Si no se reconoce, se marca como \"unknown\"\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            try:\n",
        "                # Generar embeddings\n",
        "                embedding = generar_embeddings(chunk, tokenizer, model)\n",
        "\n",
        "                # Extraer metadatos din√°micos con NER y POS\n",
        "                dynamic_metadata = extraer_metadatos(chunk)\n",
        "\n",
        "                # Combinar metadatos\n",
        "                metadatos = {**dynamic_metadata, \"type\": tipo, \"filename\": filename}\n",
        "\n",
        "                # Agregar a la base de datos\n",
        "                collection.add(\n",
        "                    documents=[chunk],\n",
        "                    metadatas=[metadatos],\n",
        "                    ids=[str(uuid.uuid4())],\n",
        "                    embeddings=[embedding]\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar el chunk {i} del archivo {filename}: {e}\")\n",
        "\n",
        "# Configuraci√≥n del modelo y tokenizador\n",
        "tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "\n",
        "# Lista de archivos a procesar\n",
        "archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_comentarios.docx\", \"translated_rese√±a_espa√±ol.docx\"]\n",
        "\n",
        "procesar_archivos_y_llenar_bd(archivos, tokenizer_e5, model_e5, collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SILkkBm8kD0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fdf785c-9b41-4261-aca0-30948d017c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\"Throughout the game, you are going to send your Courtiers, your Warriors, and your Gardeners out into the land to curry favour with the Emperor so that you can earn your prestige. The player with the most prestige at the end of the game wins. The entire game lasts nine turns, so, in case you missed it in the first paragraph. This game is very tight. In your turn you will pick up one die from one of the three bridges, here you'll need to pick either the highest\\nnumber die or the lowest number die and depending on its colour or its number you will place it on one of the action slots on the board (all of which change every game). You will then fire off the action of that location. And maybe some sub-actions. And maybe some more sub actions; if you've been really, really clever. Turn then passes to your opponent where they will do something similar. However, no matter what they do, their action is pretty much guaranteed to utterly piss in your chips. Play continues to the next player.\", \"On the one hand we have the initial action cards, which will be placed on the personal board allowing you to solve the indicated action by placing a\\ngiven in the player's domain. The other determines, on the one hand, the initial resources for the game and, on the other, the first benefit obtained by solving a lamp action. The main premise of the game will be to send servants to the different spaces of the board. Thus, each player will have a set of Gardeners, Warriors and Courtiers. Each one is associated with a type of resource and, when sent to the main board, it will allow you to activate an additional action. Outfielders provide victory points at the end of the game and, if they are placed on the bridge associated with their card, it will be reactivated at the end of the first two rounds. Warriors provide points based on the courtiers in the castle and provide a benefit that is greater based on resource spending.\", 'This may be more or less confusing depending on the language of the rule book you use, but the core principle of resolving gardeners are the end of rounds 1 & 2 are: In turn order each player: 1. Notes which gardeners currently on the board are under bridges that still have dice 2. Takes the \"bonus\" activate associated with those existing gardeners (identified in #1) in whatever order that player chooses (#2 may involve placing more gardeners, which still earn their immediate benefit, but those \"new\" gardeners weren\\'t there during #1, so they don\\'t get a bonus activation)\\n That is the only logical conclusion, otherwise in theory this could on until you run out of resources, and the point is that you get to use the gardeners again IF that gardener was under a bridge with a die at the end of the first or second round! so iv paid 3 coins and 2 rices in order to place the gardener on the Black rock garden. so i got 2 seals in that moment . may i activate this gardener on Gardens phase ?']]\n",
            "Resultado 1:\n",
            "Texto: Throughout the game, you are going to send your Courtiers, your Warriors, and your Gardeners out into the land to curry favour with the Emperor so that you can earn your prestige. The player with the most prestige at the end of the game wins. The entire game lasts nine turns, so, in case you missed it in the first paragraph. This game is very tight. In your turn you will pick up one die from one of the three bridges, here you'll need to pick either the highest\n",
            "number die or the lowest number die and depending on its colour or its number you will place it on one of the action slots on the board (all of which change every game). You will then fire off the action of that location. And maybe some sub-actions. And maybe some more sub actions; if you've been really, really clever. Turn then passes to your opponent where they will do something similar. However, no matter what they do, their action is pretty much guaranteed to utterly piss in your chips. Play continues to the next player.\n",
            "Metadatos: {'entities': '', 'filename': 'translated_comentarios.docx', 'keywords': 'game, die, number', 'type': 'comments'}\n",
            "Similitud: 0.2671\n",
            "--------------------------------------------------\n",
            "Resultado 2:\n",
            "Texto: On the one hand we have the initial action cards, which will be placed on the personal board allowing you to solve the indicated action by placing a\n",
            "given in the player's domain. The other determines, on the one hand, the initial resources for the game and, on the other, the first benefit obtained by solving a lamp action. The main premise of the game will be to send servants to the different spaces of the board. Thus, each player will have a set of Gardeners, Warriors and Courtiers. Each one is associated with a type of resource and, when sent to the main board, it will allow you to activate an additional action. Outfielders provide victory points at the end of the game and, if they are placed on the bridge associated with their card, it will be reactivated at the end of the first two rounds. Warriors provide points based on the courtiers in the castle and provide a benefit that is greater based on resource spending.\n",
            "Metadatos: {'entities': 'Gardeners, Warriors and Courtiers, the end of the first', 'filename': 'translated_rese√±a_espa√±ol.docx', 'keywords': 'action, board, game', 'type': 'review'}\n",
            "Similitud: 0.2750\n",
            "--------------------------------------------------\n",
            "Resultado 3:\n",
            "Texto: This may be more or less confusing depending on the language of the rule book you use, but the core principle of resolving gardeners are the end of rounds 1 & 2 are: In turn order each player: 1. Notes which gardeners currently on the board are under bridges that still have dice 2. Takes the \"bonus\" activate associated with those existing gardeners (identified in #1) in whatever order that player chooses (#2 may involve placing more gardeners, which still earn their immediate benefit, but those \"new\" gardeners weren't there during #1, so they don't get a bonus activation)\n",
            " That is the only logical conclusion, otherwise in theory this could on until you run out of resources, and the point is that you get to use the gardeners again IF that gardener was under a bridge with a die at the end of the first or second round! so iv paid 3 coins and 2 rices in order to place the gardener on the Black rock garden. so i got 2 seals in that moment . may i activate this gardener on Gardens phase ?\n",
            "Metadatos: {'entities': '1, 2, 1, the end of the first, Gardens', 'filename': 'translated_comentarios.docx', 'keywords': 'gardeners, order, gardener', 'type': 'comments'}\n",
            "Similitud: 0.2774\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def realizar_consulta(texto, tokenizer, model, collection, k=3):\n",
        "    # Generar el embedding del texto de consulta\n",
        "    embedding = generar_embeddings(texto, tokenizer, model)\n",
        "\n",
        "    # Realizar la b√∫squeda en la colecci√≥n de ChromaDB\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=[embedding],\n",
        "        n_results=k  # N√∫mero de resultados que deseas obtener\n",
        "    )\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Consulta de ejemplo\n",
        "texto_consulta = \"Explain the rules of the game.\"\n",
        "\n",
        "# Realizar la consulta en la colecci√≥n\n",
        "r = realizar_consulta(texto_consulta, tokenizer_e5, model_e5, collection)\n",
        "print(r['documents'])\n",
        "# Mostrar los resultados de la consulta, incluyendo el score de similitud\n",
        "for i, doc in enumerate(r['documents'][0]):\n",
        "    similarity_score = r['distances'][0][i]\n",
        "    metadata = r['metadatas'][0][i]  # Metadatos asociados con el documento\n",
        "    print(f\"Resultado {i+1}:\")\n",
        "    print(f\"Texto: {doc}\")  # Texto del documento\n",
        "    print(f\"Metadatos: {metadata}\")  # Metadatos del documento\n",
        "    print(f\"Similitud: {similarity_score:.4f}\")  # Formatear el primer score de similitud\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPW3Uj9MkTG"
      },
      "source": [
        "### Base de Datos de Grafos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vvSIQXrhtq9D"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Crear el cliente de Hugging Face con tu API Key\n",
        "qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "# Funci√≥n para crear el nodo central \"The White Castle\"\n",
        "def crear_nodo_central(driver):\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            session.run(\"\"\"\n",
        "                MERGE (central:Game {name: 'The White Castle'})\n",
        "            \"\"\")\n",
        "            print(\"Nodo central 'The White Castle' creado o ya existente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al crear el nodo central: {e}\")\n",
        "\n",
        "# Funci√≥n para leer archivos .docx y dividir el texto en fragmentos\n",
        "def leer_documento(file_path, max_length=1000):\n",
        "    try:\n",
        "        # Leer el documento .docx\n",
        "        doc = docx.Document(file_path)\n",
        "        texto = \"\\n\".join([parrafo.text.strip() for parrafo in doc.paragraphs])\n",
        "\n",
        "        # Dividir el texto en fragmentos si excede el max_length\n",
        "        palabras = texto.split()\n",
        "        fragmentos = []\n",
        "        fragmento_actual = \"\"\n",
        "\n",
        "        for palabra in palabras:\n",
        "            if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "                fragmentos.append(fragmento_actual)\n",
        "                fragmento_actual = palabra\n",
        "            else:\n",
        "                fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "        if fragmento_actual:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "\n",
        "        return fragmentos\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Funci√≥n para extraer las tr√≠adas RDF de un p√°rrafo\n",
        "def extraer_triadass_rdf(texto_parrafo):\n",
        "    try:\n",
        "        # Usar el cliente de Hugging Face para obtener la respuesta del modelo\n",
        "        response = qwen_client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "            messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system used to generate RDF triplets for a graph in a Neo4j Aura database. \"\n",
        "                        \"The central node 'The White Castle' has already been created in the database. \"\n",
        "                        \"Your priority must be finding relevant information related to the game 'The White Castle', \"\n",
        "                        \"such as the designer, creators, or important relationships. \"\n",
        "                        \"Focus only on triplets where either the subject or object is 'The White Castle'. \"\n",
        "                        \"Do not attempt to recreate 'The White Castle'. \"\n",
        "                        \"Your output must be like this: (subject, predicate, object)\"\n",
        "                    ),\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": f\"extract this in RDF triples: {texto_parrafo}\"}\n",
        "            ],\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        triadass_rdf_texto = response.choices[0].message['content'].strip()\n",
        "        if not triadass_rdf_texto:\n",
        "            print(\"No triples found.\")\n",
        "            return []\n",
        "\n",
        "        print(triadass_rdf_texto)\n",
        "        return triadass_rdf_texto\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener la respuesta del modelo: {e}\")\n",
        "        return []\n",
        "\n",
        "# Funci√≥n para parsear las tr√≠adas RDF extra√≠das\n",
        "def parsear_triadass_rdf(texto):\n",
        "    triadass = []\n",
        "    lineas = texto.split(\"\\n\")\n",
        "    for linea in lineas:\n",
        "        if linea.strip():\n",
        "            # Asumimos que las tripletas se presentan entre par√©ntesis\n",
        "            partes = linea.replace('(', '').replace(')', '').split(\",\")  # Separar por comas y eliminar par√©ntesis\n",
        "            if len(partes) == 3:\n",
        "                sujeto = partes[0].strip()\n",
        "                predicado = partes[1].strip()\n",
        "                objeto = partes[2].strip()\n",
        "\n",
        "                # Verificar si 'The White Castle' est√° presente\n",
        "                if \"The White Castle\" in (sujeto, objeto):\n",
        "                    triadass.append((sujeto, predicado, objeto))\n",
        "                else:\n",
        "                    print(f\"Tripleta descartada (no incluye 'The White Castle'): {linea}\")\n",
        "    return triadass\n",
        "\n",
        "# Funci√≥n para almacenar las tr√≠adas RDF en Neo4j\n",
        "def almacenar_triadass_rdf(triadass, driver):\n",
        "    with driver.session() as session:\n",
        "        for sujeto, predicado, objeto in triadass:\n",
        "            try:\n",
        "                # Asegurarse de que las cadenas no tengan caracteres problem√°ticos\n",
        "                sujeto = sujeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "                objeto = objeto.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
        "\n",
        "                # Crear nodos y relaciones\n",
        "                cypher_query = f\"\"\"\n",
        "                MERGE (sujeto:Entity {{name: '{sujeto}'}})\n",
        "                MERGE (objeto:Entity {{name: '{objeto}'}})\n",
        "                MERGE (sujeto)-[:{predicado.replace(' ', '_').upper()}]->(objeto)\n",
        "                \"\"\"\n",
        "                session.run(cypher_query)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al procesar la tr√≠ada ({sujeto}, {predicado}, {objeto}): {e}\")\n",
        "\n",
        "# Funci√≥n para procesar m√∫ltiples archivos y almacenarlos en Neo4j\n",
        "def procesar_documentos(archivos, driver):\n",
        "    for file_path in archivos:\n",
        "        print(f\"Procesando el archivo: {file_path}\")\n",
        "        parrafos = leer_documento(file_path)\n",
        "        for parrafo in parrafos:\n",
        "            print(f\"Extrayendo tr√≠adas para el p√°rrafo: {parrafo}\")\n",
        "            triadass_rdf_texto = extraer_triadass_rdf(parrafo)\n",
        "            if triadass_rdf_texto:  # Solo procesar si se extrajeron tr√≠adas\n",
        "                triadass = parsear_triadass_rdf(triadass_rdf_texto)\n",
        "                if triadass:\n",
        "                    almacenar_triadass_rdf(triadass, driver)\n",
        "                else:\n",
        "                    print(f\"No se encontraron tr√≠adas v√°lidas en el p√°rrafo: {parrafo}\")\n",
        "\n",
        "# Funci√≥n principal para inicializar la conexi√≥n con Neo4j y procesar los documentos\n",
        "def main():\n",
        "    from neo4j import GraphDatabase\n",
        "\n",
        "    # Conectar a Neo4j\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "    # Crear el nodo central\n",
        "    crear_nodo_central(driver)\n",
        "\n",
        "    # Archivos a procesar\n",
        "    archivos = [\"quick_start_english.docx\", \"rulebook_english.docx\", \"translated_rese√±a_espa√±ol.docx\"]\n",
        "\n",
        "    # Procesar los documentos y almacenar las tr√≠adas RDF en Neo4j\n",
        "    procesar_documentos(archivos, driver)\n",
        "\n",
        "    # Cerrar la conexi√≥n con Neo4j\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPqLVthBGho"
      },
      "source": [
        "## Queries Dinamicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqtXwrAY-ZO"
      },
      "source": [
        "#### Doc Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "L9TBVV-BZFnX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DocSearch:\n",
        "    def __init__(self, db_client, collection):\n",
        "        self.db_client = db_client\n",
        "        self.collection = collection\n",
        "        self.tokenizer_e5 = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.model_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_e5.to(self.device)\n",
        "\n",
        "        # Usar el pipeline de Hugging Face para el modelo BGE ReRanker\n",
        "        self.reranker = pipeline(\"text-classification\", model=\"BAAI/bge-reranker-v2-m3\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    def average_pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "    def generar_embeddings(self, texto):\n",
        "        inputs = self.tokenizer_e5(texto, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_e5(**inputs)\n",
        "            embeddings = self.average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "        return F.normalize(embeddings, p=2, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    def rerank(self, query, top_documents):\n",
        "        query_prediction = self.reranker(query)\n",
        "        query_score = query_prediction[0]['score']\n",
        "\n",
        "        document_scores = [\n",
        "            self.reranker(doc[\"document\"])[0]['score'] for doc in top_documents\n",
        "        ]\n",
        "\n",
        "        min_score, max_score = min(document_scores), max(document_scores)\n",
        "        document_scores = [(score - min_score) / (max_score - min_score) for score in document_scores]\n",
        "        scores = [0.5 * query_score + 0.5 * doc_score for doc_score in document_scores]\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            doc[\"rerank_score\"] = scores[i]\n",
        "\n",
        "        return sorted(top_documents, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    def realizar_consulta(self, texto, k=3):\n",
        "        embedding = self.generar_embeddings(texto)\n",
        "        resultados = self.collection.query(\n",
        "            query_embeddings=[embedding], n_results=k\n",
        "        )\n",
        "        return resultados\n",
        "\n",
        "    def penalizar_redundancia(self, top_documents, threshold=0.95, penalty_score=0.5):\n",
        "        \"\"\"\n",
        "        Penaliza documentos similares usando similitud coseno entre embeddings.\n",
        "        Garantiza que al menos un documento ser√° devuelto.\n",
        "        Tambi√©n penaliza documentos con el metadato 'filename: translated_comentarios.docx' reduciendo su puntuaci√≥n.\n",
        "        \"\"\"\n",
        "        if not top_documents:\n",
        "            return top_documents  # Retornar directamente si la lista est√° vac√≠a\n",
        "\n",
        "        embeddings = [self.generar_embeddings(doc[\"document\"]) for doc in top_documents]\n",
        "        sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "        penalized_docs = []\n",
        "        added_indices = set()\n",
        "\n",
        "        for i, doc in enumerate(top_documents):\n",
        "            score = 1.0  # Puntuaci√≥n predeterminada\n",
        "\n",
        "            # Penalizar documentos con filename: translated_comentarios.docx\n",
        "            if doc.get(\"metadata\", {}).get(\"filename\") == \"translated_comentarios.docx\":\n",
        "                score -= penalty_score  # Reducir la puntuaci√≥n de estos documentos\n",
        "\n",
        "            # Incluir documentos solo si no son demasiado similares a los ya seleccionados\n",
        "            if all(sim_matrix[i][j] < threshold for j in range(len(top_documents)) if i != j and j in added_indices):\n",
        "                penalized_docs.append({**doc, \"penalized_score\": score})\n",
        "                added_indices.add(i)\n",
        "\n",
        "        # Garantizar que al menos un documento est√© presente\n",
        "        if not penalized_docs:\n",
        "            penalized_docs.append(top_documents[0])  # Incluir el primer documento por defecto\n",
        "\n",
        "        # Ordenar los documentos penalizados por su puntuaci√≥n\n",
        "        penalized_docs.sort(key=lambda x: x.get(\"penalized_score\", 1.0), reverse=True)\n",
        "\n",
        "        return penalized_docs\n",
        "\n",
        "\n",
        "    def hybrid_search(self, prompt, n_results=3, n_rerank=8, redundancy_threshold=0.95) -> str:\n",
        "        try:\n",
        "            resultados = self.realizar_consulta(prompt, k=n_rerank)\n",
        "            documents = resultados['documents'][0]\n",
        "            metadatas = resultados['metadatas'][0]\n",
        "            distances = resultados['distances'][0]\n",
        "\n",
        "            tokenized_documents = [doc.split() for doc in documents]\n",
        "            tokenized_query = prompt.split()\n",
        "            bm25 = BM25Okapi(tokenized_documents)\n",
        "            keyword_scores = bm25.get_scores(tokenized_query)\n",
        "            keyword_scores = (np.array(keyword_scores) - np.min(keyword_scores)) / (np.max(keyword_scores) - np.min(keyword_scores))\n",
        "\n",
        "            semantic_scores = 1 - (np.array(distances) - np.min(distances)) / (np.max(distances) - np.min(distances))\n",
        "            combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
        "\n",
        "            top_documents = [\n",
        "                {\"document\": doc, \"metadata\": meta, \"score\": score}\n",
        "                for doc, meta, score in zip(documents, metadatas, combined_scores)\n",
        "            ]\n",
        "            top_documents = sorted(top_documents, key=lambda x: x[\"score\"], reverse=True)[:n_rerank]\n",
        "\n",
        "            # Aplicar penalizaci√≥n de redundancia\n",
        "            top_documents = self.penalizar_redundancia(top_documents, threshold=redundancy_threshold)\n",
        "\n",
        "            # Reordenar con el modelo ReRanker\n",
        "            reranked_results = self.rerank(prompt, top_documents)[:n_results]\n",
        "\n",
        "            # Construir el string de resultados\n",
        "            result_string = \"\\n\\n\".join(\n",
        "                [\n",
        "                    f\"{i+1}. Document:\\n\\\"{doc['document'][:1000]}...\\\"\\n\"\n",
        "                    for i, doc in enumerate(reranked_results)\n",
        "                ]\n",
        "            )\n",
        "            return f\"Results:\\n\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error en hybrid_search: {e}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmeZ9GChkBHq",
        "outputId": "27200bc1-21b8-482a-c41f-796b7875fbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:\n",
            "\n",
            "1. Document:\n",
            "\"I kind of want to tell someone \"we're playing White Castle tonight\" and then bring that thing out. That would actually be funny! Enlace al hilo: https://boardgamegeek.com/thread/3220134/number-of-courtiers-in-the-rooms Hi all, at the gate field of the castle there is no limit of courtiers. This rule is mentioned in the rulebook. But how is the situation in the rooms of the castle? thanks, rolwin https://boardgamegeek.com/thread/3168240/can-there-be-two-co... No limit There is no limit of courtiers in the rooms. btw: We thought about this and tried a house rule of 1 meeple per color per room to push players to move upwards in the castle. Didn't change anything in the game experience Enlace al hilo:\n",
            "https://boardgamegeek.com/thread/3220104/daimyo-seals-for-passing-checkpoint...\"\n",
            "\n",
            "\n",
            "2. Document:\n",
            "\"Here flies the flag of the Sakai clan, commanded by Daimio Sakai Tadakiyo. The\n",
            "White Castle is a Euro-like game with resource management mechanisms, worker placement, and dice placement to perform actions. During the game, over the course of three rounds (in which you only have 9 turns, 3 times per round), players send members of their clan to tend the gardens, defend the castle or move up the social ladder of the nobility. At the end of the game, players are awarded victory points in a variety of ways. The first thing you notice is that the box in which the game comes is much smaller than similar games of this category. It's the size of a Carcassonne box, which would almost give you the impression that The White Castle is a game of the same quality/difficulty. And that is not the case. You get a lot more for your thirty euros (which is a real bargain). Once the box is open, you notice that there is no insert and that all the space is filled with ... the game. And that's darn clever!...\"\n",
            "\n",
            "\n",
            "3. Document:\n",
            "\"The White Castle box 77 cards 82 tiles 5 boards 85 wooden pieces 15 dice 1 rulebook The\n",
            "components are high quality with wooden meeples used, dice of 3 different colors, and cardboard tokens. The game comes with 3 cardboard bridges that you will construct when first opening the game. Blocks are used to keep track of resources on player boards, and cards are used to change actions throughout the game. How‚Äôs It Play? Players will be taking turns choosing one of the dice on one of the three bridges to take actions with. The game consists of 3 rounds where players will take 3 turns in each round, so 9 turns total. When choosing a die to perform your action with, you can choose the higher value die of a certain color, or the lower valued die of a certain color. The color will determine what actions you can perform on the board, and how much money you will gain or possibly pay....\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "doc_search = DocSearch(client_castle, collection)\n",
        "query = \"What are the rules of the white castel\"\n",
        "results = doc_search.hybrid_search(query, n_results=3, n_rerank=8, redundancy_threshold=0.9)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYHHExIeZDx2"
      },
      "source": [
        "#### Tabular Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OCwISpD-ZFQ-"
      },
      "outputs": [],
      "source": [
        "class TabularSearch:\n",
        "    def __init__(self, data_frame: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Inicializa la clase de b√∫squeda tabular con par√°metros fijos.\n",
        "\n",
        "        :param data_frame: DataFrame de Pandas donde se realizar√° la b√∫squeda.\n",
        "        \"\"\"\n",
        "        self.data_frame = data_frame\n",
        "        self.temperature = 0.4  # Configuraci√≥n fija\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]  # Configuraci√≥n fija\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_tabular(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Env√≠a un prompt al modelo Qwen para generar la consulta tabular.\n",
        "\n",
        "        :param prompt: Prompt que incluye instrucci√≥n, contexto, entrada, y salida deseada.\n",
        "        :return: Consulta generada por el modelo.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are a system used to generate a query to a tabular search into a dataframe. \"\n",
        "                            \"The dataframe contains the following columns: \"\n",
        "                            \"'Rating', 'Year', 'Review Count', 'Min Players', 'Max Players', 'Play Time (min)', \"\n",
        "                            \"'Suggested Age', 'Complexity', 'Likes', 'Price (USD)'. \"\n",
        "                            \"Your Output must be just the code to access the relevant column. No aditional explantion or information JUST THE CODE \"\n",
        "                            \"Examples of outputs: \"\n",
        "                            \"df['column_name'] \"\n",
        "                            \"df[['column_name', 'column_name2']]\"\n",
        "\n",
        "                        ),\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            return response.choices[0].message[\"content\"].strip()\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error al llamar al modelo Qwen: {e}\")\n",
        "\n",
        "    def tabular_search(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Realiza una b√∫squeda tabular basada en el prompt generado y devuelve los resultados como una cadena.\n",
        "\n",
        "        :param prompt: Prompt que el modelo usar√° para generar una consulta.\n",
        "        :return: Resultados filtrados del DataFrame formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Llamar al modelo para generar la consulta tabular\n",
        "            tabular_query = self.query_model_for_tabular(prompt)\n",
        "            print(f\"Consulta generada: {tabular_query}\")\n",
        "\n",
        "            # Limpiar la consulta generada\n",
        "            cleaned_tabular_query = (\n",
        "                tabular_query.replace('```python', '')\n",
        "                .replace(\"```\", '')\n",
        "                .replace(\"df\", 'self.data_frame')\n",
        "                .strip()\n",
        "            )\n",
        "\n",
        "            # Ejecutar la consulta generada usando eval()\n",
        "            result = eval(f\"{cleaned_tabular_query}\")\n",
        "\n",
        "            # Verificar si hay resultados\n",
        "            if result.empty:\n",
        "                return \"No se encontraron resultados para la consulta.\"\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            result_string = result.to_string(index=False)  # Sin √≠ndices para una salida m√°s limpia\n",
        "            return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda tabular: {e}\")\n",
        "            return f\"Error al procesar la consulta: {e}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYkFO_DZBdO"
      },
      "source": [
        "#### Graph Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6GM7fw-jZDZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphSearch:\n",
        "    def __init__(self, graph_client, api_key: str):\n",
        "        \"\"\"\n",
        "        Inicializa la clase para b√∫squedas en una base de datos de grafos.\n",
        "\n",
        "        :param graph_client: Cliente conectado a la base de datos de grafos.\n",
        "        :param api_key: Clave de API para autenticar el cliente de Hugging Face.\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.graph_client = graph_client\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "        self.qwen_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "    def query_model_for_cypher(self, prompt: str, pos_context: str) -> str:\n",
        "        if not prompt:\n",
        "            print(\"Error: El prompt est√° vac√≠o.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a system specialized in generating Cypher queries for graph databases. \"\n",
        "                        \"The database contains a single node and its relationships with other entities. The central node is labeld 'The White Castle' but you just have to match it like twc: Entity, \"\n",
        "                        \"Examples of relatiosns can be, HAS_DESIGNER, HAS_COVER_ART_BY, INVOLVES, HASMECHANIC OR HASRULE \"\n",
        "                        \"All nodes in the graph are Entity\"\n",
        "                        \"Your task is to analyze the relationships found in the database and generate Cypher queries based on the user‚Äôs request. \"\n",
        "                        \"Your output must be just cypher code.\"\n",
        "                    ),\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"Relationships extracted from the graph database (raw data): {pos_context}\\n\"\n",
        "                        f\"Prompt: {prompt}\"\n",
        "                    ),\n",
        "                }],\n",
        "                max_tokens=500,\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "            )\n",
        "            cypher_query = response.choices[0].message[\"content\"].strip()\n",
        "            return cypher_query.replace(\"```cypher\", \"\").replace(\"```\", \"\")\n",
        "        except KeyError:\n",
        "            print(\"Error: Respuesta mal formada del modelo.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar la consulta Cypher: {e}\")\n",
        "            return None\n",
        "\n",
        "    def graph_search(self, prompt: str, pos_context: str = \"\"):\n",
        "        \"\"\"\n",
        "        Genera y ejecuta una consulta Cypher basada en un prompt y contexto POS.\n",
        "\n",
        "        :param prompt: Prompt proporcionado por el usuario.\n",
        "        :param pos_context: Contexto adicional obtenido a partir de POS.\n",
        "        :return: Resultados formateados como una cadena.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generar la consulta Cypher\n",
        "            cypher_query = self.query_model_for_cypher(prompt, pos_context)\n",
        "            if not cypher_query:\n",
        "                print(\"Error: No se pudo generar la consulta Cypher.\")\n",
        "                return \"Error: No se pudo generar una consulta v√°lida.\"\n",
        "            # Ejecutar la consulta en el cliente Neo4j\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                records = result.data()\n",
        "\n",
        "            # Convertir los resultados en un string formateado\n",
        "            if records:\n",
        "                result_string = \"\\n\".join([str(record) for record in records])\n",
        "                return f\"Resultados obtenidos:\\n{result_string}\"\n",
        "            else:\n",
        "                return \"No se encontraron resultados en la base de datos de grafos.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda en grafos: {e}\")\n",
        "            return f\"Error al buscar en la base de datos: {e}\"\n",
        "\n",
        "\n",
        "    def search_relations_by_pos(self, pos_word: str):\n",
        "        \"\"\"\n",
        "        Realiza una consulta para encontrar relaciones cuyo nombre contenga la palabra clave extra√≠da con POS.\n",
        "\n",
        "        :param pos_word: Palabra clave extra√≠da del texto.\n",
        "        :return: Resultado de la b√∫squeda de relaciones o None si ocurre un error.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cypher_query = f\"\"\"\n",
        "              MATCH ()-[r]->()\n",
        "              WHERE type(r) =~ '.*{pos_word.upper()}.*'\n",
        "              RETURN r\n",
        "              \"\"\"\n",
        "\n",
        "            # Ejecutar la consulta en el cliente de grafos\n",
        "            with self.graph_client.session() as session:\n",
        "                result = session.run(cypher_query)\n",
        "                # Obtener todos los resultados\n",
        "                records = result.data()\n",
        "                return records  # Retorna los resultados de la consulta\n",
        "        except Exception as e:\n",
        "            print(f\"Error al realizar la b√∫squeda de relaciones: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def add_pos_context(prompt: str, graph_search_instance) -> str:\n",
        "    \"\"\"\n",
        "    Toma el prompt, realiza un an√°lisis POS para agregar entidades al contexto y busca relaciones con esas entidades.\n",
        "\n",
        "    :param prompt: Texto de entrada del usuario.\n",
        "    :param graph_search_instance: Instancia de GraphSearch que se utilizar√° para buscar relaciones.\n",
        "    :return: Texto con contexto adicional de las palabras clave y relaciones encontradas.\n",
        "    \"\"\"\n",
        "    # Cargar modelo de spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Procesar el texto y extraer palabras clave con POS\n",
        "    doc = nlp(prompt)\n",
        "    pos_words = [token.text for token in doc if token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"}]\n",
        "    # Agregar contexto para cada palabra clave y realizar la b√∫squeda de relaciones\n",
        "    pos_context = \"\"\n",
        "    for pos_word in pos_words:\n",
        "        # Buscar relaciones en Neo4j que contienen la palabra clave en su nombre\n",
        "        relations = graph_search_instance.search_relations_by_pos(pos_word)\n",
        "        if relations:\n",
        "            pos_context += f\"Found the following relationships containing '{pos_word}': {relations}. \"\n",
        "\n",
        "    # Verifica el contexto POS generado\n",
        "    return pos_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gRgHZsFxGvDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bd7553-1c8a-4010-dc6d-a7aff56a3868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contexto POS: \n",
            "Resultados de la b√∫squeda: Resultados obtenidos:\n",
            "{'creator': {'name': 'Isra'}}\n",
            "{'creator': {'name': 'Shei'}}\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n de Neo4j\n",
        "NEO4J_URI = \"neo4j+s://db16de97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"PYNWCagVuuOIaUJ1Ef9-lyiB0WbcQ5CrwfhFHwWjM_M\"\n",
        "\n",
        "# Inicializar el cliente de Neo4j\n",
        "graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Instancia de la clase GraphSearch\n",
        "graph_search_instance = GraphSearch(graph_client=graph_client, api_key=\"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\")\n",
        "\n",
        "prompt = \"Who created The White Castle??\"\n",
        "pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "print(\"Contexto POS:\", pos_context)\n",
        "\n",
        "# Realizar b√∫squeda en el grafo\n",
        "results = graph_search_instance.graph_search(prompt, pos_context)\n",
        "print(\"Resultados de la b√∫squeda:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kf-lcXlhi8n"
      },
      "source": [
        "## Clasificador Basado en modelo entrenado con ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzMp-Hkhoi8",
        "outputId": "e69ed8d3-30a2-4539-eb21-54519fb4c9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n: 0.9090909090909091\n",
            "Reporte de clasificaci√≥n:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89         5\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       0.80      1.00      0.89         4\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.93      0.93        11\n",
            "weighted avg       0.93      0.91      0.91        11\n",
            "\n",
            "Prompt: '¬øQu√© dice el manual sobre el uso de cartas especiales?' - Clasificaci√≥n: rules\n",
            "Prompt: '¬øEs este juego adecuado para jugadores avanzados?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øC√≥mo recomiendan jugar en partidas de 2 jugadores?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øQu√© pasa si se agotan los recursos en el tablero?' - Clasificaci√≥n: rules\n",
            "Prompt: '¬øEs m√°s divertido jugar en parejas o individualmente?' - Clasificaci√≥n: reviews\n",
            "Prompt: '¬øQu√© estrategias funcionan mejor con 4 jugadores?' - Clasificaci√≥n: reviews\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Dataset ampliado de prompts\n",
        "dataset = [\n",
        "    # Reglas (rules)\n",
        "    (0, \"¬øCu√°les son las reglas para construir un edificio?\"),\n",
        "    (0, \"¬øQu√© pasa si hay un empate en los puntos?\"),\n",
        "    (0, \"¬øCu√°ntas acciones puedo realizar en un turno?\"),\n",
        "    (0, \"¬øQu√© ocurre si no puedo pagar un recurso necesario?\"),\n",
        "    (0, \"¬øC√≥mo se resuelve un desempate en el final del juego?\"),\n",
        "    (0, \"¬øPuedo construir m√°s de un edificio en un solo turno?\"),\n",
        "    (0, \"¬øQu√© limitaciones existen para colocar fichas?\"),\n",
        "    (0, \"¬øEs obligatorio usar todos los recursos en un turno?\"),\n",
        "    (0, \"¬øQu√© pasa si termino un turno con m√°s de tres cartas?\"),\n",
        "    (0, \"¬øC√≥mo se distribuyen los puntos al final del juego?\"),\n",
        "    (0, \"¬øPuedo intercambiar recursos con otros jugadores?\"),\n",
        "    (0, \"¬øQu√© acciones est√°n permitidas en la fase de preparaci√≥n?\"),\n",
        "    (0, \"¬øCu√°ntos turnos tiene cada ronda?\"),\n",
        "    (0, \"¬øCu√°l es el orden para activar habilidades especiales?\"),\n",
        "    (0, \"¬øQu√© reglas aplican para las cartas especiales?\"),\n",
        "    (0, \"¬øPuedo usar habilidades en el turno de otro jugador?\"),\n",
        "    (0, \"¬øQu√© ocurre si el mazo de cartas se agota?\"),\n",
        "    (0, \"¬øHay un l√≠mite de fichas que puedo usar en un turno?\"),\n",
        "\n",
        "    # Rese√±as (reviews)\n",
        "    (1, \"¬øQu√© opinan los jugadores sobre la tem√°tica del juego?\"),\n",
        "    (1, \"¬øEste juego es recomendado para principiantes?\"),\n",
        "    (1, \"¬øC√≥mo describen los jugadores la complejidad del juego?\"),\n",
        "    (1, \"¬øQu√© tan rejugable es este juego seg√∫n las rese√±as?\"),\n",
        "    (1, \"¬øEs un buen juego para jugar en familia?\"),\n",
        "    (1, \"¬øCu√°l es la duraci√≥n t√≠pica de una partida?\"),\n",
        "    (1, \"¬øQu√© tan equilibradas est√°n las estrategias disponibles?\"),\n",
        "    (1, \"¬øC√≥mo se compara este juego con otros del mismo g√©nero?\"),\n",
        "    (1, \"¬øLos componentes del juego tienen buena calidad?\"),\n",
        "    (1, \"¬øQu√© aspectos destacan m√°s los jugadores en sus rese√±as?\"),\n",
        "    (1, \"¬øHay alguna rese√±a negativa sobre el juego?\"),\n",
        "    (1, \"¬øEste juego es m√°s adecuado para expertos o principiantes?\"),\n",
        "    (1, \"¬øQu√© tan divertido es jugar con grupos grandes?\"),\n",
        "    (1, \"¬øLas reglas son f√°ciles de aprender seg√∫n las rese√±as?\"),\n",
        "    (1, \"¬øLos gr√°ficos del juego ayudan a la inmersi√≥n?\"),\n",
        "    (1, \"¬øEs un juego m√°s social o estrat√©gico?\"),\n",
        "    (1, \"¬øLos jugadores mencionan alg√∫n problema recurrente en el dise√±o?\"),\n",
        "    (1, \"¬øSe necesitan expansiones para disfrutar el juego al m√°ximo?\"),\n",
        "\n",
        "    # Comentarios (comments)\n",
        "    (2, \"¬øCu√°les son las mejores estrategias iniciales?\"),\n",
        "    (2, \"¬øHay estrategias avanzadas para ganar m√°s puntos?\"),\n",
        "    (2, \"¬øQu√© tipo de combinaciones de cartas son m√°s efectivas?\"),\n",
        "    (2, \"¬øC√≥mo maximizar los recursos en las primeras rondas?\"),\n",
        "    (2, \"¬øEs mejor centrarse en la defensa o en la ofensiva?\"),\n",
        "    (2, \"¬øQu√© habilidades son m√°s √∫tiles para principiantes?\"),\n",
        "    (2, \"¬øHay estrategias espec√≠ficas para jugar con 2 jugadores?\"),\n",
        "    (2, \"¬øCu√°l es la mejor manera de gestionar los recursos limitados?\"),\n",
        "    (2, \"¬øC√≥mo sacar ventaja de los bonos de las cartas especiales?\"),\n",
        "    (2, \"¬øQu√© t√°cticas recomiendan los jugadores experimentados?\"),\n",
        "    (2, \"¬øC√≥mo adaptar la estrategia dependiendo de los oponentes?\"),\n",
        "    (2, \"¬øEs m√°s beneficioso priorizar los edificios grandes?\"),\n",
        "    (2, \"¬øQu√© estrategias funcionan mejor en partidas r√°pidas?\"),\n",
        "    (2, \"¬øC√≥mo influye el orden de turno en la estrategia?\"),\n",
        "    (2, \"¬øCu√°l es el mejor momento para usar cartas especiales?\"),\n",
        "    (2, \"¬øQu√© cartas son clave para asegurar la victoria?\"),\n",
        "    (2, \"¬øC√≥mo combinar habilidades de edificios para optimizar el puntaje?\"),\n",
        "    (2, \"¬øQu√© errores comunes se deben evitar en estrategias avanzadas?\"),\n",
        "]\n",
        "\n",
        "\n",
        "# Mapeo de categor√≠as\n",
        "categories = {0: \"rules\", 1: \"reviews\", 2: \"comments\"}\n",
        "\n",
        "# Preparar datos\n",
        "X = [text for _, text in dataset]\n",
        "y = [label for label, _ in dataset]\n",
        "\n",
        "# Generar embeddings para el dataset\n",
        "X_vectorized = model.encode(X)\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluar el modelo\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(\"Precisi√≥n:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Reporte de clasificaci√≥n:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Nuevos prompts de prueba\n",
        "new_prompts = [\n",
        "    \"¬øQu√© dice el manual sobre el uso de cartas especiales?\",\n",
        "    \"¬øEs este juego adecuado para jugadores avanzados?\",\n",
        "    \"¬øC√≥mo recomiendan jugar en partidas de 2 jugadores?\",\n",
        "    \"¬øQu√© pasa si se agotan los recursos en el tablero?\",\n",
        "    \"¬øEs m√°s divertido jugar en parejas o individualmente?\",\n",
        "    \"¬øQu√© estrategias funcionan mejor con 4 jugadores?\",\n",
        "]\n",
        "new_embeddings = model.encode(new_prompts)\n",
        "new_predictions = classifier.predict(new_embeddings)\n",
        "\n",
        "# Mostrar resultados\n",
        "for prompt, pred in zip(new_prompts, new_predictions):\n",
        "    print(f\"Prompt: '{prompt}' - Clasificaci√≥n: {categories[pred]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kt-mzmWhbqB"
      },
      "source": [
        "## Clasificador Basado en LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-sMIbvRZGUS"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen2.5-72B-Instruct\", context=None):\n",
        "        \"\"\"\n",
        "        Inicializa el clasificador con un modelo de Hugging Face y un contexto base.\n",
        "\n",
        "        :param model_name: Nombre del modelo pre-entrenado en Hugging Face.\n",
        "        :param context: Contexto inicial que describe las bases de datos.\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = \"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\"  # Tu token de autenticaci√≥n\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.model_name = model_name\n",
        "        self.temperature = 0.4\n",
        "        self.stop_sequences = [\"END_RESPONSE\"]\n",
        "\n",
        "\n",
        "        # Usar el cliente de inferencia de Hugging Face\n",
        "        self.qwen_client = InferenceClient(api_key=self.api_key)\n",
        "\n",
        "        # Contexto configurable con valor por defecto\n",
        "        self.context = context or self._default_context()\n",
        "\n",
        "        # Definici√≥n de las categor√≠as\n",
        "        self.labels = [\"Documents\", \"Graph\", \"Table\"]\n",
        "\n",
        "    def _default_context(self):\n",
        "        return (\n",
        "            \"Documents: This category is for any question regarding the rules, strategies, and textual information of the game.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'What are the rules?' => 'Documents'\\n\"\n",
        "            \"  - 'How do you play the game?' => 'Documents'\\n\\n\"\n",
        "\n",
        "            \"Graph: This category is for questions related to game creators, designers, and interactions between people.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'Who designed the game?' => 'Graph'\\n\"\n",
        "            \"  - 'What is the connection between the game designers?' => 'Graph'\\n\\n\"\n",
        "\n",
        "            \"Table: This category includes specific data about the game, such as number of players, price, and other game statistics.\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            \"  - 'How much does the game cost?' => 'Table'\\n\"\n",
        "            \"  - 'How long does the game last?' => 'Table'\\n\\n\"\n",
        "        )\n",
        "\n",
        "    def clasificar(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Clasifica un prompt en una de las categor√≠as: 'Documents', 'Graph' o 'Table'.\n",
        "\n",
        "        :param prompt: Consulta del usuario en texto.\n",
        "        :return: Etiqueta de clasificaci√≥n con mayor probabilidad.\n",
        "        \"\"\"\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"El prompt proporcionado est√° vac√≠o.\")\n",
        "\n",
        "        # Combinar contexto y prompt\n",
        "        input_text = f\"{self.context} Prompt: {prompt}\"\n",
        "\n",
        "        try:\n",
        "            # Llamar a Qwen para obtener la clasificaci√≥n (utilizando InferenceClient)\n",
        "            response = self.qwen_client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a classifier that categorizes queries into Documents, Graph, or Table.\"\n",
        "                }, {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": input_text\n",
        "                }],\n",
        "                temperature=self.temperature,\n",
        "                stop=self.stop_sequences,\n",
        "                max_tokens=100  # Limitamos el tama√±o de la respuesta\n",
        "            )\n",
        "\n",
        "            # El modelo Qwen proporcionar√° una respuesta de clasificaci√≥n\n",
        "            classification = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Aqu√≠ asumimos que el modelo devolver√° una respuesta adecuada\n",
        "            return classification\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al clasificar el prompt: {e}\")\n",
        "            return \"unknown\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AWBLGaHAJPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb31ba98-7d27-4bd9-a1ba-b31f0487ac3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the rules of the game?\n",
            "Categor√≠a clasificada: Documents\n",
            "\n",
            "Prompt: Who designed the game?\n",
            "Categor√≠a clasificada: Graph\n",
            "\n",
            "Prompt: How much does the game cost?\n",
            "Categor√≠a clasificada: Table\n",
            "\n",
            "Prompt: What is the connection between the game designers?\n",
            "Categor√≠a clasificada: Graph\n",
            "\n",
            "Prompt: How long does the game last?\n",
            "Categor√≠a clasificada: Table\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear una instancia del clasificador\n",
        "classifier = Classifier()\n",
        "\n",
        "# Listado de prompts a probar\n",
        "prompts = [\n",
        "    \"What are the rules of the game?\",  # Deber√≠a ser clasificado como 'Documents'\n",
        "    \"Who designed the game?\",          # Deber√≠a ser clasificado como 'Graph'\n",
        "    \"How much does the game cost?\",    # Deber√≠a ser clasificado como 'Table'\n",
        "    \"What is the connection between the game designers?\",  # Deber√≠a ser 'Graph'\n",
        "    \"How long does the game last?\"     # Deber√≠a ser 'Table'\n",
        "]\n",
        "\n",
        "# Probar clasificaci√≥n para cada prompt\n",
        "for prompt in prompts:\n",
        "    category = classifier.clasificar(prompt)\n",
        "    print(f\"Prompt: {prompt}\\nCategor√≠a clasificada: {category}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfWzpA1ivFW"
      },
      "source": [
        "# ***The White Castle Chatbot***\n",
        "## Chat whith an expert in the famous board game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTIsThbtReG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a40d52-e24b-4cfb-cb54-06a876b5648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who designed this game?\n",
            "Categor√≠a del prompt: Graph\n",
            "Respuesta generada: The designers of The White Castle are Isra and Shei.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: who covered the art of the game?\n",
            "Categor√≠a del prompt: Graph\n",
            "\n",
            "This question is about the person or people who created or designed the art for the game, which falls under the category of interactions between people involved in the game's creation.\n",
            "Respuesta generada: The art of The White Castle is covered by an artist named Kwanchai Moriuchi.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: hay algun otro artista detras del juego\n",
            "Categor√≠a del prompt: Graph\n",
            "Respuesta generada: El artista detr√°s de El Castillo Blanco es Joan Guardiet.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: what is the rating of this game?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: df['Rating']\n",
            "Respuesta generada: The rating of this game is 8.0.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: cuanto sale el juego?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: Price (USD)\n",
            "Error al realizar la b√∫squeda tabular: name 'Price' is not defined\n",
            "Respuesta generada: El precio de The White Castle no se especifica en la informaci√≥n proporcionada.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: how much does it cost?\n",
            "Categor√≠a del prompt: Table\n",
            "Consulta generada: df['Price (USD)']\n",
            "Respuesta generada: 26.31\n",
            "    \n",
            "    Question: what are the available colors for the pawns?\n",
            "\n",
            "    Retrieved Information: \n",
            "    La lista de colores disponibles para las figuritas es la siguiente:\n",
            "    - Rojo\n",
            "    - Azul\n",
            "    - Verde\n",
            "    - Amarillo\n",
            "    - Negro\n",
            "    \n",
            "    Please provide only the available colors, no extra steps or explanations.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: me gustaria aprender a jugar me podrias dar las reglas basicas para comenzar a jugar?\n",
            "Categor√≠a del prompt: Documents\n",
            "Respuesta generada: Para jugar a *The White Castle*, cada jugador comienza con un personaje y un conjunto de recursos. Tu objetivo es llegar al Castillo Blanco y reclamarlo como tuyo. En el camino, te encontrar√°s con obst√°culos y otros jugadores. El combate se resuelve tirando los dados y comparando los resultados. Algunas √°reas requieren que se lance un n√∫mero espec√≠fico para continuar. Los recursos se pueden gastar para obtener ventajas o para curar a tu personaje. El primer jugador en llegar al Castillo Blanco y reclamarlo gana la partida.\n",
            "\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: I am experience in the game but not an expert, could you give me some tips to improve in this game, i mean what rules or mechanics do i have to improve in order to become a better player\n",
            "Categor√≠a del prompt: Documents\n",
            "\n",
            "This query is seeking advice and tips on how to improve gameplay, which involves understanding the rules and mechanics of the game. Therefore, it falls under the \"Documents\" category.\n",
            "Respuesta generada: To enhance your gameplay in *The White Castle*, you may want to focus on mastering the combat system, as it plays a significant role in the game. Additionally, paying close attention to resource management and strategy development can also help you improve. Keep practicing and studying the game to continue your growth as a player.\n",
            "--------------------------------------------------  \n",
            "\n",
            " (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: exit\n",
            "¬°Hasta luego!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from langdetect import detect\n",
        "from jinja2 import Template\n",
        "\n",
        "chat_graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "def dividir_texto(texto, max_length=1000):\n",
        "    palabras = texto.split()\n",
        "    fragmentos = []\n",
        "    fragmento_actual = \"\"\n",
        "\n",
        "    for palabra in palabras:\n",
        "        if len(fragmento_actual) + len(palabra) + 1 > max_length:\n",
        "            fragmentos.append(fragmento_actual)\n",
        "            fragmento_actual = palabra\n",
        "        else:\n",
        "            fragmento_actual += \" \" + palabra if fragmento_actual else palabra\n",
        "\n",
        "    if fragmento_actual:\n",
        "        fragmentos.append(fragmento_actual)\n",
        "\n",
        "    return fragmentos\n",
        "\n",
        "# Funci√≥n para traducir un texto a ingl√©s\n",
        "def traducir_a_espa√±ol(texto):\n",
        "      fragmentos = dividir_texto(texto)\n",
        "      traduccion = \"\"\n",
        "      for fragmento in fragmentos:\n",
        "          traduccion += ts.translate_text(fragmento, translator='bing', from_language='en', to_language='es') + \"\\n\"\n",
        "      return traduccion\n",
        "\n",
        "\n",
        "\n",
        "# Funci√≥n para detectar el idioma del texto\n",
        "def detect_language(text):\n",
        "    return detect(text)\n",
        "\n",
        "# Funci√≥n para generar el template del chat\n",
        "def zephyr_chat_template(messages, add_generation_prompt=True):\n",
        "    template_str  = \"{% for message in messages %}\"\n",
        "    template_str += \"{% if message['role'] == 'user' %}\"\n",
        "    template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
        "    template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'system' %}\"\n",
        "    template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% else %}\"\n",
        "    template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "    template_str += \"{% endfor %}\"\n",
        "    template_str += \"{% if add_generation_prompt %}\"\n",
        "    template_str += \"<|assistant|>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "\n",
        "    # Crear un objeto de plantilla con la cadena de plantilla\n",
        "    template = Template(template_str)\n",
        "\n",
        "    # Renderizar la plantilla con los mensajes proporcionados\n",
        "    return template.render(messages=messages, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "# Funci√≥n para obtener el modelo generativo (en este caso usando Zephyr)\n",
        "def generate_response_with_model(context):\n",
        "    api_key = \"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\"\n",
        "\n",
        "    api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    data = {\n",
        "        \"inputs\": context,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 256,\n",
        "            \"temperature\": 0.68,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        if isinstance(result, list):\n",
        "            return result[0].get('generated_text', 'No se gener√≥ texto.')\n",
        "        else:\n",
        "            return \"Error en la respuesta del modelo: la respuesta no es una lista.\"\n",
        "    else:\n",
        "        return f\"Error en la solicitud: {response.status_code} - {response.text}\"\n",
        "\n",
        "game_state = \"\"\"\n",
        "    You are a chatbot specialized in the famous board game *The White Castle*.\n",
        "    You may want to think and process step by step the information that you have before yoy respond\n",
        "    Your task is to generate responses based on the user's question and the relevant information retrieved from the database.\n",
        "    You should take into account the question, the retrieved information, and the context to provide a detailed and accurate response.\n",
        "    You will rearly recive the exact information to the question but you have to formulate your answer based on what you know.\n",
        "    For instance you may no be provided with the entire rulebook but you can say \"Some of the rules consist of ...\"\n",
        "    Your answers should be clear, concise, and directly related to the game, *The White Castle* and you dont hace to cite any retrieved information, take it as if you already know it.\n",
        "    \"\"\"\n",
        "loop_flag = True\n",
        "while loop_flag:\n",
        "    # Paso 1: Obtener el prompt del usuario\n",
        "    user_prompt = input(\" (Ingrese 'exit' para salir) Por favor, ingrese su consulta sobre el juego: \")\n",
        "    if user_prompt.lower() == \"exit\":\n",
        "        print(\"¬°Hasta luego!\")\n",
        "        break\n",
        "    esp_flag = False\n",
        "\n",
        "    # Detectar el idioma del texto\n",
        "    if detect_language(user_prompt) == \"es\":\n",
        "        user_prompt = ts.translate_text(user_prompt, translator='bing', from_language='es', to_language='en')\n",
        "        esp_flag = True\n",
        "\n",
        "    # Paso 2: Clasificar el prompt\n",
        "    classifier = Classifier()\n",
        "    category = classifier.clasificar(user_prompt)\n",
        "    print(f\"Categor√≠a del prompt: {category}\")\n",
        "\n",
        "    # Paso 3: Recuperar la informaci√≥n basada en la clasificaci√≥n\n",
        "    if category == \"Documents\":\n",
        "        doc_search = DocSearch(client_castle, collection)\n",
        "        retrieved_info = doc_search.hybrid_search(user_prompt)\n",
        "    elif category == \"Graph\":\n",
        "        graph_search = GraphSearch(graph_client=chat_graph_client, api_key=\"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\")\n",
        "        pos_context = add_pos_context(prompt, graph_search_instance)\n",
        "        # Realizar b√∫squeda en el grafo\n",
        "        retrieved_info = graph_search.graph_search(user_prompt, pos_context)\n",
        "    elif category == \"Table\":\n",
        "        tabular_search = TabularSearch(df_castle)\n",
        "        retrieved_info = tabular_search.tabular_search(user_prompt)\n",
        "    else:\n",
        "        retrieved_info = \"No se pudo clasificar la consulta adecuadamente.\"\n",
        "\n",
        "\n",
        "\n",
        "    # Ajusta el contexto para que sea relevante y claro\n",
        "    context = f\"\"\"\n",
        "    Role: {game_state}\n",
        "    Question: {user_prompt}\n",
        "\n",
        "    Retrieved Information: {retrieved_info}\n",
        "\n",
        "    Please provide only the direct answer, no extra steps or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Paso 5: Generar respuesta con un modelo generativo (Zephyr)\n",
        "    response = generate_response_with_model(context)\n",
        "\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\")[1].strip()\n",
        "    elif \"Response:\" in response:\n",
        "        answer = response.split(\"Response:\")[1].strip()\n",
        "    else:\n",
        "        answer = response\n",
        "\n",
        "\n",
        "    # Si el texto original estaba en espa√±ol, traducimos la respuesta generada al espa√±ol\n",
        "    if esp_flag:\n",
        "        answer = traducir_a_espa√±ol(answer)\n",
        "    print(f\"Respuesta generada: {answer}\")\n",
        "    print(f\"{'-' * 50}  \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cClPcYLfIXa"
      },
      "source": [
        "# ReAct Agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from langdetect import detect\n",
        "\n",
        "chat_graph_client = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Logger configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def doc_search(query):\n",
        "    try:\n",
        "        # Extraer el texto del diccionario\n",
        "        query_string = query.get('query', '') if isinstance(query, dict) else query\n",
        "        if not query_string:\n",
        "            raise ValueError(\"Query is empty.\")\n",
        "\n",
        "        # Realizar la b√∫squeda\n",
        "        searcher = DocSearch(client_castle, collection)\n",
        "        return searcher.hybrid_search(query_string)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in doc_search: {str(e)}\")\n",
        "        return \"Could not retrieve information from the vector database.\"\n",
        "\n",
        "\n",
        "def graph_search(query):\n",
        "    try:\n",
        "        # Extraer el texto del diccionario\n",
        "        query_string = query.get('query', '') if isinstance(query, dict) else query\n",
        "        if not query_string:\n",
        "            raise ValueError(\"Query is empty.\")\n",
        "\n",
        "        # Realizar la b√∫squeda\n",
        "        searcher = GraphSearch(graph_client=chat_graph_client, api_key=\"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\")\n",
        "        pos_context = add_pos_context(query_string, searcher)\n",
        "        return searcher.graph_search(query_string, pos_context)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in graph_search: {str(e)}\")\n",
        "        return \"Could not retrieve information from the graph database.\"\n",
        "\n",
        "\n",
        "def table_search(query):\n",
        "    try:\n",
        "        # Extraer el texto del diccionario\n",
        "        query_string = query.get('query', '') if isinstance(query, dict) else query\n",
        "        if not query_string:\n",
        "            raise ValueError(\"Query is empty.\")\n",
        "\n",
        "        # Realizar la b√∫squeda\n",
        "        searcher = TabularSearch(df_castle)\n",
        "        return searcher.tabular_search(query_string)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in table_search: {str(e)}\")\n",
        "        return \"Could not retrieve information from the tabular database.\"\n",
        "\n",
        "\n",
        "# Create the tools for the agent\n",
        "tools_list = [\n",
        "    FunctionTool.from_defaults(fn=graph_search, description=\"Search for information in the graph database. Use: query text\"),\n",
        "    FunctionTool.from_defaults(fn=table_search, description=\"Search for information in the tabular database. Use: query text\"),\n",
        "    FunctionTool.from_defaults(fn=doc_search, description=\"Search for information in the vector database. Use: query text\")\n",
        "]\n",
        "\n",
        "\n",
        "# Configure the LLM of Ollama to use Llama 3.2\n",
        "llm = Ollama(\n",
        "    model=\"llama3.2:latest\",\n",
        "    request_timeout=15.0,\n",
        "    temperature=0.15,\n",
        "    context_window=4096\n",
        ")\n",
        "\n",
        "# Assign the LLM configuration to the Settings object for global use\n",
        "Settings.llm = llm\n",
        "\n",
        "# Create the ReAct agent with the tools and defined configurations\n",
        "agent = ReActAgent.from_tools(\n",
        "    tools_list,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    chat_formatter=ReActChatFormatter(),\n",
        "    system_prompt = \"\"\"Your role: Answer questions about the board game 'The White Castle' using only information provided by the available tools. You can receive question in English and Spanish.\n",
        "\n",
        "        ## Available Tools:\n",
        "        graph_search: Information about designers, artists, and other concrete relationships between elements of the game.\n",
        "        table_search: ['Rating', 'Year', 'Review Count', 'Min Players', 'Max Players', 'Play Time (min)', 'Suggested Age', 'Complexity', 'Likes', 'Price (USD)'.] this is the only information available in the table.\n",
        "        doc_search: General information, rulebook, review and feedback, comments or opinions given by players.\n",
        "\n",
        "        ### Instructions for each query:\n",
        "        0. If the query is in Spanish, translate it first to English.\n",
        "        1. Analyze the query to determine the appropriate tool.\n",
        "        2. Call one or more tools using exactly the query received (in English) as a plain string.\n",
        "        3. Do not invent information. Only respond with data obtained from the tools.\n",
        "        4. **All the tools only receive one parameter: the query (in English) from the user, as a plain string. No additional properties or structured data.**\n",
        "\n",
        "        5. Response format:\n",
        "          - Thought: Explain what information you need and the tool to use.\n",
        "          - Action: Call the appropriate tool.\n",
        "          - Action Input: The query as a plain string.\n",
        "          - Observation: The response from the tool.\n",
        "          - Final Answer: A clear and complete response based on the obtained information.\n",
        "\n",
        "\n",
        "        5. Response format:\n",
        "          - Thought: Explain what information you need and the tool to use.\n",
        "          - Action: Call the appropriate tool.\n",
        "          - Action Input: quey recieved\n",
        "          - Observation: The response from the tool.\n",
        "          - Final Answer: A clear and complete response based on the obtained information.\n",
        "\n",
        "        ### Example Interaction:\n",
        "        Example 1:\n",
        "        Query: \"What are the rules of the game?\"\n",
        "        - Thought: I need to consult information about how to win the game in the rules.\n",
        "        - Action: doc_search\n",
        "        - Action Input: \"What are the rules to win the game?\"\n",
        "        - Observation: \"The players' objectives revolve around selecting dice, collecting resources, and strategizing the family members of their players within the Himeji Castle to accumulate victory points.\"\n",
        "        - Final Answer: \"The game rules revolve around selecting dice, collecting resources, and strategizing the family members of their players within the Himeji Castle to accumulate victory points.\"\n",
        "\n",
        "        Example 2:\n",
        "        - Query: \"Who designed the game?\"\n",
        "        - Thought: I need to search for information about the game's designers.\n",
        "        - Action: graph_search\n",
        "        - Action Input: \"Who designed the game?\"\n",
        "        - Observation: \"Isha and Shei\"\n",
        "        - Final Answer: The game designers are Israel Cendrero and Sheila Santos.\n",
        "\n",
        "        ### Additional Rules:\n",
        "        - Do not use prior information; each query is independent.\n",
        "        - Process the keywords in the query and call only the tools that correspond to the query.\n",
        "        - If the information is not available, respond: \"No information found for your query.\"\n",
        "        \"\"\"\n",
        "        ,\n",
        "    react_chat_history=False,\n",
        "    context=\"\"\"You are an expert assistant who answers queries in english or spanish about the board game called 'The White Castle'.\"\"\"\n",
        ")\n",
        "\n",
        "# Function to interact with the ReAct agent\n",
        "def chat_with_agent(query: str):\n",
        "    try:\n",
        "        if not query.strip():\n",
        "            return \"The query is empty.\"\n",
        "\n",
        "        if len(query) > 500:\n",
        "            return \"The query is too long. Please try to summarize it.\"\n",
        "\n",
        "        # Detect the language of the query\n",
        "        query_lang = detect(query)\n",
        "        esp_flag = False\n",
        "        if query_lang == \"es\":\n",
        "            translated_query = traducir_a_ingles(query)\n",
        "            esp_flag = True\n",
        "        else:\n",
        "            translated_query = query\n",
        "\n",
        "        # Get the response from the agent\n",
        "        response = agent.chat(translated_query)\n",
        "\n",
        "        # Translate back to Spanish if the original query was in Spanish\n",
        "        if esp_flag:\n",
        "            response = traducir_a_espa√±ol(response)\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in the agent: {str(e)}\")\n",
        "        return f\"Error processing the query: {str(e)}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdRb8SFUu9h",
        "outputId": "2996e197-44c5-439f-ccf4-f4d030071bd1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage to interact with the agent\n",
        "def run_example():\n",
        "    queries = [\n",
        "    \"Who designed the game and whats the minimun of players to play it?\",  # Requiere graph_search y table_search\n",
        "    \"What are the game rules and whats the rating of the game?\",  # Requiere doc_search y table_search\n",
        "    \"how complex is the game and how many likes from people does it have?\",  # Requiere table_search para ambas\n",
        "    \"Who cover the art of the game and how much time does it takes?\",  # Requiere graph_search y table_search\n",
        "    \"¬øEn qu√© a√±o se lanz√≥ el juego y cu√°ntos jugadores puede tener como maximo?\",  # Requiere table_search para ambas\n",
        "]\n",
        "\n",
        "\n",
        "    print(\"\\n=== Example Interaction with the ReAct Agent ===\")\n",
        "    for i, query in enumerate(queries):\n",
        "        print(f\"\\nQuery {i+1}: {query}\")\n",
        "        print(\"------------------------------------------------------\")\n",
        "        response = chat_with_agent(query)\n",
        "        print(f\"Response {i+1}:\\n{response}\")\n",
        "        print(\"------------------------------------------------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_example()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbJMgJ9fIrv8",
        "outputId": "d2496335-9e45-4690-b703-e6608972e419"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Example Interaction with the ReAct Agent ===\n",
            "\n",
            "Query 1: Who designed the game and whats the minimun of players to play it?\n",
            "------------------------------------------------------\n",
            "Consulta generada: df[['Min Players']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "ERROR:__main__:Error in the agent: Reached max iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1:\n",
            "Error processing the query: Reached max iterations.\n",
            "------------------------------------------------------\n",
            "\n",
            "Query 2: What are the game rules and whats the rating og the game?\n",
            "------------------------------------------------------\n",
            "Response 2:\n",
            "The game has a rating of 9/10, according to the reviews provided. It is considered a highly strategic and accessible game with a small box size that offers a lot for its price.\n",
            "------------------------------------------------------\n",
            "\n",
            "Query 3: how complex is the game and how many likes from people does it have?\n",
            "------------------------------------------------------\n",
            "Response 3:\n",
            "The game complexity of White Castle is considered moderate to high, with a rating of 6/10 on Board Game Geek (BGG). It has a unique blend of resource management, worker placement, and dice placement mechanics that make it engaging but not overly complex. The gameplay typically lasts around 30-60 minutes, making it a great option for players who want a strategic game without a huge time commitment.\n",
            "------------------------------------------------------\n",
            "\n",
            "Query 4: Who cover the art of the game and how much time does it takes?\n",
            "------------------------------------------------------\n",
            "Consulta generada: df['Artist']\n",
            "Error al realizar la b√∫squeda tabular: 'Artist'\n",
            "Consulta generada: The request does not match the columns available in the dataframe. However, if you are looking for a specific column, here is the format:\n",
            "\n",
            "```python\n",
            "df['column_name']\n",
            "```\n",
            "\n",
            "If you need multiple columns, you can use:\n",
            "\n",
            "```python\n",
            "df[['column_name1', 'column_name2']]\n",
            "```\n",
            "\n",
            "Since the specific columns you are asking for are not available, please provide the relevant column names from the dataframe.\n",
            "Error al realizar la b√∫squeda tabular: invalid syntax (<string>, line 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error in the agent: Reached max iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 4:\n",
            "Error processing the query: Reached max iterations.\n",
            "------------------------------------------------------\n",
            "\n",
            "Query 5: ¬øEn qu√© a√±o se lanz√≥ el juego y cu√°ntos jugadores puede tener como maximo?\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: type)} {position: line: 4, column: 114, offset: 259} for query: \"\\nMATCH (twc:Entity)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(component:Entity)\\nWHERE twc.name = 'The White Castle'\\nOPTIONAL MATCH (twc)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(releaseYear:Entity {type: 'releaseYear'})\\nOPTIONAL MATCH (twc)-[:HASPLAYERCOUNTRESTRICTION]->(playerCount:Entity {type: 'playerCount'})\\nRETURN releaseYear.value AS releaseYear, playerCount.value AS playerCount\\n\"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: type)} {position: line: 5, column: 73, offset: 353} for query: \"\\nMATCH (twc:Entity)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(component:Entity)\\nWHERE twc.name = 'The White Castle'\\nOPTIONAL MATCH (twc)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(releaseYear:Entity {type: 'releaseYear'})\\nOPTIONAL MATCH (twc)-[:HASPLAYERCOUNTRESTRICTION]->(playerCount:Entity {type: 'playerCount'})\\nRETURN releaseYear.value AS releaseYear, playerCount.value AS playerCount\\n\"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: value)} {position: line: 6, column: 20, offset: 394} for query: \"\\nMATCH (twc:Entity)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(component:Entity)\\nWHERE twc.name = 'The White Castle'\\nOPTIONAL MATCH (twc)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(releaseYear:Entity {type: 'releaseYear'})\\nOPTIONAL MATCH (twc)-[:HASPLAYERCOUNTRESTRICTION]->(playerCount:Entity {type: 'playerCount'})\\nRETURN releaseYear.value AS releaseYear, playerCount.value AS playerCount\\n\"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: value)} {position: line: 6, column: 54, offset: 428} for query: \"\\nMATCH (twc:Entity)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(component:Entity)\\nWHERE twc.name = 'The White Castle'\\nOPTIONAL MATCH (twc)-[:HASGAMECOMPONENT|HAS_PLAYER_DOMAIN_BOARD|HASPLAYERCOUNTRESTRICTION]->(releaseYear:Entity {type: 'releaseYear'})\\nOPTIONAL MATCH (twc)-[:HASPLAYERCOUNTRESTRICTION]->(playerCount:Entity {type: 'playerCount'})\\nRETURN releaseYear.value AS releaseYear, playerCount.value AS playerCount\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta generada: df[['Max Players', 'Min Players']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: HAS_GAME_OBJECTIVE)} {position: line: 2, column: 126, offset: 126} for query: \"\\nMATCH (twc:Entity {name: 'The White Castle'})-[:HASGAMEMECHANIC|HAS_GAME_MECHANIC|HASGAMEMODE|HAS_GAME_MODE|HASGAMEOBJECTIVE|HAS_GAME_OBJECTIVE]->(element:Entity)\\nRETURN element.name, element.complexity\\n\"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: complexity)} {position: line: 3, column: 30, offset: 193} for query: \"\\nMATCH (twc:Entity {name: 'The White Castle'})-[:HASGAMEMECHANIC|HAS_GAME_MECHANIC|HASGAMEMODE|HAS_GAME_MODE|HASGAMEOBJECTIVE|HAS_GAME_OBJECTIVE]->(element:Entity)\\nRETURN element.name, element.complexity\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta generada: df['Complexity']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error in the agent: name 'traducir_a_espa√±ol' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 5:\n",
            "Error processing the query: name 'traducir_a_espa√±ol' is not defined\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliograf√≠a\n",
        "\n",
        "```bibtex\n",
        "@misc{qwen2.5,\n",
        "    title = {Qwen2.5: A Party of Foundation Models},\n",
        "    url = {https://qwenlm.github.io/blog/qwen2.5/},\n",
        "    author = {Qwen Team},\n",
        "    month = {September},\n",
        "    year = {2024}\n",
        "}\n",
        "\n",
        "@article{qwen2,\n",
        "    title={Qwen2 Technical Report},\n",
        "    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n",
        "    journal={arXiv preprint arXiv:2407.10671},\n",
        "    year={2024}\n",
        "}\n",
        "\n",
        "@misc{intfloat2023e5,\n",
        "    title = {Multilingual E5: A Text Embedding Model for Retrieval Tasks},\n",
        "    author = {Intfloat Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/intfloat/multilingual-e5-small}}\n",
        "}\n",
        "\n",
        "@misc{bge2023reranker,\n",
        "    title = {BAAI General Embedding Reranker v2 (BGE ReRanker)},\n",
        "    author = {BAAI Team},\n",
        "    year = {2023},\n",
        "    howpublished = {\\url{https://huggingface.co/BAAI/bge-reranker-v2-m3}}\n",
        "}\n",
        "\n",
        "@misc{phi3ollama2024,\n",
        "    title = {Phi-3: A Series of Lightweight Language Models},\n",
        "    author = {Ollama Team},\n",
        "    year = {2024},\n",
        "    howpublished = {\\url{https://ollama.ai/library/phi3}}\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "VF3OeiFbAPw8"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2UZAbT5Vc2kB",
        "ZjhPfbqfBX8L",
        "9IomKasVMiEJ",
        "7qPW3Uj9MkTG",
        "6GPqLVthBGho",
        "VRqtXwrAY-ZO",
        "1Kf-lcXlhi8n",
        "3Kt-mzmWhbqB",
        "vAfWzpA1ivFW"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}